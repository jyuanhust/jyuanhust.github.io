<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="yuan" href="https://jyuanhust.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="yuan" href="https://jyuanhust.github.io/atom.xml"><link rel="alternate" type="application/json" title="yuan" href="https://jyuanhust.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="chapter_convolutional-modern"><link rel="canonical" href="https://jyuanhust.github.io/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-modern/batch-norm/"><title>batch-norm - chapter_convolutional-modern - pytorch深度学习 - ai | Mi Manchi = yuan = Whatever is worth doing at all is worth doing well</title><meta name="generator" content="Hexo 6.2.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">batch-norm</h1><div class="meta"><span class="item" title="创建时间：2023-02-15 00:00:00"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2023-02-15T00:00:00+08:00">2023-02-15</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>8.7k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>8 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Mi Manchi</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(52).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(6).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(22).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(82).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(66).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(85).webp"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/" itemprop="item" rel="index" title="分类于 ai"><span itemprop="name">ai</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="item" rel="index" title="分类于 pytorch深度学习"><span itemprop="name">pytorch深度学习</span></a><meta itemprop="position" content="2"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-convolutional-modern/" itemprop="item" rel="index" title="分类于 chapter_convolutional-modern"><span itemprop="name">chapter_convolutional-modern</span></a><meta itemprop="position" content="3"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://jyuanhust.github.io/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-modern/batch-norm/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="yuan"><meta itemprop="description" content="Whatever is worth doing at all is worth doing well, "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="yuan"></span><div class="body md" itemprop="articleBody"><h1 id="批量规范化"><a class="anchor" href="#批量规范化">#</a> 批量规范化</h1><p>🏷 <code>sec_batch_norm</code></p><p>训练深层神经网络是十分困难的，特别是在较短的时间内使他们收敛更加棘手。<br>本节将介绍<em>批量规范化</em>（batch normalization） :cite: <code>Ioffe.Szegedy.2015</code> ，这是一种流行且有效的技术，可持续加速深层网络的收敛速度。<br>再结合在 :numref: <code>sec_resnet</code> 中将介绍的残差块，批量规范化使得研究人员能够训练 100 层以上的网络。</p><h2 id="训练深层网络"><a class="anchor" href="#训练深层网络">#</a> 训练深层网络</h2><p>为什么需要批量规范化层呢？让我们来回顾一下训练神经网络时出现的一些实际挑战。</p><p>首先，数据预处理的方式通常会对最终结果产生巨大影响。<br>回想一下我们应用多层感知机来预测房价的例子（ :numref: <code>sec_kaggle_house</code> ）。<br>使用真实数据时，我们的第一步是标准化输入特征，使其平均值为 0，方差为 1。<br>直观地说，这种标准化可以很好地与我们的优化器配合使用，因为它可以将参数的量级进行统一。</p><p>第二，对于典型的多层感知机或卷积神经网络。当我们训练时，中间层中的变量（例如，多层感知机中的仿射变换输出）可能具有更广的变化范围：不论是沿着从输入到输出的层，跨同一层中的单元，或是随着时间的推移，模型参数的随着训练更新变幻莫测。<br>批量规范化的发明者非正式地假设，这些变量分布中的这种偏移可能会阻碍网络的收敛。<br>直观地说，我们可能会猜想，如果一个层的可变值是另一层的 100 倍，这可能需要对学习率进行补偿调整。</p><p>第三，更深层的网络很复杂，容易过拟合。<br>这意味着正则化变得更加重要。</p><p>批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。<br>接下来，我们应用比例系数和比例偏移。<br>正是由于这个基于<em>批量</em>统计的<em>标准化</em>，才有了<em>批量规范化</em>的名称。</p><p>请注意，如果我们尝试使用大小为 1 的小批量应用批量规范化，我们将无法学到任何东西。<br>这是因为在减去均值之后，每个隐藏单元将为 0。<br>所以，只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。<br>请注意，在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。</p><p>从形式上来说，用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi><mo>∈</mo><mi mathvariant="script">B</mi></mrow><annotation encoding="application/x-tex">\mathbf{x} \in \mathcal{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5782em;vertical-align:-.0391em"></span><span class="mord"><span class="mord mathbf">x</span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal" style="margin-right:.03041em">B</span></span></span></span></span> 表示一个来自小批量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">B</mi></mrow><annotation encoding="application/x-tex">\mathcal{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal" style="margin-right:.03041em">B</span></span></span></span></span> 的输入，批量规范化<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">B</mi><mi mathvariant="normal">N</mi></mrow><annotation encoding="application/x-tex">\mathrm{BN}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathrm">B</span><span class="mord mathrm">N</span></span></span></span></span> 根据以下表达式转换<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.44444em;vertical-align:0"></span><span class="mord"><span class="mord mathbf">x</span></span></span></span></span>：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">B</mi><mi mathvariant="normal">N</mi></mrow><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="bold-italic">γ</mi><mo>⊙</mo><mfrac><mrow><mi mathvariant="bold">x</mi><mo>−</mo><msub><mover accent="true"><mi mathvariant="bold-italic">μ</mi><mo>^</mo></mover><mi mathvariant="script">B</mi></msub></mrow><msub><mover accent="true"><mi mathvariant="bold-italic">σ</mi><mo>^</mo></mover><mi mathvariant="script">B</mi></msub></mfrac><mo>+</mo><mi mathvariant="bold-italic">β</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\mathrm{BN}(\mathbf{x}) = \boldsymbol{\gamma} \odot \frac{\mathbf{x} - \hat{\boldsymbol{\mu}}_\mathcal{B}}{\hat{\boldsymbol{\sigma}}_\mathcal{B}} + \boldsymbol{\beta}.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathrm">B</span><span class="mord mathrm">N</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.7777700000000001em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.06389em">γ</span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:2.22088em;vertical-align:-.8360000000000001em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3848799999999999em"><span style="top:-2.3139999999999996em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.70788em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.03704em">σ</span></span></span></span></span><span style="top:-3.01344em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.32833099999999993em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:.03041em">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.6769999999999996em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathbf">x</span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.70788em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">μ</span></span></span></span></span><span style="top:-3.01344em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.25em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.19444em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.23419099999999995em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:.03041em">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.8360000000000001em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.03403em">β</span></span></span><span class="mord">.</span></span></span></span></span></p><p>:eqlabel: <code>eq_batchnorm</code></p><p>在 :eqref: <code>eq_batchnorm</code> 中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi mathvariant="bold-italic">μ</mi><mo>^</mo></mover><mi mathvariant="script">B</mi></msub></mrow><annotation encoding="application/x-tex">\hat{\boldsymbol{\mu}}_\mathcal{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.95202em;vertical-align:-.24414em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.70788em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">μ</span></span></span></span></span><span style="top:-3.01344em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.25em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.19444em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.23419099999999995em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:.03041em">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span></span></span></span> 是小批量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">B</mi></mrow><annotation encoding="application/x-tex">\mathcal{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal" style="margin-right:.03041em">B</span></span></span></span></span> 的样本均值，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi mathvariant="bold-italic">σ</mi><mo>^</mo></mover><mi mathvariant="script">B</mi></msub></mrow><annotation encoding="application/x-tex">\hat{\boldsymbol{\sigma}}_\mathcal{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.85788em;vertical-align:-.15em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.70788em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.03704em">σ</span></span></span></span></span><span style="top:-3.01344em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.32833099999999993em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:.03041em">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 是小批量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">B</mi></mrow><annotation encoding="application/x-tex">\mathcal{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathcal" style="margin-right:.03041em">B</span></span></span></span></span> 的样本标准差。<br>应用标准化后，生成的小批量的平均值为 0 和单位方差为 1。<br>由于单位方差（与其他一些魔法数）是一个主观的选择，因此我们通常包含<br><em>拉伸参数</em>（scale）<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">γ</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\gamma}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.63888em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.06389em">γ</span></span></span></span></span></span> 和<em>偏移参数</em>（shift）<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">β</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.03403em">β</span></span></span></span></span></span>，它们的形状与<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.44444em;vertical-align:0"></span><span class="mord"><span class="mord mathbf">x</span></span></span></span></span> 相同。<br>请注意，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">γ</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\gamma}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.63888em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.06389em">γ</span></span></span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">β</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.03403em">β</span></span></span></span></span></span> 是需要与其他模型参数一起学习的参数。</p><p>由于在训练过程中，中间层的变化幅度不能过于剧烈，而批量规范化将每一层主动居中，并将它们重新调整为给定的平均值和大小（通过<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi mathvariant="bold-italic">μ</mi><mo>^</mo></mover><mi mathvariant="script">B</mi></msub></mrow><annotation encoding="application/x-tex">\hat{\boldsymbol{\mu}}_\mathcal{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.95202em;vertical-align:-.24414em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.70788em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">μ</span></span></span></span></span><span style="top:-3.01344em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.25em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.19444em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.23419099999999995em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:.03041em">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi mathvariant="bold-italic">σ</mi><mo>^</mo></mover><mi mathvariant="script">B</mi></msub></mrow><annotation encoding="application/x-tex">{\hat{\boldsymbol{\sigma}}_\mathcal{B}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.85788em;vertical-align:-.15em"></span><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.70788em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.03704em">σ</span></span></span></span></span><span style="top:-3.01344em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.32833099999999993em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:.03041em">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></span>）。</p><p>从形式上来看，我们计算出 :eqref: <code>eq_batchnorm</code> 中的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi mathvariant="bold-italic">μ</mi><mo>^</mo></mover><mi mathvariant="script">B</mi></msub></mrow><annotation encoding="application/x-tex">\hat{\boldsymbol{\mu}}_\mathcal{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.95202em;vertical-align:-.24414em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.70788em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">μ</span></span></span></span></span><span style="top:-3.01344em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.25em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.19444em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.23419099999999995em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:.03041em">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi mathvariant="bold-italic">σ</mi><mo>^</mo></mover><mi mathvariant="script">B</mi></msub></mrow><annotation encoding="application/x-tex">{\hat{\boldsymbol{\sigma}}_\mathcal{B}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.85788em;vertical-align:-.15em"></span><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.70788em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.03704em">σ</span></span></span></span></span><span style="top:-3.01344em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.32833099999999993em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:.03041em">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></span>，如下所示：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mover accent="true"><mi mathvariant="bold-italic">μ</mi><mo>^</mo></mover><mi mathvariant="script">B</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="script">B</mi><mi mathvariant="normal">∣</mi></mrow></mfrac><munder><mo>∑</mo><mrow><mi mathvariant="bold">x</mi><mo>∈</mo><mi mathvariant="script">B</mi></mrow></munder><mi mathvariant="bold">x</mi><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msubsup><mover accent="true"><mi mathvariant="bold-italic">σ</mi><mo>^</mo></mover><mi mathvariant="script">B</mi><mn>2</mn></msubsup></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="script">B</mi><mi mathvariant="normal">∣</mi></mrow></mfrac><munder><mo>∑</mo><mrow><mi mathvariant="bold">x</mi><mo>∈</mo><mi mathvariant="script">B</mi></mrow></munder><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo>−</mo><msub><mover accent="true"><mi mathvariant="bold-italic">μ</mi><mo>^</mo></mover><mi mathvariant="script">B</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mi>ϵ</mi><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} \hat{\boldsymbol{\mu}}_\mathcal{B} &amp;= \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} \mathbf{x},\\ \hat{\boldsymbol{\sigma}}_\mathcal{B}^2 &amp;= \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} (\mathbf{x} - \hat{\boldsymbol{\mu}}_{\mathcal{B}})^2 + \epsilon.\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:5.886291999999999em;vertical-align:-2.6931459999999996em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.1931459999999996em"><span style="top:-5.193146em"><span class="pstrut" style="height:3.32144em"></span><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.70788em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">μ</span></span></span></span></span><span style="top:-3.01344em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.25em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.19444em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.23419099999999995em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:.03041em">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span></span></span><span style="top:-2.2500000000000004em"><span class="pstrut" style="height:3.32144em"></span><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.70788em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.03704em">σ</span></span></span></span></span><span style="top:-3.01344em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.9118879999999999em"><span style="top:-2.4530000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:.03041em">B</span></span></span></span><span style="top:-3.16078em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.6931459999999996em"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.1931459999999996em"><span style="top:-5.193146em"><span class="pstrut" style="height:3.32144em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">∣</span><span class="mord"><span class="mord mathcal" style="margin-right:.03041em">B</span></span><span class="mord">∣</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em"><span style="top:-1.8556639999999998em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathbf mtight">x</span></span><span class="mrel mtight">∈</span><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:.03041em">B</span></span></span></span></span><span style="top:-3.0500049999999996em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.321706em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathbf">x</span></span><span class="mpunct">,</span></span></span><span style="top:-2.2500000000000004em"><span class="pstrut" style="height:3.32144em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">∣</span><span class="mord"><span class="mord mathcal" style="margin-right:.03041em">B</span></span><span class="mord">∣</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em"><span style="top:-1.8556639999999998em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathbf mtight">x</span></span><span class="mrel mtight">∈</span><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:.03041em">B</span></span></span></span></span><span style="top:-3.0500049999999996em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.321706em"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.70788em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">μ</span></span></span></span></span><span style="top:-3.01344em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.25em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.19444em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.23419099999999995em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:.03041em">B</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8641079999999999em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord mathnormal">ϵ</span><span class="mord">.</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.6931459999999996em"><span></span></span></span></span></span></span></span></span></span></span></span></p><p>请注意，我们在方差估计值中添加一个小的常量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon &gt; 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5782em;vertical-align:-.0391em"></span><span class="mord mathnormal">ϵ</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">0</span></span></span></span>，以确保我们永远不会尝试除以零，即使在经验方差估计值可能消失的情况下也是如此。估计值<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi mathvariant="bold-italic">μ</mi><mo>^</mo></mover><mi mathvariant="script">B</mi></msub></mrow><annotation encoding="application/x-tex">\hat{\boldsymbol{\mu}}_\mathcal{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.95202em;vertical-align:-.24414em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.70788em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">μ</span></span></span></span></span><span style="top:-3.01344em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.25em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.19444em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.23419099999999995em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:.03041em">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi mathvariant="bold-italic">σ</mi><mo>^</mo></mover><mi mathvariant="script">B</mi></msub></mrow><annotation encoding="application/x-tex">{\hat{\boldsymbol{\sigma}}_\mathcal{B}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.85788em;vertical-align:-.15em"></span><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.70788em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.03704em">σ</span></span></span></span></span><span style="top:-3.01344em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.32833099999999993em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:.03041em">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></span> 通过使用平均值和方差的噪声（noise）估计来抵消缩放问题。<br>乍看起来，这种噪声是一个问题，而事实上它是有益的。</p><p>事实证明，这是深度学习中一个反复出现的主题。<br>由于尚未在理论上明确的原因，优化中的各种噪声源通常会导致更快的训练和较少的过拟合：这种变化似乎是正则化的一种形式。<br>在一些初步研究中， :cite: <code>Teye.Azizpour.Smith.2018</code> 和 :cite: <code>Luo.Wang.Shao.ea.2018</code> 分别将批量规范化的性质与贝叶斯先验相关联。<br>这些理论揭示了为什么批量规范化最适应<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>50</mn><mo>∼</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">50 \sim 100</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">5</span><span class="mord">0</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span></span></span></span> 范围中的中等批量大小的难题。</p><p>另外，批量规范化层在” 训练模式 “（通过小批量统计数据规范化）和 “预测模式”（通过数据集统计规范化）中的功能不同。<br>在训练过程中，我们无法得知使用整个数据集来估计平均值和方差，所以只能根据每个小批次的平均值和方差不断训练模型。<br>而在预测模式下，可以根据整个数据集精确计算批量规范化所需的平均值和方差。</p><p>现在，我们了解一下批量规范化在实践中是如何工作的。</p><h2 id="批量规范化层"><a class="anchor" href="#批量规范化层">#</a> 批量规范化层</h2><p>回想一下，批量规范化和其他层之间的一个关键区别是，由于批量规范化在完整的小批量上运行，因此我们不能像以前在引入其他层时那样忽略批量大小。<br>我们在下面讨论这两种情况：全连接层和卷积层，他们的批量规范化实现略有不同。</p><h3 id="全连接层"><a class="anchor" href="#全连接层">#</a> 全连接层</h3><p>通常，我们将批量规范化层置于全连接层中的仿射变换和激活函数之间。<br>设全连接层的输入为 x，权重参数和偏置参数分别为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">W</mi></mrow><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68611em;vertical-align:0"></span><span class="mord"><span class="mord mathbf" style="margin-right:.01597em">W</span></span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">b</mi></mrow><annotation encoding="application/x-tex">\mathbf{b}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord"><span class="mord mathbf">b</span></span></span></span></span>，激活函数为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal">ϕ</span></span></span></span>，批量规范化的运算符为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">B</mi><mi mathvariant="normal">N</mi></mrow><annotation encoding="application/x-tex">\mathrm{BN}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord"><span class="mord mathrm">B</span><span class="mord mathrm">N</span></span></span></span></span>。<br>那么，使用批量规范化的全连接层的输出的计算详情如下：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">h</mi><mo>=</mo><mi>ϕ</mi><mo stretchy="false">(</mo><mrow><mi mathvariant="normal">B</mi><mi mathvariant="normal">N</mi></mrow><mo stretchy="false">(</mo><mi mathvariant="bold">W</mi><mi mathvariant="bold">x</mi><mo>+</mo><mi mathvariant="bold">b</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\mathbf{h} = \phi(\mathrm{BN}(\mathbf{W}\mathbf{x} + \mathbf{b}) ).</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord"><span class="mord mathbf">h</span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">ϕ</span><span class="mopen">(</span><span class="mord"><span class="mord mathrm">B</span><span class="mord mathrm">N</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathbf" style="margin-right:.01597em">W</span></span><span class="mord"><span class="mord mathbf">x</span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathbf">b</span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span></p><p>回想一下，均值和方差是在应用变换的 &quot;相同&quot; 小批量上计算的。</p><h3 id="卷积层"><a class="anchor" href="#卷积层">#</a> 卷积层</h3><p>同样，对于卷积层，我们可以在卷积层之后和非线性激活函数之前应用批量规范化。<br>当卷积有多个输出通道时，我们需要对这些通道的 “每个” 输出执行批量规范化，每个通道都有自己的拉伸（scale）和偏移（shift）参数，这两个参数都是标量。<br>假设我们的小批量包含<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">m</span></span></span></span> 个样本，并且对于每个通道，卷积的输出具有高度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal">p</span></span></span></span> 和宽度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.03588em">q</span></span></span></span>。<br>那么对于卷积层，我们在每个输出通道的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>⋅</mo><mi>p</mi><mo>⋅</mo><mi>q</mi></mrow><annotation encoding="application/x-tex">m \cdot p \cdot q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.44445em;vertical-align:0"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.63889em;vertical-align:-.19444em"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.03588em">q</span></span></span></span> 个元素上同时执行每个批量规范化。<br>因此，在计算平均值和方差时，我们会收集所有空间位置的值，然后在给定通道内应用相同的均值和方差，以便在每个空间位置对值进行规范化。</p><h3 id="预测过程中的批量规范化"><a class="anchor" href="#预测过程中的批量规范化">#</a> 预测过程中的批量规范化</h3><p>正如我们前面提到的，批量规范化在训练模式和预测模式下的行为通常不同。<br>首先，将训练好的模型用于预测时，我们不再需要样本均值中的噪声以及在微批次上估计每个小批次产生的样本方差了。<br>其次，例如，我们可能需要使用我们的模型对逐个样本进行预测。<br>一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。<br>可见，和暂退法一样，批量规范化层在训练模式和预测模式下的计算结果也是不一样的。</p><h2 id="从零实现"><a class="anchor" href="#从零实现">#</a> (<strong>从零实现</strong>)</h2><p>下面，我们从头开始实现一个具有张量的批量规范化层。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> torch</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">from</span> torch <span class="token keyword">import</span> nn</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l</pre></td></tr><tr><td data-num="4"></td><td><pre></pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token keyword">def</span> <span class="token function">batch_norm</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> beta<span class="token punctuation">,</span> moving_mean<span class="token punctuation">,</span> moving_var<span class="token punctuation">,</span> eps<span class="token punctuation">,</span> momentum<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    <span class="token comment"># 通过 is_grad_enabled 来判断当前模式是训练模式还是预测模式</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    <span class="token keyword">if</span> <span class="token keyword">not</span> torch<span class="token punctuation">.</span>is_grad_enabled<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="9"></td><td><pre>        <span class="token comment"># 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差</span></pre></td></tr><tr><td data-num="10"></td><td><pre>        X_hat <span class="token operator">=</span> <span class="token punctuation">(</span>X <span class="token operator">-</span> moving_mean<span class="token punctuation">)</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>moving_var <span class="token operator">+</span> eps<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="11"></td><td><pre>    <span class="token keyword">else</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="12"></td><td><pre>        <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="13"></td><td><pre>        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="14"></td><td><pre>            <span class="token comment"># 使用全连接层的情况，计算特征维上的均值和方差</span></pre></td></tr><tr><td data-num="15"></td><td><pre>            mean <span class="token operator">=</span> X<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="16"></td><td><pre>            var <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>X <span class="token operator">-</span> mean<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="17"></td><td><pre>        <span class="token keyword">else</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="18"></td><td><pre>            <span class="token comment"># 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。</span></pre></td></tr><tr><td data-num="19"></td><td><pre>            <span class="token comment"># 这里我们需要保持 X 的形状以便后面可以做广播运算</span></pre></td></tr><tr><td data-num="20"></td><td><pre>            mean <span class="token operator">=</span> X<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="21"></td><td><pre>            var <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>X <span class="token operator">-</span> mean<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="22"></td><td><pre>        <span class="token comment"># 训练模式下，用当前的均值和方差做标准化</span></pre></td></tr><tr><td data-num="23"></td><td><pre>        X_hat <span class="token operator">=</span> <span class="token punctuation">(</span>X <span class="token operator">-</span> mean<span class="token punctuation">)</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>var <span class="token operator">+</span> eps<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="24"></td><td><pre>        <span class="token comment"># 更新移动平均的均值和方差</span></pre></td></tr><tr><td data-num="25"></td><td><pre>        moving_mean <span class="token operator">=</span> momentum <span class="token operator">*</span> moving_mean <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> momentum<span class="token punctuation">)</span> <span class="token operator">*</span> mean</pre></td></tr><tr><td data-num="26"></td><td><pre>        moving_var <span class="token operator">=</span> momentum <span class="token operator">*</span> moving_var <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> momentum<span class="token punctuation">)</span> <span class="token operator">*</span> var</pre></td></tr><tr><td data-num="27"></td><td><pre>    Y <span class="token operator">=</span> gamma <span class="token operator">*</span> X_hat <span class="token operator">+</span> beta  <span class="token comment"># 缩放和移位</span></pre></td></tr><tr><td data-num="28"></td><td><pre>    <span class="token keyword">return</span> Y<span class="token punctuation">,</span> moving_mean<span class="token punctuation">.</span>data<span class="token punctuation">,</span> moving_var<span class="token punctuation">.</span>data</pre></td></tr></table></figure><p>我们现在可以 [<strong>创建一个正确的 <code>BatchNorm</code> 层</strong>]。<br>这个层将保持适当的参数：拉伸 <code>gamma</code> 和偏移 <code>beta</code> , 这两个参数将在训练过程中更新。<br>此外，我们的层将保存均值和方差的移动平均值，以便在模型预测期间随后使用。</p><p>撇开算法细节，注意我们实现层的基础设计模式。<br>通常情况下，我们用一个单独的函数定义其数学原理，比如说 <code>batch_norm</code> 。<br>然后，我们将此功能集成到一个自定义层中，其代码主要处理数据移动到训练设备（如 GPU）、分配和初始化任何必需的变量、跟踪移动平均线（此处为均值和方差）等问题。<br>为了方便起见，我们并不担心在这里自动推断输入形状，因此我们需要指定整个特征的数量。<br>不用担心，深度学习框架中的批量规范化 API 将为我们解决上述问题，我们稍后将展示这一点。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">class</span> <span class="token class-name">BatchNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    <span class="token comment"># num_features：完全连接层的输出数量或卷积层的输出通道数。</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    <span class="token comment"># num_dims：2 表示完全连接层，4 表示卷积层</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_features<span class="token punctuation">,</span> num_dims<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="5"></td><td><pre>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>        <span class="token keyword">if</span> num_dims <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="7"></td><td><pre>            shape <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_features<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>        <span class="token keyword">else</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="9"></td><td><pre>            shape <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_features<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre>        <span class="token comment"># 参与求梯度和迭代的拉伸和偏移参数，分别初始化成 1 和 0</span></pre></td></tr><tr><td data-num="11"></td><td><pre>        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre>        self<span class="token punctuation">.</span>beta <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="13"></td><td><pre>        <span class="token comment"># 非模型参数的变量初始化为 0 和 1</span></pre></td></tr><tr><td data-num="14"></td><td><pre>        self<span class="token punctuation">.</span>moving_mean <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="15"></td><td><pre>        self<span class="token punctuation">.</span>moving_var <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>shape<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="16"></td><td><pre></pre></td></tr><tr><td data-num="17"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="18"></td><td><pre>        <span class="token comment"># 如果 X 不在内存上，将 moving_mean 和 moving_var</span></pre></td></tr><tr><td data-num="19"></td><td><pre>        <span class="token comment"># 复制到 X 所在显存上</span></pre></td></tr><tr><td data-num="20"></td><td><pre>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>moving_mean<span class="token punctuation">.</span>device <span class="token operator">!=</span> X<span class="token punctuation">.</span>device<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="21"></td><td><pre>            self<span class="token punctuation">.</span>moving_mean <span class="token operator">=</span> self<span class="token punctuation">.</span>moving_mean<span class="token punctuation">.</span>to<span class="token punctuation">(</span>X<span class="token punctuation">.</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="22"></td><td><pre>            self<span class="token punctuation">.</span>moving_var <span class="token operator">=</span> self<span class="token punctuation">.</span>moving_var<span class="token punctuation">.</span>to<span class="token punctuation">(</span>X<span class="token punctuation">.</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="23"></td><td><pre>        <span class="token comment"># 保存更新过的 moving_mean 和 moving_var</span></pre></td></tr><tr><td data-num="24"></td><td><pre>        Y<span class="token punctuation">,</span> self<span class="token punctuation">.</span>moving_mean<span class="token punctuation">,</span> self<span class="token punctuation">.</span>moving_var <span class="token operator">=</span> batch_norm<span class="token punctuation">(</span></pre></td></tr><tr><td data-num="25"></td><td><pre>            X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gamma<span class="token punctuation">,</span> self<span class="token punctuation">.</span>beta<span class="token punctuation">,</span> self<span class="token punctuation">.</span>moving_mean<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="26"></td><td><pre>            self<span class="token punctuation">.</span>moving_var<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-5</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="27"></td><td><pre>        <span class="token keyword">return</span> Y</pre></td></tr></table></figure><h2 id="使用批量规范化层的-lenet"><a class="anchor" href="#使用批量规范化层的-lenet">#</a> 使用批量规范化层的 LeNet</h2><p>为了更好理解如何 [<strong>应用 <code>BatchNorm</code> </strong>]，下面我们将其应用 (<strong>于 LeNet 模型</strong>)（ :numref: <code>sec_lenet</code> ）。<br>回想一下，批量规范化是在卷积层或全连接层之后、相应的激活函数之前应用的。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> BatchNorm<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> num_dims<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> BatchNorm<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> num_dims<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token operator">*</span><span class="token number">4</span><span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span> BatchNorm<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> num_dims<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span> BatchNorm<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> num_dims<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>和以前一样，我们将 [<strong>在 Fashion-MNIST 数据集上训练网络</strong>]。<br>这个代码与我们第一次训练 LeNet（ :numref: <code>sec_lenet</code> ）时几乎完全相同，主要区别在于学习率大得多。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">256</span></pre></td></tr><tr><td data-num="2"></td><td><pre>train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data_fashion_mnist<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>d2l<span class="token punctuation">.</span>train_ch6<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> d2l<span class="token punctuation">.</span>try_gpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>loss 0.267, train acc 0.900, test acc 0.860
40878.1 examples/sec on cuda:0
</code></pre><p><img data-src="/batch-norm_files/batch-norm_7_1.svg" alt="svg"></p><p>让我们来看看从第一个批量规范化层中学到的 [<strong>拉伸参数 <code>gamma</code> 和偏移参数 <code>beta</code> </strong>]。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>net<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>gamma<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> net<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>beta<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>(tensor([2.2493, 1.6559, 2.7877, 2.4000, 4.1816, 3.5716], device='cuda:0',
        grad_fn=&lt;ReshapeAliasBackward0&gt;),
 tensor([-1.3887, -0.4102, -1.0248,  2.1779, -2.4067, -3.6746], device='cuda:0',
        grad_fn=&lt;ReshapeAliasBackward0&gt;))
</code></pre><h2 id="简明实现"><a class="anchor" href="#简明实现">#</a> [<strong>简明实现</strong>]</h2><p>除了使用我们刚刚定义的 <code>BatchNorm</code> ，我们也可以直接使用深度学习框架中定义的 <code>BatchNorm</code> 。<br>该代码看起来几乎与我们上面的代码相同。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>下面，我们 [<strong>使用相同超参数来训练模型</strong>]。<br>请注意，通常高级 API 变体运行速度快得多，因为它的代码已编译为 C++ 或 CUDA，而我们的自定义代码由 Python 实现。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>d2l<span class="token punctuation">.</span>train_ch6<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> d2l<span class="token punctuation">.</span>try_gpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>loss 0.263, train acc 0.902, test acc 0.862
71480.6 examples/sec on cuda:0
</code></pre><p><img data-src="/batch-norm_files/batch-norm_13_1.svg" alt="svg"></p><h2 id="争议"><a class="anchor" href="#争议">#</a> 争议</h2><p>直观地说，批量规范化被认为可以使优化更加平滑。<br>然而，我们必须小心区分直觉和对我们观察到的现象的真实解释。<br>回想一下，我们甚至不知道简单的神经网络（多层感知机和传统的卷积神经网络）为什么如此有效。<br>即使在暂退法和权重衰减的情况下，它们仍然非常灵活，因此无法通过常规的学习理论泛化保证来解释它们是否能够泛化到看不见的数据。</p><p>在提出批量规范化的论文中，作者除了介绍了其应用，还解释了其原理：通过减少<em>内部协变量偏移</em>（internal covariate shift）。<br>据推测，作者所说的<em>内部协变量转移</em>类似于上述的投机直觉，即变量值的分布在训练过程中会发生变化。<br>然而，这种解释有两个问题：<br>1、这种偏移与严格定义的<em>协变量偏移</em>（covariate shift）非常不同，所以这个名字用词不当；<br>2、这种解释只提供了一种不明确的直觉，但留下了一个有待后续挖掘的问题：为什么这项技术如此有效？<br>本书旨在传达实践者用来发展深层神经网络的直觉。<br>然而，重要的是将这些指导性直觉与既定的科学事实区分开来。<br>最终，当你掌握了这些方法，并开始撰写自己的研究论文时，你会希望清楚地区分技术和直觉。</p><p>随着批量规范化的普及，<em>内部协变量偏移</em>的解释反复出现在技术文献的辩论，特别是关于 “如何展示机器学习研究” 的更广泛的讨论中。<br>Ali Rahimi 在接受 2017 年 NeurIPS 大会的 “接受时间考验奖”（Test of Time Award）时发表了一篇令人难忘的演讲。他将 “内部协变量转移” 作为焦点，将现代深度学习的实践比作炼金术。<br>他对该示例进行了详细回顾 :cite: <code>Lipton.Steinhardt.2018</code> ，概述了机器学习中令人不安的趋势。<br>此外，一些作者对批量规范化的成功提出了另一种解释：在某些方面，批量规范化的表现出与原始论文 :cite: <code>Santurkar.Tsipras.Ilyas.ea.2018</code> 中声称的行为是相反的。</p><p>然而，与机器学习文献中成千上万类似模糊的说法相比，内部协变量偏移没有更值得批评。<br>很可能，它作为这些辩论的焦点而产生共鸣，要归功于目标受众对它的广泛认可。<br>批量规范化已经被证明是一种不可或缺的方法。它适用于几乎所有图像分类器，并在学术界获得了数万引用。</p><h2 id="小结"><a class="anchor" href="#小结">#</a> 小结</h2><ul><li>在模型训练过程中，批量规范化利用小批量的均值和标准差，不断调整神经网络的中间输出，使整个神经网络各层的中间输出值更加稳定。</li><li>批量规范化在全连接层和卷积层的使用略有不同。</li><li>批量规范化层和暂退层一样，在训练模式和预测模式下计算不同。</li><li>批量规范化有许多有益的副作用，主要是正则化。另一方面，” 减少内部协变量偏移 “的原始动机似乎不是一个有效的解释。</li></ul><h2 id="练习"><a class="anchor" href="#练习">#</a> 练习</h2><ol><li>在使用批量规范化之前，我们是否可以从全连接层或卷积层中删除偏置参数？为什么？</li><li>比较 LeNet 在使用和不使用批量规范化情况下的学习率。<ol><li>绘制训练和测试准确度的提高。</li><li>学习率有多高？</li></ol></li><li>我们是否需要在每个层中进行批量规范化？尝试一下？</li><li>可以通过批量规范化来替换暂退法吗？行为会如何改变？</li><li>确定参数 <code>beta</code> 和 <code>gamma</code> ，并观察和分析结果。</li><li>查看高级 API 中有关 <code>BatchNorm</code> 的在线文档，以查看其他批量规范化的应用。</li><li>研究思路：可以应用的其他 “规范化” 转换？可以应用概率积分变换吗？全秩协方差估计可以么？</li></ol><p><span class="exturl" data-url="aHR0cHM6Ly9kaXNjdXNzLmQybC5haS90LzE4NzQ=">Discussions</span></p><div class="tags"><a href="/tags/chapter-convolutional-modern/" rel="tag"><i class="ic i-tag"></i> chapter_convolutional-modern</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2023-03-04 13:38:25" itemprop="dateModified" datetime="2023-03-04T13:38:25+08:00">2023-03-04</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="yuan 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="yuan 支付宝"><p>支付宝</p></div><div><img data-src="/images/paypal.png" alt="yuan 贝宝"><p>贝宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>yuan <i class="ic i-at"><em>@</em></i>yuan</li><li class="link"><strong>本文链接：</strong> <a href="https://jyuanhust.github.io/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-modern/batch-norm/" title="batch-norm">https://jyuanhust.github.io/2023/02/15/ai/pytorch深度学习/chapter_convolutional-modern/batch-norm/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-modern/densenet/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;gitee.com&#x2F;zkz0&#x2F;image&#x2F;raw&#x2F;master&#x2F;img&#x2F;img(18).webp" title="densenet"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> chapter_convolutional-modern</span><h3>densenet</h3></a></div><div class="item right"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-neural-networks/padding-and-strides/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;gitee.com&#x2F;zkz0&#x2F;image&#x2F;raw&#x2F;master&#x2F;img&#x2F;img(14).webp" title="padding-and-strides"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> chapter_convolutional-neural-networks</span><h3>padding-and-strides</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E8%A7%84%E8%8C%83%E5%8C%96"><span class="toc-number">1.</span> <span class="toc-text">批量规范化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%B7%B1%E5%B1%82%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.</span> <span class="toc-text">训练深层网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E8%A7%84%E8%8C%83%E5%8C%96%E5%B1%82"><span class="toc-number">1.2.</span> <span class="toc-text">批量规范化层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82"><span class="toc-number">1.2.1.</span> <span class="toc-text">全连接层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">1.2.2.</span> <span class="toc-text">卷积层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E6%89%B9%E9%87%8F%E8%A7%84%E8%8C%83%E5%8C%96"><span class="toc-number">1.2.3.</span> <span class="toc-text">预测过程中的批量规范化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.3.</span> <span class="toc-text">(从零实现)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%89%B9%E9%87%8F%E8%A7%84%E8%8C%83%E5%8C%96%E5%B1%82%E7%9A%84-lenet"><span class="toc-number">1.4.</span> <span class="toc-text">使用批量规范化层的 LeNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E6%98%8E%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.5.</span> <span class="toc-text">[简明实现]</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%89%E8%AE%AE"><span class="toc-number">1.6.</span> <span class="toc-text">争议</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.7.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0"><span class="toc-number">1.8.</span> <span class="toc-text">练习</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li class="active"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-modern/batch-norm/" rel="bookmark" title="batch-norm">batch-norm</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-modern/densenet/" rel="bookmark" title="densenet">densenet</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-modern/index/" rel="bookmark" title="index">index</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-modern/alexnet/" rel="bookmark" title="alexnet">alexnet</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-modern/googlenet/" rel="bookmark" title="googlenet">googlenet</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-modern/nin/" rel="bookmark" title="nin">nin</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-modern/resnet/" rel="bookmark" title="resnet">resnet</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-modern/vgg/" rel="bookmark" title="vgg">vgg</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="yuan" data-src="/images/avatar.jpg"><p class="name" itemprop="name">yuan</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">429</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">72</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">61</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item email" data-url="bWFpbHRvOjIwODM2MzU1MjVAcXEuY29t" title="mailto:2083635525@qq.com"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li><li class="item"><a href="/links/" rel="section"><i class="ic i-magic"></i>链接</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-modern/densenet/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-neural-networks/padding-and-strides/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-optimization/" title="分类于 chapter_optimization">chapter_optimization</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_optimization/momentum/" title="momentum">momentum</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/frontend/" title="分类于 前端">前端</a></div><span><a href="/2022/08/09/frontend/base/TypeScript/" title="TypeScript">TypeScript</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2023/09/07/computer-science/algorithm/%E5%8D%8E%E7%A7%91%E6%9C%BA%E8%AF%95/note/" title="未命名">未命名</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-natural-language-processing-pretraining/" title="分类于 chapter_natural-language-processing-pretraining">chapter_natural-language-processing-pretraining</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_natural-language-processing-pretraining/bert-dataset/" title="bert-dataset">bert-dataset</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-multilayer-perceptrons/" title="分类于 chapter_multilayer-perceptrons">chapter_multilayer-perceptrons</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_multilayer-perceptrons/dropout/" title="dropout">dropout</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2023/06/25/computer-science/base/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E9%AA%8C/note/" title="未命名">未命名</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-natural-language-processing-applications/" title="分类于 chapter_natural-language-processing-applications">chapter_natural-language-processing-applications</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_natural-language-processing-applications/sentiment-analysis-and-dataset/" title="sentiment-analysis-and-dataset">sentiment-analysis-and-dataset</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-appendix-tools-for-deep-learning/" title="分类于 chapter_appendix-tools-for-deep-learning">chapter_appendix-tools-for-deep-learning</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_appendix-tools-for-deep-learning/selecting-servers-gpus/" title="selecting-servers-gpus">selecting-servers-gpus</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2023/06/25/computer-science/%E6%AF%94%E8%B5%9B/%E5%A4%A7%E5%94%90%E6%9D%AF/%E4%BB%BB%E5%8A%A11-5G%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E4%B8%8E%E7%BB%84%E7%BD%91%E9%83%A8%E7%BD%B2/" title="未命名">未命名</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2022/07/24/computer-science/base/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%9E%E9%AA%8C/828/lab1-Xv6andUnixutilities/" title="Xv6 and Unix utilities">Xv6 and Unix utilities</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">yuan @ Mi Manchi</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">2.9m 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">44:38</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2023/02/15/ai/pytorch深度学习/chapter_convolutional-modern/batch-norm/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html>