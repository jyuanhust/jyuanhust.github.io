<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="yuan" href="https://jyuanhust.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="yuan" href="https://jyuanhust.github.io/atom.xml"><link rel="alternate" type="application/json" title="yuan" href="https://jyuanhust.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="chapter_computational-performance"><link rel="canonical" href="https://jyuanhust.github.io/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/multiple-gpus/"><title>multiple-gpus - chapter_computational-performance - pytorch深度学习 - ai | Mi Manchi = yuan = Whatever is worth doing at all is worth doing well</title><meta name="generator" content="Hexo 6.2.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">multiple-gpus</h1><div class="meta"><span class="item" title="创建时间：2023-02-15 00:00:00"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2023-02-15T00:00:00+08:00">2023-02-15</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>7.5k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>7 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Mi Manchi</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(61).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(3).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(97).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(96).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(18).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(29).webp"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/" itemprop="item" rel="index" title="分类于 ai"><span itemprop="name">ai</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="item" rel="index" title="分类于 pytorch深度学习"><span itemprop="name">pytorch深度学习</span></a><meta itemprop="position" content="2"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-computational-performance/" itemprop="item" rel="index" title="分类于 chapter_computational-performance"><span itemprop="name">chapter_computational-performance</span></a><meta itemprop="position" content="3"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://jyuanhust.github.io/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/multiple-gpus/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="yuan"><meta itemprop="description" content="Whatever is worth doing at all is worth doing well, "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="yuan"></span><div class="body md" itemprop="articleBody"><h1 id="多gpu训练"><a class="anchor" href="#多gpu训练">#</a> 多 GPU 训练</h1><p>🏷 <code>sec_multi_gpu</code></p><p>到目前为止，我们讨论了如何在 CPU 和 GPU 上高效地训练模型，同时在 :numref: <code>sec_auto_para</code> 中展示了深度学习框架如何在 CPU 和 GPU 之间自动地并行化计算和通信，还在 :numref: <code>sec_use_gpu</code> 中展示了如何使用 <code>nvidia-smi</code> 命令列出计算机上所有可用的 GPU。<br>但是我们没有讨论如何真正实现深度学习训练的并行化。<br>是否一种方法，以某种方式分割数据到多个设备上，并使其能够正常工作呢？<br>本节将详细介绍如何从零开始并行地训练网络，<br>这里需要运用小批量随机梯度下降算法（详见 :numref: <code>sec_minibatch_sgd</code> ）。<br>后面我还讲介绍如何使用高级 API 并行训练网络（请参阅 :numref: <code>sec_multi_gpu_concise</code> ）。</p><h2 id="问题拆分"><a class="anchor" href="#问题拆分">#</a> 问题拆分</h2><p>我们从一个简单的计算机视觉问题和一个稍稍过时的网络开始。<br>这个网络有多个卷积层和汇聚层，最后可能有几个全连接的层，看起来非常类似于 LeNet :cite: <code>LeCun.Bottou.Bengio.ea.1998</code> 或 AlexNet :cite: <code>Krizhevsky.Sutskever.Hinton.2012</code> 。<br>假设我们有多个 GPU（如果是桌面服务器则有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span></span></span></span> 个，AWS g4dn.12xlarge 上有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">4</span></span></span></span> 个，p3.16xlarge 上有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn></mrow><annotation encoding="application/x-tex">8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">8</span></span></span></span> 个，p2.16xlarge 上有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn></mrow><annotation encoding="application/x-tex">16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">6</span></span></span></span> 个）。<br>我们希望以一种方式对训练进行拆分，为实现良好的加速比，还能同时受益于简单且可重复的设计选择。<br>毕竟，多个 GPU 同时增加了内存和计算能力。<br>简而言之，对于需要分类的小批量训练数据，我们有以下选择。</p><p>第一种方法，在多个 GPU 之间拆分网络。<br>也就是说，每个 GPU 将流入特定层的数据作为输入，跨多个后续层对数据进行处理，然后将数据发送到下一个 GPU。<br>与单个 GPU 所能处理的数据相比，我们可以用更大的网络处理数据。<br>此外，每个 GPU 占用的<em>显存</em>（memory footprint）可以得到很好的控制，虽然它只是整个网络显存的一小部分。</p><p>然而，GPU 的接口之间需要的密集同步可能是很难办的，特别是层之间计算的工作负载不能正确匹配的时候，<br>还有层之间的接口需要大量的数据传输的时候（例如：激活值和梯度，数据量可能会超出 GPU 总线的带宽）。<br>此外，计算密集型操作的顺序对拆分来说也是非常重要的，这方面的最好研究可参见 :cite: <code>Mirhoseini.Pham.Le.ea.2017</code> ，其本质仍然是一个困难的问题，目前还不清楚研究是否能在特定问题上实现良好的线性缩放。<br>综上所述，除非存框架或操作系统本身支持将多个 GPU 连接在一起，否则不建议这种方法。</p><p>第二种方法，拆分层内的工作。<br>例如，将问题分散到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">4</span></span></span></span> 个 GPU，每个 GPU 生成<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn></mrow><annotation encoding="application/x-tex">16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">6</span></span></span></span> 个通道的数据，而不是在单个 GPU 上计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>64</mn></mrow><annotation encoding="application/x-tex">64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">6</span><span class="mord">4</span></span></span></span> 个通道。<br>对于全连接的层，同样可以拆分输出单元的数量。<br>:numref: <code>fig_alexnet_original</code> 描述了这种设计，其策略用于处理显存非常小（当时为 2GB）的 GPU。<br>当通道或单元的数量不太小时，使计算性能有良好的提升。<br>此外，由于可用的显存呈线性扩展，多个 GPU 能够处理不断变大的网络。</p><p><img data-src="/./images/alexnet-original.svg" alt="由于GPU显存有限，原有AlexNet设计中的模型并行"><br>🏷 <code>fig_alexnet_original</code></p><p>然而，我们需要大量的同步或<em>屏障操作</em>（barrier operation），因为每一层都依赖于所有其他层的结果。<br>此外，需要传输的数据量也可能比跨 GPU 拆分层时还要大。<br>因此，基于带宽的成本和复杂性，我们同样不推荐这种方法。</p><p>最后一种方法，跨多个 GPU 对数据进行拆分。<br>这种方式下，所有 GPU 尽管有不同的观测结果，但是执行着相同类型的工作。<br>在完成每个小批量数据的训练之后，梯度在 GPU 上聚合。<br>这种方法最简单，并可以应用于任何情况，同步只需要在每个小批量数据处理之后进行。<br>也就是说，当其他梯度参数仍在计算时，完成计算的梯度参数就可以开始交换。<br>而且，GPU 的数量越多，小批量包含的数据量就越大，从而就能提高训练效率。<br>但是，添加更多的 GPU 并不能让我们训练更大的模型。</p><p><img data-src="/./images/splitting.svg" alt="在多个GPU上并行化。从左到右：原始问题、网络并行、分层并行、数据并行"><br>🏷 <code>fig_splitting</code></p><p>:numref: <code>fig_splitting</code> 中比较了多个 GPU 上不同的并行方式。<br>总体而言，只要 GPU 的显存足够大，数据并行是最方便的。<br>有关分布式训练分区的详细描述，请参见 :cite: <code>Li.Andersen.Park.ea.2014</code> 。<br>在深度学习的早期，GPU 的显存曾经是一个棘手的问题，然而如今除了非常特殊的情况，这个问题已经解决。<br>下面我们将重点讨论数据并行性。</p><h2 id="数据并行性"><a class="anchor" href="#数据并行性">#</a> 数据并行性</h2><p>假设一台机器有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span></span></span></span> 个 GPU。<br>给定需要训练的模型，虽然每个 GPU 上的参数值都是相同且同步的，但是每个 GPU 都将独立地维护一组完整的模型参数。<br>例如， :numref: <code>fig_data_parallel</code> 演示了在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">k=2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span></span></span></span> 时基于数据并行方法训练模型。</p><p><img data-src="/./images/data-parallel.svg" alt="利用两个GPU上的数据，并行计算小批量随机梯度下降"><br>🏷 <code>fig_data_parallel</code></p><p>一般来说，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span></span></span></span> 个 GPU 并行训练过程如下：</p><ul><li>在任何一次训练迭代中，给定的随机的小批量样本都将被分成<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span></span></span></span> 个部分，并均匀地分配到 GPU 上；</li><li>每个 GPU 根据分配给它的小批量子集，计算模型参数的损失和梯度；</li><li>将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span></span></span></span> 个 GPU 中的局部梯度聚合，以获得当前小批量的随机梯度；</li><li>聚合梯度被重新分发到每个 GPU 中；</li><li>每个 GPU 使用这个小批量随机梯度，来更新它所维护的完整的模型参数集。</li></ul><p>在实践中请注意，当在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span></span></span></span> 个 GPU 上训练时，需要扩大小批量的大小为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span></span></span></span> 的倍数，这样每个 GPU 都有相同的工作量，就像只在单个 GPU 上训练一样。<br>因此，在 16-GPU 服务器上可以显著地增加小批量数据量的大小，同时可能还需要相应地提高学习率。<br>还请注意， :numref: <code>sec_batch_norm</code> 中的批量规范化也需要调整，例如，为每个 GPU 保留单独的批量规范化参数。</p><p>下面我们将使用一个简单网络来演示多 GPU 训练。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token operator">%</span>matplotlib inline</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> torch</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">from</span> torch <span class="token keyword">import</span> nn</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l</pre></td></tr></table></figure><h2 id="简单网络"><a class="anchor" href="#简单网络">#</a> [<strong>简单网络</strong>]</h2><p>我们使用 :numref: <code>sec_lenet</code> 中介绍的（稍加修改的）LeNet，<br>从零开始定义它，从而详细说明参数交换和同步。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># 初始化模型参数</span></pre></td></tr><tr><td data-num="2"></td><td><pre>scale <span class="token operator">=</span> <span class="token number">0.01</span></pre></td></tr><tr><td data-num="3"></td><td><pre>W1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> scale</pre></td></tr><tr><td data-num="4"></td><td><pre>b1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>W2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> scale</pre></td></tr><tr><td data-num="6"></td><td><pre>b2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">50</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>W3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">800</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> scale</pre></td></tr><tr><td data-num="8"></td><td><pre>b3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre>W4 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> scale</pre></td></tr><tr><td data-num="10"></td><td><pre>b4 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="11"></td><td><pre>params <span class="token operator">=</span> <span class="token punctuation">[</span>W1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> W3<span class="token punctuation">,</span> b3<span class="token punctuation">,</span> W4<span class="token punctuation">,</span> b4<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="12"></td><td><pre></pre></td></tr><tr><td data-num="13"></td><td><pre><span class="token comment"># 定义模型</span></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token keyword">def</span> <span class="token function">lenet</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> params<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="15"></td><td><pre>    h1_conv <span class="token operator">=</span> F<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span>X<span class="token punctuation">,</span> weight<span class="token operator">=</span>params<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>params<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="16"></td><td><pre>    h1_activation <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>h1_conv<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="17"></td><td><pre>    h1 <span class="token operator">=</span> F<span class="token punctuation">.</span>avg_pool2d<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span>h1_activation<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="18"></td><td><pre>    h2_conv <span class="token operator">=</span> F<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span>h1<span class="token punctuation">,</span> weight<span class="token operator">=</span>params<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>params<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="19"></td><td><pre>    h2_activation <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>h2_conv<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="20"></td><td><pre>    h2 <span class="token operator">=</span> F<span class="token punctuation">.</span>avg_pool2d<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span>h2_activation<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="21"></td><td><pre>    h2 <span class="token operator">=</span> h2<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>h2<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="22"></td><td><pre>    h3_linear <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>h2<span class="token punctuation">,</span> params<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> params<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="23"></td><td><pre>    h3 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>h3_linear<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="24"></td><td><pre>    y_hat <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>h3<span class="token punctuation">,</span> params<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> params<span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="25"></td><td><pre>    <span class="token keyword">return</span> y_hat</pre></td></tr><tr><td data-num="26"></td><td><pre></pre></td></tr><tr><td data-num="27"></td><td><pre><span class="token comment"># 交叉熵损失函数</span></pre></td></tr><tr><td data-num="28"></td><td><pre>loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span></pre></td></tr></table></figure><h2 id="数据同步"><a class="anchor" href="#数据同步">#</a> 数据同步</h2><p>对于高效的多 GPU 训练，我们需要两个基本操作。<br>首先，我们需要 [<strong>向多个设备分发参数</strong>] 并附加梯度（ <code>get_params</code> ）。<br>如果没有参数，就不可能在 GPU 上评估网络。<br>第二，需要跨多个设备对参数求和，也就是说，需要一个 <code>allreduce</code> 函数。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">def</span> <span class="token function">get_params</span><span class="token punctuation">(</span>params<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    new_params <span class="token operator">=</span> <span class="token punctuation">[</span>p<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> params<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    <span class="token keyword">for</span> p <span class="token keyword">in</span> new_params<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="4"></td><td><pre>        p<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    <span class="token keyword">return</span> new_params</pre></td></tr></table></figure><p>通过将模型参数复制到一个 GPU。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>new_params <span class="token operator">=</span> get_params<span class="token punctuation">(</span>params<span class="token punctuation">,</span> d2l<span class="token punctuation">.</span>try_gpu<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b1 权重:'</span><span class="token punctuation">,</span> new_params<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b1 梯度:'</span><span class="token punctuation">,</span> new_params<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grad<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>b1 权重: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
b1 梯度: None
</code></pre><p>由于还没有进行任何计算，因此权重参数的梯度仍然为零。<br>假设现在有一个向量分布在多个 GPU 上，下面的 [<strong> <code>allreduce</code> 函数将所有向量相加，并将结果广播给所有 GPU</strong>]。<br>请注意，我们需要将数据复制到累积结果的设备，才能使函数正常工作。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">def</span> <span class="token function">allreduce</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="3"></td><td><pre>        data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">+=</span> data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="5"></td><td><pre>        data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>device<span class="token punctuation">)</span></pre></td></tr></table></figure><p>通过在不同设备上创建具有不同值的向量并聚合它们。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>data <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>d2l<span class="token punctuation">.</span>try_gpu<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'allreduce之前：\n'</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'\n'</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>allreduce<span class="token punctuation">(</span>data<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'allreduce之后：\n'</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'\n'</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>allreduce之前：
 tensor([[1., 1.]], device='cuda:0') 
 tensor([[2., 2.]], device='cuda:1')
allreduce之后：
 tensor([[3., 3.]], device='cuda:0') 
 tensor([[3., 3.]], device='cuda:1')
</code></pre><h2 id="数据分发"><a class="anchor" href="#数据分发">#</a> 数据分发</h2><p>我们需要一个简单的工具函数，[<strong>将一个小批量数据均匀地分布在多个 GPU 上</strong>]。<br>例如，有两个 GPU 时，我们希望每个 GPU 可以复制一半的数据。<br>因为深度学习框架的内置函数编写代码更方便、更简洁，所以在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">4 \times 5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">4</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">5</span></span></span></span> 矩阵上使用它进行尝试。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>data <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>devices <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="3"></td><td><pre>split <span class="token operator">=</span> nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>data<span class="token punctuation">,</span> devices<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'input :'</span><span class="token punctuation">,</span> data<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'load into'</span><span class="token punctuation">,</span> devices<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'output:'</span><span class="token punctuation">,</span> split<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>input : tensor([[ 0,  1,  2,  3,  4],
        [ 5,  6,  7,  8,  9],
        [10, 11, 12, 13, 14],
        [15, 16, 17, 18, 19]])
load into [device(type='cuda', index=0), device(type='cuda', index=1)]
output: (tensor([[0, 1, 2, 3, 4],
        [5, 6, 7, 8, 9]], device='cuda:0'), tensor([[10, 11, 12, 13, 14],
        [15, 16, 17, 18, 19]], device='cuda:1'))
</code></pre><p>为了方便以后复用，我们定义了可以同时拆分数据和标签的 <code>split_batch</code> 函数。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">#@save</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">def</span> <span class="token function">split_batch</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> devices<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    <span class="token triple-quoted-string string">"""将X和y拆分到多个设备上"""</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    <span class="token keyword">assert</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    <span class="token keyword">return</span> <span class="token punctuation">(</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>X<span class="token punctuation">,</span> devices<span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="6"></td><td><pre>            nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>y<span class="token punctuation">,</span> devices<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></table></figure><h2 id="训练"><a class="anchor" href="#训练">#</a> 训练</h2><p>现在我们可以 [<strong>在一个小批量上实现多 GPU 训练</strong>]。<br>在多个 GPU 之间同步数据将使用刚才讨论的辅助函数 <code>allreduce</code> 和 <code>split_and_load</code> 。<br>我们不需要编写任何特定的代码来实现并行性。<br>因为计算图在小批量内的设备之间没有任何依赖关系，因此它是 “自动地” 并行执行。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">def</span> <span class="token function">train_batch</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> device_params<span class="token punctuation">,</span> devices<span class="token punctuation">,</span> lr<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    X_shards<span class="token punctuation">,</span> y_shards <span class="token operator">=</span> split_batch<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> devices<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    <span class="token comment"># 在每个 GPU 上分别计算损失</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    ls <span class="token operator">=</span> <span class="token punctuation">[</span>loss<span class="token punctuation">(</span>lenet<span class="token punctuation">(</span>X_shard<span class="token punctuation">,</span> device_W<span class="token punctuation">)</span><span class="token punctuation">,</span> y_shard<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>          <span class="token keyword">for</span> X_shard<span class="token punctuation">,</span> y_shard<span class="token punctuation">,</span> device_W <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span></pre></td></tr><tr><td data-num="6"></td><td><pre>              X_shards<span class="token punctuation">,</span> y_shards<span class="token punctuation">,</span> device_params<span class="token punctuation">)</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    <span class="token keyword">for</span> l <span class="token keyword">in</span> ls<span class="token punctuation">:</span>  <span class="token comment"># 反向传播在每个 GPU 上分别执行</span></pre></td></tr><tr><td data-num="8"></td><td><pre>        l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    <span class="token comment"># 将每个 GPU 的所有梯度相加，并将其广播到所有 GPU</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="11"></td><td><pre>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>device_params<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="12"></td><td><pre>            allreduce<span class="token punctuation">(</span></pre></td></tr><tr><td data-num="13"></td><td><pre>                <span class="token punctuation">[</span>device_params<span class="token punctuation">[</span>c<span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>grad <span class="token keyword">for</span> c <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>devices<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="14"></td><td><pre>    <span class="token comment"># 在每个 GPU 上分别更新模型参数</span></pre></td></tr><tr><td data-num="15"></td><td><pre>    <span class="token keyword">for</span> param <span class="token keyword">in</span> device_params<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="16"></td><td><pre>        d2l<span class="token punctuation">.</span>sgd<span class="token punctuation">(</span>param<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 在这里，我们使用全尺寸的小批量</span></pre></td></tr></table></figure><p>现在，我们可以 [<strong>定义训练函数</strong>]。<br>与前几章中略有不同：训练函数需要分配 GPU 并将所有模型参数复制到所有设备。<br>显然，每个小批量都是使用 <code>train_batch</code> 函数来处理多个 GPU。<br>我们只在一个 GPU 上计算模型的精确度，而让其他 GPU 保持空闲，尽管这是相对低效的，但是使用方便且代码简洁。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>num_gpus<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> lr<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data_fashion_mnist<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    devices <span class="token operator">=</span> <span class="token punctuation">[</span>d2l<span class="token punctuation">.</span>try_gpu<span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_gpus<span class="token punctuation">)</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    <span class="token comment"># 将模型参数复制到 num_gpus 个 GPU</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    device_params <span class="token operator">=</span> <span class="token punctuation">[</span>get_params<span class="token punctuation">(</span>params<span class="token punctuation">,</span> d<span class="token punctuation">)</span> <span class="token keyword">for</span> d <span class="token keyword">in</span> devices<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    num_epochs <span class="token operator">=</span> <span class="token number">10</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    animator <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Animator<span class="token punctuation">(</span><span class="token string">'epoch'</span><span class="token punctuation">,</span> <span class="token string">'test acc'</span><span class="token punctuation">,</span> xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    timer <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Timer<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="10"></td><td><pre>        timer<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="11"></td><td><pre>        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="12"></td><td><pre>            <span class="token comment"># 为单个小批量执行多 GPU 训练</span></pre></td></tr><tr><td data-num="13"></td><td><pre>            train_batch<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> device_params<span class="token punctuation">,</span> devices<span class="token punctuation">,</span> lr<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="14"></td><td><pre>            torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>synchronize<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="15"></td><td><pre>        timer<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="16"></td><td><pre>        <span class="token comment"># 在 GPU0 上评估模型</span></pre></td></tr><tr><td data-num="17"></td><td><pre>        animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>evaluate_accuracy_gpu<span class="token punctuation">(</span></pre></td></tr><tr><td data-num="18"></td><td><pre>            <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> lenet<span class="token punctuation">(</span>x<span class="token punctuation">,</span> device_params<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> devices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="19"></td><td><pre>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'测试精度：</span><span class="token interpolation"><span class="token punctuation">&#123;</span>animator<span class="token punctuation">.</span>Y<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">&#125;</span></span><span class="token string">，</span><span class="token interpolation"><span class="token punctuation">&#123;</span>timer<span class="token punctuation">.</span>avg<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.1f</span><span class="token punctuation">&#125;</span></span><span class="token string">秒/轮，'</span></span></pre></td></tr><tr><td data-num="20"></td><td><pre>          <span class="token string-interpolation"><span class="token string">f'在</span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">str</span><span class="token punctuation">(</span>devices<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>让我们看看 [<strong>在单个 GPU 上运行</strong>] 效果得有多好。<br>首先使用的批量大小是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>256</mn></mrow><annotation encoding="application/x-tex">256</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span></span></span></span>，学习率是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.2</mn></mrow><annotation encoding="application/x-tex">0.2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span></span></span></span>。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>train<span class="token punctuation">(</span>num_gpus<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>测试精度：0.84，2.4秒/轮，在[device(type='cuda', index=0)]
</code></pre><p><img data-src="/multiple-gpus_files/multiple-gpus_21_1.svg" alt="svg"></p><p>保持批量大小和学习率不变，并 [<strong>增加为 2 个 GPU</strong>]，我们可以看到测试精度与之前的实验基本相同。<br>不同的 GPU 个数在算法寻优方面是相同的。<br>不幸的是，这里没有任何有意义的加速：模型实在太小了；而且数据集也太小了。在这个数据集中，我们实现的多 GPU 训练的简单方法受到了巨大的 Python 开销的影响。<br>在未来，我们将遇到更复杂的模型和更复杂的并行化方法。<br>尽管如此，让我们看看 Fashion-MNIST 数据集上会发生什么。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>train<span class="token punctuation">(</span>num_gpus<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>测试精度：0.83，2.5秒/轮，在[device(type='cuda', index=0), device(type='cuda', index=1)]
</code></pre><p><img data-src="/multiple-gpus_files/multiple-gpus_23_1.svg" alt="svg"></p><h2 id="小结"><a class="anchor" href="#小结">#</a> 小结</h2><ul><li>有多种方法可以在多个 GPU 上拆分深度网络的训练。拆分可以在层之间、跨层或跨数据上实现。前两者需要对数据传输过程进行严格编排，而最后一种则是最简单的策略。</li><li>数据并行训练本身是不复杂的，它通过增加有效的小批量数据量的大小提高了训练效率。</li><li>在数据并行中，数据需要跨多个 GPU 拆分，其中每个 GPU 执行自己的前向传播和反向传播，随后所有的梯度被聚合为一，之后聚合结果向所有的 GPU 广播。</li><li>小批量数据量更大时，学习率也需要稍微提高一些。</li></ul><h2 id="练习"><a class="anchor" href="#练习">#</a> 练习</h2><ol><li>在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span></span></span></span> 个 GPU 上进行训练时，将批量大小从<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">b</span></span></span></span> 更改为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>⋅</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">k \cdot b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">b</span></span></span></span>，即按 GPU 的数量进行扩展。</li><li>比较不同学习率时模型的精确度，随着 GPU 数量的增加学习率应该如何扩展？</li><li>实现一个更高效的 <code>allreduce</code> 函数用于在不同的 GPU 上聚合不同的参数？为什么这样的效率更高？</li><li>实现模型在多 GPU 下测试精度的计算。</li></ol><p><span class="exturl" data-url="aHR0cHM6Ly9kaXNjdXNzLmQybC5haS90LzI4MDA=">Discussions</span></p><div class="tags"><a href="/tags/chapter-computational-performance/" rel="tag"><i class="ic i-tag"></i> chapter_computational-performance</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2023-03-04 13:38:25" itemprop="dateModified" datetime="2023-03-04T13:38:25+08:00">2023-03-04</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="yuan 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="yuan 支付宝"><p>支付宝</p></div><div><img data-src="/images/paypal.png" alt="yuan 贝宝"><p>贝宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>yuan <i class="ic i-at"><em>@</em></i>yuan</li><li class="link"><strong>本文链接：</strong> <a href="https://jyuanhust.github.io/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/multiple-gpus/" title="multiple-gpus">https://jyuanhust.github.io/2023/02/15/ai/pytorch深度学习/chapter_computational-performance/multiple-gpus/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_attention-mechanisms/bahdanau-attention/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;gitee.com&#x2F;zkz0&#x2F;image&#x2F;raw&#x2F;master&#x2F;img&#x2F;img(15).webp" title="bahdanau-attention"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> chapter_attention-mechanisms</span><h3>bahdanau-attention</h3></a></div><div class="item right"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_attention-mechanisms/attention-cues/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;gitee.com&#x2F;zkz0&#x2F;image&#x2F;raw&#x2F;master&#x2F;img&#x2F;img(64).webp" title="attention-cues"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> chapter_attention-mechanisms</span><h3>attention-cues</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9Agpu%E8%AE%AD%E7%BB%83"><span class="toc-number">1.</span> <span class="toc-text">多 GPU 训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E6%8B%86%E5%88%86"><span class="toc-number">1.1.</span> <span class="toc-text">问题拆分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E6%80%A7"><span class="toc-number">1.2.</span> <span class="toc-text">数据并行性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E7%BD%91%E7%BB%9C"><span class="toc-number">1.3.</span> <span class="toc-text">[简单网络]</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5"><span class="toc-number">1.4.</span> <span class="toc-text">数据同步</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%88%86%E5%8F%91"><span class="toc-number">1.5.</span> <span class="toc-text">数据分发</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">1.6.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.7.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0"><span class="toc-number">1.8.</span> <span class="toc-text">练习</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/async-computation/" rel="bookmark" title="async-computation">async-computation</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/auto-parallelism/" rel="bookmark" title="auto-parallelism">auto-parallelism</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/hardware/" rel="bookmark" title="hardware">hardware</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/hybridize/" rel="bookmark" title="hybridize">hybridize</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/index/" rel="bookmark" title="index">index</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/parameterserver/" rel="bookmark" title="parameterserver">parameterserver</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/multiple-gpus-concise/" rel="bookmark" title="multiple-gpus-concise">multiple-gpus-concise</a></li><li class="active"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/multiple-gpus/" rel="bookmark" title="multiple-gpus">multiple-gpus</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="yuan" data-src="/images/avatar.jpg"><p class="name" itemprop="name">yuan</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">429</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">72</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">61</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item email" data-url="bWFpbHRvOjIwODM2MzU1MjVAcXEuY29t" title="mailto:2083635525@qq.com"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li><li class="item"><a href="/links/" rel="section"><i class="ic i-magic"></i>链接</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_attention-mechanisms/bahdanau-attention/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_attention-mechanisms/attention-cues/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-recurrent-modern/" title="分类于 chapter_recurrent-modern">chapter_recurrent-modern</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_recurrent-modern/seq2seq/" title="seq2seq">seq2seq</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 computer-science">computer-science</a></div><span><a href="/2023/03/03/computer-science/%E6%AF%94%E8%B5%9B/%E5%A4%A7%E5%94%90%E6%9D%AF/note/" title="大唐杯">大唐杯</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-recurrent-modern/" title="分类于 chapter_recurrent-modern">chapter_recurrent-modern</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_recurrent-modern/deep-rnn/" title="deep-rnn">deep-rnn</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E8%BD%AF%E8%80%83%E9%AB%98%E7%BA%A7/" title="分类于 软考高级">软考高级</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%BD%AF%E8%80%83%E9%AB%98%E7%BA%A7/%E8%BD%AF%E8%80%83%E9%AB%98%E7%BA%A7%E9%80%89%E6%8B%A9%E9%A2%98/" title="分类于 软考高级选择题">软考高级选择题</a></div><span><a href="/2022/09/15/computer-science/%E6%AF%94%E8%B5%9B/%E9%AB%98%E7%BA%A7%E8%BD%AF%E8%80%83/%E9%80%89%E6%8B%A9%E9%A2%98/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" title="操作系统--软考高级">操作系统--软考高级</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/" title="分类于 nlp">nlp</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/huggingface/" title="分类于 huggingface">huggingface</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/huggingface/Tokenizer%E5%BA%93/" title="分类于 Tokenizer库">Tokenizer库</a></div><span><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/BPE/" title="Byte-Pair Encoding tokenization">Byte-Pair Encoding tokenization</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2023/06/25/computer-science/algorithm/leetCode/5-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/" title="未命名">未命名</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2023/06/25/computer-science/base/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/%E6%80%BB%E7%BA%BF/" title="未命名">未命名</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" title="分类于 编程语言">编程语言</a></div><span><a href="/2022/07/22/language/C++/C-%E6%95%99%E7%A8%8B/" title="C++教程">C++教程</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-references/" title="分类于 chapter_references">chapter_references</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_references/zreferences/" title="zreferences">zreferences</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/backend/" title="分类于 后端">后端</a> <i class="ic i-angle-right"></i> <a href="/categories/backend/django/" title="分类于 django">django</a></div><span><a href="/2022/09/12/backend/django/django%E9%85%8D%E7%BD%AE%E8%B7%A8%E5%9F%9F%E8%AF%B7%E6%B1%82/" title="django配置跨域请求">django配置跨域请求</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">yuan @ Mi Manchi</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">2.9m 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">44:38</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2023/02/15/ai/pytorch深度学习/chapter_computational-performance/multiple-gpus/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html>