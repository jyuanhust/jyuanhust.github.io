<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="yuan" href="https://jyuanhust.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="yuan" href="https://jyuanhust.github.io/atom.xml"><link rel="alternate" type="application/json" title="yuan" href="https://jyuanhust.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="chapter_computational-performance"><link rel="canonical" href="https://jyuanhust.github.io/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/hardware/"><title>hardware - chapter_computational-performance - pytorch深度学习 - ai | Mi Manchi = yuan = Whatever is worth doing at all is worth doing well</title><meta name="generator" content="Hexo 6.2.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">hardware</h1><div class="meta"><span class="item" title="创建时间：2023-02-15 00:00:00"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2023-02-15T00:00:00+08:00">2023-02-15</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>12k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>11 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Mi Manchi</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(59).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(46).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(92).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(98).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(88).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(95).webp"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/" itemprop="item" rel="index" title="分类于 ai"><span itemprop="name">ai</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="item" rel="index" title="分类于 pytorch深度学习"><span itemprop="name">pytorch深度学习</span></a><meta itemprop="position" content="2"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-computational-performance/" itemprop="item" rel="index" title="分类于 chapter_computational-performance"><span itemprop="name">chapter_computational-performance</span></a><meta itemprop="position" content="3"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://jyuanhust.github.io/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/hardware/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="yuan"><meta itemprop="description" content="Whatever is worth doing at all is worth doing well, "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="yuan"></span><div class="body md" itemprop="articleBody"><h1 id="硬件"><a class="anchor" href="#硬件">#</a> 硬件</h1><p>🏷 <code>sec_hardware</code></p><p>很好地理解算法和模型才可以捕获统计方面的问题，构建出具有出色性能的系统。同时，至少对底层硬件有一定的了解也是必不可少的。本节不能替代硬件和系统设计的相关课程。相反，本节的内容可以作为理解某些算法为什么比其他算法更高效以及如何实现良好吞吐量的起点。一个好的设计可以很容易地在性能上造就数量级的差异，这也是后续产生的能够训练网络（例如，训练时间为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span> 周）和无法训练网络（训练时间为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn></mrow><annotation encoding="application/x-tex">3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">3</span></span></span></span> 个月，导致错过截止期）之间的差异。我们先从计算机的研究开始。然后深入查看 CPU 和 GPU。最后，再查看数据中心或云中的多台计算机的连接方式。</p><p><img data-src="/./images/latencynumbers.png" alt="每个程序员都应该知道的延迟数字"><br>🏷 <code>fig_latencynumbers</code></p><p>也可以通过 :numref: <code>fig_latencynumbers</code> 进行简单的了解，图片源自科林・斯科特的<span class="exturl" data-url="aHR0cHM6Ly9wZW9wbGUuZWVjcy5iZXJrZWxleS5lZHUvfnJjcy9yZXNlYXJjaC9pbnRlcmFjdGl2ZV9sYXRlbmN5Lmh0bWw=">互动帖子</span>，在帖子中很好地概述了过去十年的进展。原始的数字是取自于杰夫迪恩的<span class="exturl" data-url="aHR0cHM6Ly9zdGF0aWMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL21lZGlhL3Jlc2VhcmNoLmdvb2dsZS5jb20vZW4vL3Blb3BsZS9qZWZmL1N0YW5mb3JkLURMLU5vdi0yMDEwLnBkZg=="> Stanford 讲座</span>。下面的讨论解释了这些数字的一些基本原理，以及它们如何指导我们去设计算法。下面的讨论是非常笼统和粗略的。很显然，它并不能代替一门完整的课程，而只是为了给统计建模者提供足够的信息，让他们做出合适的设计决策。对于计算机体系结构的深入概述，建议读者参考 :cite: <code>Hennessy.Patterson.2011</code> 或关于该主题的最新课程，例如<span class="exturl" data-url="aHR0cDovL2luc3QuZWVjcy5iZXJrZWxleS5lZHUvfmNzMTUyL3NwMTkv"> Arste Asanovic</span>。</p><h2 id="计算机"><a class="anchor" href="#计算机">#</a> 计算机</h2><p>大多数深度学习研究者和实践者都可以使用一台具有相当数量的内存、计算资源、某种形式的加速器（如一个或者多个 GPU）的计算机。计算机由以下关键部件组成：</p><ul><li>一个处理器（也被称为 CPU），它除了能够运行操作系统和许多其他功能之外，还能够执行给定的程序。它通常由<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn></mrow><annotation encoding="application/x-tex">8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">8</span></span></span></span> 个或更多个核心组成；</li><li>内存（随机访问存储，RAM）用于存储和检索计算结果，如权重向量和激活参数，以及训练数据；</li><li>一个或多个以太网连接，速度从 1GB/s 到 100GB/s 不等。在高端服务器上可能用到更高级的互连；</li><li>高速扩展总线（PCIe）用于系统连接一个或多个 GPU。服务器最多有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn></mrow><annotation encoding="application/x-tex">8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">8</span></span></span></span> 个加速卡，通常以更高级的拓扑方式连接，而桌面系统则有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span> 个或<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span></span></span></span> 个加速卡，具体取决于用户的预算和电源负载的大小；</li><li>持久性存储设备，如磁盘驱动器、固态驱动器，在许多情况下使用高速扩展总线连接。它为系统需要的训练数据和中间检查点需要的存储提供了足够的传输速度。</li></ul><p><img data-src="/./images/mobo-symbol.svg" alt="计算机组件的连接"><br>🏷 <code>fig_mobo-symbol</code></p><p>如 :numref: <code>fig_mobo-symbol</code> 所示，高速扩展总线由直接连接到 CPU 的多个通道组成，将 CPU 与大多数组件（网络、GPU 和存储）连接在一起。例如，AMD 的 Threadripper3 有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>64</mn></mrow><annotation encoding="application/x-tex">64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">6</span><span class="mord">4</span></span></span></span> 个 PCIe4.0 通道，每个通道都能够双向传输 16Gbit/s 的数据。内存直接连接到 CPU，总带宽高达 100GB/s。</p><p>当我们在计算机上运行代码时，需要将数据转移到处理器上（CPU 或 GPU）执行计算，然后将结果从处理器移回到随机访问存储和持久存储器中。因此，为了获得良好的性能，需要确保每一步工作都能无缝链接，而不希望系统中的任何一部分成为主要的瓶颈。例如，如果不能快速加载图像，那么处理器就无事可做。同样地，如果不能快速移动矩阵到 CPU（或 GPU）上，那么 CPU（或 GPU）就会无法全速运行。最后，如果希望在网络上同步多台计算机，那么网络就不应该拖累计算速度。一种选择是通信和计算交错进行。接下来将详细地介绍各个组件。</p><h2 id="内存"><a class="anchor" href="#内存">#</a> 内存</h2><p>最基本的内存主要用于存储需要随时访问的数据。目前，CPU 的内存通常为<span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvRERSNF9TRFJBTQ=="> DDR4</span> 类型，每个模块提供 20-25Gb/s 的带宽。每个模块都有一条<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>64</mn></mrow><annotation encoding="application/x-tex">64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">6</span><span class="mord">4</span></span></span></span> 位宽的总线。通常使用成对的内存模块来允许多个通道。CPU 有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span></span></span></span> 到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">4</span></span></span></span> 个内存通道，也就是说，它们内存带宽的峰值在 40GB/s 到 100GB/s 之间。一般每个通道有两个物理存储体（bank）。例如 AMD 的 Zen 3 Threadripper 有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn></mrow><annotation encoding="application/x-tex">8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">8</span></span></span></span> 个插槽。</p><p>虽然这些数字令人印象深刻，但实际上它们只能说明了一部分故事。当我们想要从内存中读取一部分内容时，需要先告诉内存模块在哪里可以找到信息。也就是说，我们需要先将<em>地址</em>（address）发送到 RAM。然后我们可以选择只读取一条<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>64</mn></mrow><annotation encoding="application/x-tex">64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">6</span><span class="mord">4</span></span></span></span> 位记录还是一长串记录。后者称为<em>突发读取</em>（burst read）。概括地说，向内存发送地址并设置传输大约需要 100ns（细节取决于所用内存芯片的特定定时系数），每个后续传输只需要 0.2ns。总之，第一次读取的成本是后续读取的 500 倍！请注意，每秒最多可以执行一千万次随机读取。这说明应该尽可能地避免随机内存访问，而是使用突发模式读取和写入。</p><p>当考虑到拥有多个物理存储体时，事情就更加复杂了。每个存储体大部分时候都可以独立地读取内存。这意味着两件事。一方面，如果随机读操作均匀分布在内存中，那么有效的随机读操作次数将高达 4 倍。这也意味着执行随机读取仍然不是一个好主意，因为突发读取的速度也快了 4 倍。另一方面，由于内存对齐是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>64</mn></mrow><annotation encoding="application/x-tex">64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">6</span><span class="mord">4</span></span></span></span> 位边界，因此最好将任何数据结构与相同的边界对齐。当设置了适当的标志时，编译器基本上就是<span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvRGF0YV9zdHJ1Y3R1cmVfYWxpZ25tZW50">自动化</span>地执行对齐操作。我们鼓励好奇的读者回顾一下<span class="exturl" data-url="aHR0cDovL3dlYi5jZWNzLnBkeC5lZHUvfnplc2hhbi9lY2U1ODVfbGVjNS5wZGY="> Zeshan Chishti 关于 DRAM 的讲座</span>。</p><p>GPU 内存的带宽要求甚至更高，因为它们的处理单元比 CPU 多得多。总的来说，解决这些问题有两种选择。首先是使内存总线变得更宽。例如，NVIDIA 的 RTX 2080Ti 有一条 352 位宽的总线。这样就可以同时传输更多的信息。其次，GPU 使用特定的高性能内存。消费级设备，如 NVIDIA 的 RTX 和 Titan 系列，通常使用<span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvR0REUjZfU0RSQU0lRUYlQkMlODklRTglOEElQUYlRTclODklODclRUYlQkMlOEMlRTYlODAlQkIlRTUlQjglQTYlRTUlQUUlQkQlRTglQjYlODUlRTglQkYlODc1MDBHQi9zJUUzJTgwJTgyJUU1JThGJUE2JUU0JUI4JTgwJUU3JUE3JThEJUU5JTgwJTg5JUU2JThCJUE5JUU2JTk4JUFGJUU0JUJEJUJGJUU3JTk0JUE4SEJNJUVGJUJDJTg4JUU5JUFCJTk4JUU1JUI4JUE2JUU1JUFFJUJEJUU1JUFEJTk4JUU1JTgyJUE4JUU1JTk5JUE4"> GDDR6</span> 模块。它们使用截然不同的接口，直接与专用硅片上的 GPU 连接。这使得它们非常昂贵，通常仅限于高端服务器芯片，如 NVIDIA Volta V100 系列加速卡。毫不意外的是 GPU 的内存通常比 CPU 的内存小得多，因为前者的成本更高。就目的而言，它们的性能与特征大体上是相似的，只是 GPU 的速度更快。就本书而言，我们完全可以忽略细节，因为这些技术只在调整 GPU 核心以获得高吞吐量时才起作用。</p><h2 id="存储器"><a class="anchor" href="#存储器">#</a> 存储器</h2><p>随机访问存储的一些关键特性是 <em>带宽</em>（bandwidth）和 <em>延迟</em>（latency）。存储设备也是如此，只是不同设备之间的特性差异可能更大。</p><h3 id="硬盘驱动器"><a class="anchor" href="#硬盘驱动器">#</a> 硬盘驱动器</h3><p><em>硬盘驱动器</em>（hard disk drive，HDD）已经使用了半个多世纪。简单的说，它们包含许多旋转的盘片，这些盘片的磁头可以放置在任何给定的磁道上进行读写。高端磁盘在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9</mn></mrow><annotation encoding="application/x-tex">9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">9</span></span></span></span> 个盘片上可容纳高达 16TB 的容量。硬盘的主要优点之一是相对便宜，而它们的众多缺点之一是典型的灾难性故障模式和相对较高的读取延迟。</p><p>要理解后者，请了解一个事实即硬盘驱动器的转速大约为 7200RPM（每分钟转数）。它们如果转速再快些，就会由于施加在碟片上的离心力而破碎。在访问磁盘上的特定扇区时，还有一个关键问题：需要等待碟片旋转到位（可以移动磁头，但是无法对磁盘加速）。因此，可能需要<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn></mrow><annotation encoding="application/x-tex">8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">8</span></span></span></span> 毫秒才能使用请求的数据。一种常见的描述方式是，硬盘驱动器可以以大约 100IOPs（每秒输入 / 输出操作）的速度工作，并且在过去二十年中这个数字基本上没变。同样糟糕的是，带宽（大约为 100-200MB/s）也很难增加。毕竟，每个磁头读取一个磁道的比特，因此比特率只随信息密度的平方根缩放。因此，对于非常大的数据集，HDD 正迅速降级为归档存储和低级存储。</p><h3 id="固态驱动器"><a class="anchor" href="#固态驱动器">#</a> 固态驱动器</h3><p>固态驱动器（solid state drives，SSD）使用闪存持久地存储信息。这允许更快地访问存储的记录。现代的固态驱动器的 IOPs 可以达到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn></mrow><annotation encoding="application/x-tex">10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">0</span></span></span></span> 万到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>50</mn></mrow><annotation encoding="application/x-tex">50</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">5</span><span class="mord">0</span></span></span></span> 万，比硬盘驱动器快 3 个数量级。而且，它们的带宽可以达到 1-3GB/s，比硬盘驱动器快一个数量级。这些改进听起来好的难以置信，而事实上受固态驱动器的设计方式，它仍然存在下面的附加条件。</p><ul><li>固态驱动器以块的方式（256KB 或更大）存储信息。块只能作为一个整体来写入，因此需要耗费大量的时间，导致固态驱动器在按位随机写入时性能非常差。而且通常数据写入需要大量的时间还因为块必须被读取、擦除，然后再重新写入新的信息。如今固态驱动器的控制器和固件已经开发出了缓解这种情况的算法。尽管有了算法，写入速度仍然会比读取慢得多，特别是对于 QLC（四层单元）固态驱动器。提高性能的关键是维护操作的 “队列”，在队列中尽可能地优先读取和写入大的块。</li><li>固态驱动器中的存储单元磨损得比较快（通常在几千次写入之后就已经老化了）。磨损程度保护算法能够将退化平摊到许多单元。也就是说，不建议将固态驱动器用于交换分区文件或大型日志文件。</li><li>最后，带宽的大幅增加迫使计算机设计者将固态驱动器与 PCIe 总线相连接，这种驱动器称为 NVMe（非易失性内存增强），其最多可以使用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">4</span></span></span></span> 个 PCIe 通道。在 PCIe4.0 上最高可达 8GB/s。</li></ul><h3 id="云存储"><a class="anchor" href="#云存储">#</a> 云存储</h3><p>云存储提供了一系列可配置的性能。也就是说，虚拟机的存储在数量和速度上都能根据用户需要进行动态分配。建议用户在延迟太高时（例如，在训练期间存在许多小记录时）增加 IOPs 的配置数。</p><h2 id="cpu"><a class="anchor" href="#cpu">#</a> CPU</h2><p>中央处理器（central processing unit，CPU）是任何计算机的核心。它们由许多关键组件组成：<em>处理器核心</em>（processor cores）用于执行机器代码的；<em>总线</em>（bus）用于连接不同组件（注意，总线会因为处理器型号、各代产品和供应商之间的特定拓扑结构有明显不同）；<em>缓存</em>（cach）相比主内存实现更高的读取带宽和更低的延迟内存访问。最后，因为高性能线性代数和卷积运算常见于媒体处理和机器学习中，所以几乎所有的现代 CPU 都包含<em>向量处理单元</em>（vector processing unit）为这些计算提供辅助。</p><p><img data-src="/./images/skylake.svg" alt="Intel Skylake消费级四核CPU"><br>🏷 <code>fig_skylake</code></p><p>:numref: <code>fig_skylake</code> 描述了 Intel Skylake 消费级四核 CPU。它包含一个集成 GPU、缓存和一个连接四个核心的环总线。例如，以太网、WiFi、蓝牙、SSD 控制器和 USB 这些外围设备要么是芯片组的一部分，要么通过 PCIe 直接连接到 CPU。</p><h3 id="微体系结构"><a class="anchor" href="#微体系结构">#</a> 微体系结构</h3><p>每个处理器核心都由一组相当复杂的组件组成。虽然不同时代的产品和供应商的细节有所不同，但基本功能都是标准的。前端加载指令并尝试预测将采用哪条路径（例如，为了控制流），然后将指令从汇编代码解码为微指令。汇编代码通常不是处理器执行的最低级别代码，而复杂的微指令却可以被解码成一组更低级的操作，然后由实际的执行核心处理。通常执行核心能够同时执行许多操作，例如， :numref: <code>fig_cortexa77</code> 的 ARM Cortex A77 核心可以同时执行多达<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn></mrow><annotation encoding="application/x-tex">8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">8</span></span></span></span> 个操作。</p><p><img data-src="/./images/a77.svg" alt="ARM Cortex A77微体系结构"><br>🏷 <code>fig_cortexa77</code></p><p>这意味着高效的程序可以在每个时钟周期内执行多条指令，前提是这些指令可以独立执行。不是所有的处理单元都是平等的。一些专用于处理整数指令，而另一些则针对浮点性能进行了优化。为了提高吞吐量，处理器还可以在分支指令中同时执行多条代码路径，然后丢弃未选择分支的结果。这就是为什么前端的分支预测单元很重要，因为只有最有希望的路径才会被继续执行。</p><h3 id="矢量化"><a class="anchor" href="#矢量化">#</a> 矢量化</h3><p>深度学习的计算量非常大。因此，为了满足机器学习的需要，CPU 需要在一个时钟周期内执行许多操作。这种执行方式是通过向量处理单元实现的。这些处理单元有不同的名称：在 ARM 上叫做 NEON, 在 x86 上被称为<span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQWR2YW5jZWRfVmVjdG9yX0V4dGVuc2lvbnM="> AVX2</span>。一个常见的功能是它们能够执行单指令多数据（single instruction multiple data，SIMD）操作。 :numref: <code>fig_neon128</code> 显示了如何在 ARM 上的一个时钟周期中完成<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn></mrow><annotation encoding="application/x-tex">8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">8</span></span></span></span> 个整数加法。</p><p><img data-src="/./images/neon128.svg" alt="128位NEON矢量化"><br>🏷 <code>fig_neon128</code></p><p>根据体系结构的选择，此类寄存器最长可达<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>512</mn></mrow><annotation encoding="application/x-tex">512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">5</span><span class="mord">1</span><span class="mord">2</span></span></span></span> 位，最多可组合<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>64</mn></mrow><annotation encoding="application/x-tex">64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">6</span><span class="mord">4</span></span></span></span> 对数字。例如，我们可能会将两个数字相乘，然后与第三个数字相加，这也称为乘加融合（fused multiply-add）。Intel 的<span class="exturl" data-url="aHR0cHM6Ly8wMS5vcmcvb3BlbnZpbm90b29sa2l0"> OpenVino</span> 就是使用这些处理器来获得可观的吞吐量，以便在服务器级 CPU 上进行深度学习。不过请注意，这个数字与 GPU 的能力相比则相形见绌。例如，NVIDIA 的 RTX 2080Ti 拥有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4352</mn></mrow><annotation encoding="application/x-tex">4352</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">4</span><span class="mord">3</span><span class="mord">5</span><span class="mord">2</span></span></span></span> 个 CUDA 核心，每个核心都能够在任何时候处理这样的操作。</p><h3 id="缓存"><a class="anchor" href="#缓存">#</a> 缓存</h3><p>考虑以下情况：我们有一个中等规模的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">4</span></span></span></span> 核心的 CPU，如 :numref: <code>fig_skylake</code> 所示，运行在 2GHz 频率。此外，假设向量处理单元启用了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>256</mn></mrow><annotation encoding="application/x-tex">256</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span></span></span></span> 位带宽的 AVX2，其 IPC（指令 / 时钟）计数为 1。进一步假设从内存中获取用于 AVX2 操作的指令至少需要一个寄存器。这意味着 CPU 每个时钟周期需要消耗<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mn>256</mn><mtext>bit</mtext><mo>=</mo><mn>128</mn><mtext>bytes</mtext></mrow><annotation encoding="application/x-tex">4 \times 256 \text{ bit} = 128 \text{ bytes}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">4</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span><span class="mord text"><span class="mord"> bit</span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord">1</span><span class="mord">2</span><span class="mord">8</span><span class="mord text"><span class="mord"> bytes</span></span></span></span></span> 的数据。除非我们能够每秒向处理器传输<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mn>9</mn></msup><mo>×</mo><mn>128</mn><mo>=</mo><mn>256</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mn>9</mn></msup></mrow><annotation encoding="application/x-tex">2 \times 10^9 \times 128 = 256 \times 10^9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">2</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.897438em;vertical-align:-.08333em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">9</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">2</span><span class="mord">8</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.8141079999999999em;vertical-align:0"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">9</span></span></span></span></span></span></span></span></span></span></span> 字节，否则用于处理的数据将会不足。不幸的是，这种芯片的存储器接口仅支持 20-40Gb/s 的数据传输，即少了一个数量级。解决方法是尽可能避免从内存中加载新数据，而是将数据放在 CPU 的缓存上。这就是使用缓存的地方。通常使用以下名称或概念。</p><ul><li><strong>寄存器</strong>，严格来说不是缓存的一部分，用于帮助组织指令。也就是说，寄存器是 CPU 可以以时钟速度访问而没有延迟的存储位置。CPU 有几十个寄存器，因此有效地使用寄存器取决于编译器（或程序员）。例如，C 语言有一个 <code>register</code> 关键字。</li><li><strong>一级缓存</strong>是应对高内存带宽要求的第一道防线。一级缓存很小（常见的大小可能是 32-64KB），内容通常分为数据和指令。当数据在一级缓存中被找到时，其访问速度非常快，如果没有在那里找到，搜索将沿着缓存层次结构向下寻找。</li><li><strong>二级缓存</strong>是下一站。根据架构设计和处理器大小的不同，它们可能是独占的也可能是共享的。即它们可能只能由给定的核心访问，或者在多个核心之间共享。二级缓存比一级缓存大（通常每个核心 256-512KB），而速度也更慢。此外，我们首先需要检查以确定数据不在一级缓存中，才会访问二级缓存中的内容，这会增加少量的额外延迟。</li><li><strong>三级缓存</strong>在多个核之间共享，并且可以非常大。AMD 的 EPYC 3 服务器的 CPU 在多个芯片上拥有高达 256MB 的高速缓存。更常见的数字在 4-8MB 范围内。</li></ul><p>预测下一步需要哪个存储设备是优化芯片设计的关键参数之一。例如，建议以<em>向前</em>的方向遍历内存，因为大多数缓存算法将试图<em>向前读取</em>（read forward）而不是向后读取。同样，将内存访问模式保持在本地也是提高性能的一个好方法。</p><p>添加缓存是一把双刃剑。一方面，它能确保处理器核心不缺乏数据。但同时，它也增加了芯片尺寸，消耗了原本可以用来提高处理能力的面积。此外，<em>缓存未命中</em>的代价可能会很昂贵。考虑最坏的情况，如 :numref: <code>fig_falsesharing</code> 所示的<em>错误共享</em>（false sharing）。当处理器<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span> 上的线程请求数据时，内存位置缓存在处理器<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">0</span></span></span></span> 上。为了满足获取需要，处理器<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">0</span></span></span></span> 需要停止它正在做的事情，将信息写回主内存，然后让处理器<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span> 从内存中读取它。在此操作期间，两个处理器都需要等待。与高效的单处理器实现相比，这种代码在多个处理器上运行的速度可能要慢得多。这就是为什么缓存大小（除了物理大小之外）有实际限制的另一个原因。</p><p><img data-src="/./images/falsesharing.svg" alt="错误共享（图片由英特尔提供）"><br>🏷 <code>fig_falsesharing</code></p><h2 id="gpu和其他加速卡"><a class="anchor" href="#gpu和其他加速卡">#</a> GPU 和其他加速卡</h2><p>毫不夸张地说，如果没有 GPU，深度学习就不会成功。基于同样的原因，有理由认为 GPU 制造商的财富由于深度学习而显著增加。这种硬件和算法的协同进化导致了这样一种情况：无论好坏，深度学习都是更可取的统计建模范式。因此，了解 GPU 和其他加速卡（如 TPU :cite: <code>Jouppi.Young.Patil.ea.2017</code> ）的具体好处是值得的。</p><p>值得注意的是，在实践中经常会有这样一个判别：加速卡是为训练还是推断而优化的。对于后者，我们只需要计算网络中的前向传播。而反向传播不需要存储中间数据。还有，我们可能不需要非常精确的计算（FP16 或 INT8 通常就足够了）。对于前者，即训练过程中需要存储所有的中间结果用来计算梯度。而且，累积梯度也需要更高的精度，以避免数值下溢（或溢出）。这意味着最低要求也是 FP16（或 FP16 与 FP32 的混合精度）。所有这些都需要更快、更大的内存（HBM2 或者 GDDR6）和更高的处理能力。例如，NVIDIA 优化了<span class="exturl" data-url="aHR0cHM6Ly9kZXZibG9ncy5udmlkaWEuY29tL252aWRpYS10dXJpbmctYXJjaGl0ZWN0dXJlLWluLWRlcHRoLw=="> Turing</span> T4 GPU 用于推断和 V100 GPU 用于训练。</p><p>回想一下如 :numref: <code>fig_neon128</code> 所示的矢量化。处理器核心中添加向量处理单元可以显著提高吞吐量。例如，在 :numref: <code>fig_neon128</code> 的例子中，我们能够同时执行<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn></mrow><annotation encoding="application/x-tex">16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">6</span></span></span></span> 个操作。首先，如果我们添加的运算不仅优化了向量运算，而且优化了矩阵运算，会有什么好处？稍后我们将讨论基于这个策略引入的张量核（tensor cores）。第二，如果我们增加更多的核心呢？简而言之，以上就是 GPU 设计决策中的两种策略。 :numref: <code>fig_turing_processing_block</code> 给出了基本处理块的概述。它包含<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn></mrow><annotation encoding="application/x-tex">16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">6</span></span></span></span> 个整数单位和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn></mrow><annotation encoding="application/x-tex">16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">6</span></span></span></span> 个浮点单位。除此之外，两个张量核加速了与深度学习相关的附加操作的狭窄的子集。每个流式多处理器都由这样的四个块组成。</p><p><img data-src="/./images/turing-processing-block.png" alt="NVIDIA Turing处理块（图片由英伟达提供）"><br>:width: <code>150px</code><br>🏷 <code>fig_turing_processing_block</code></p><p>接下来，将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>12</mn></mrow><annotation encoding="application/x-tex">12</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">2</span></span></span></span> 个流式多处理器分组为图形处理集群，这些集群构成了高端 TU102 处理器。充足的内存通道和二级缓存完善了配置。 :numref: <code>fig_turing</code> 有相关的细节。设计这种设备的原因之一是可以根据需要独立地添加或删除模块，从而满足设计更紧凑的芯片和处理良品率问题（故障模块可能无法激活）的需要。幸运的是，在 CUDA 和框架代码层之下，这类设备的编程对深度学习的临时研究员隐藏得很好。特别是，只要有可用的资源 GPU 上就可以同时执行多个程序。尽管如此，了解设备的局限性是值得的，以避免对应的设备内存的型号不合适。</p><p><img data-src="/./images/turing.png" alt="NVIDIA Turing架构（图片由英伟达提供）"><br>:width: <code>350px</code><br>🏷 <code>fig_turing</code></p><p>最后值得一提的是<em>张量核</em>（tensor core）。它们是最近增加更多优化电路趋势的一个例子，这些优化电路对深度学习特别有效。例如，TPU 添加了用于快速矩阵乘法的脉动阵列 :cite: <code>Kung.1988</code> ，这种设计是为了支持非常小数量（第一代 TPU 支持数量为 1）的大型操作。而张量核是另一个极端。它们针对<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">4 \times 4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">4</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">4</span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn><mo>×</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">16 \times 16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">1</span><span class="mord">6</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">6</span></span></span></span> 矩阵之间的小型运算进行了优化，具体取决于它们的数值精度。 :numref: <code>fig_tensorcore</code> 给出了优化的概述。</p><p><img data-src="/./images/tensorcore.jpg" alt="NVIDIA Turing架构中的张量核心（图片由英伟达提供）"><br>:width: <code>400px</code><br>🏷 <code>fig_tensorcore</code></p><p>显然，我们最终会在优化计算时做出某些妥协。其中之一是 GPU 不太擅长处理稀疏数据和中断。尽管有一些明显的例外，如<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2d1bnJvY2svZ3Vucm9jaw=="> Gunrock</span> :cite: <code>Wang.Davidson.Pan.ea.2016</code> ，但 GPU 擅长的高带宽突发读取操作并不适合稀疏的矩阵和向量的访问模式。访问稀疏数据和处理中断这两个目标是一个积极研究的领域。例如：<span class="exturl" data-url="aHR0cDovL2RnbC5haQ==">DGL</span>，一个专为图深度学习而设计的库。</p><h2 id="网络和总线"><a class="anchor" href="#网络和总线">#</a> 网络和总线</h2><p>每当单个设备不足以进行优化时，我们就需要来回传输数据以实现同步处理，于是网络和总线就派上了用场。我们有许多设计参数：带宽、成本、距离和灵活性。应用的末端有 WiFi，它有非常好的使用范围，非常容易使用（毕竟没有线缆），而且还便宜，但它提供的带宽和延迟相对一般。头脑正常的机器学习研究人员都不会用它来构建服务器集群。接下来的内容中将重点关注适合深度学习的互连方式。</p><ul><li><strong>PCIe</strong>，一种专用总线，用于每个通道点到点连接的高带宽需求（在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn></mrow><annotation encoding="application/x-tex">16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">6</span></span></span></span> 通道插槽中的 PCIe4.0 上高达 32GB/s），延迟时间为个位数的微秒（5μs）。PCIe 链接非常宝贵。处理器拥有的数量：AMD 的 EPYC 3 有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>128</mn></mrow><annotation encoding="application/x-tex">128</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">2</span><span class="mord">8</span></span></span></span> 个通道，Intel 的 Xeon 每个芯片有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>48</mn></mrow><annotation encoding="application/x-tex">48</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">4</span><span class="mord">8</span></span></span></span> 个通道；在桌面级 CPU 上，数字分别是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>20</mn></mrow><annotation encoding="application/x-tex">20</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span><span class="mord">0</span></span></span></span>（Ryzen9）和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn></mrow><annotation encoding="application/x-tex">16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">6</span></span></span></span>（Core i9）。由于 GPU 通常有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn></mrow><annotation encoding="application/x-tex">16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">6</span></span></span></span> 个通道，这就限制了以全带宽与 CPU 连接的 GPU 数量。毕竟，它们还需要与其他高带宽外围设备（如存储和以太网）共享链路。与 RAM 访问一样，由于减少了数据包的开销，因此更适合大批量数据传输。</li><li><strong>以太网</strong>，连接计算机最常用的方式。虽然它比 PCIe 慢得多，但它的安装成本非常低，而且具有很强的弹性，覆盖的距离也要长得多。低级服务器的典型带宽为 1GBit/s。高端设备（如云中的<span class="exturl" data-url="aHR0cHM6Ly9hd3MuYW1hem9uLmNvbS9lYzIvaW5zdGFuY2UtdHlwZXMvYzUvJUVGJUJDJTg5JUVGJUJDJTg5JUU2JThGJTkwJUU0JUJFJTlCMTAlRUYlQkQlOUUxMDBHQml0L3MlRTclOUElODQlRTUlQjglQTYlRTUlQUUlQkQlRTMlODAlODIlRTQlQjglOEUlRTQlQkIlQTUlRTUlODklOEQlRTYlODklODAlRTYlOUMlODklRTclOUElODQlRTYlODMlODUlRTUlODYlQjUlRTQlQjglODAlRTYlQTAlQjclRUYlQkMlOEMlRTYlOTUlQjAlRTYlOEQlQUUlRTQlQkMlQTAlRTglQkUlOTMlRTYlOUMlODklRTUlQkUlODglRTUlQTQlQTclRTclOUElODQlRTUlQkMlODAlRTklOTQlODAlRTMlODAlODIlRTglQUYlQjclRTYlQjMlQTglRTYlODQlOEYlRUYlQkMlOEMlRTUlOEUlOUYlRTUlQTclOEIlRTQlQkIlQTUlRTUlQTQlQUElRTclQkQlOTElRTUlODclQTAlRTQlQjklOEUlRTQlQkIlOEUlRTQlQjglOEQlRTglQTIlQUIlRTclOUIlQjQlRTYlOEUlQTUlRTQlQkQlQkYlRTclOTQlQTglRUYlQkMlOEMlRTglODAlOEMlRTYlOTglQUYlRTUlOUMlQTglRTclODklQTklRTclOTAlODYlRTQlQkElOTIlRTglQkYlOUUlRTQlQjklOEIlRTQlQjglOEElRTQlQkQlQkYlRTclOTQlQTglRTYlODklQTclRTglQTElOEMlRTclOUElODQlRTUlOEQlOEYlRTglQUUlQUUlRUYlQkMlODglRTQlQkUlOEIlRTUlQTYlODJVRFAlRTYlODglOTZUQ1AvSVA="> C5 实例</span>。这进一步增加了开销。与 PCIe 类似，以太网旨在连接两个设备，例如计算机和交换机。</li><li><strong>交换机</strong>，一种连接多个设备的方式，该连接方式下的任何一对设备都可以同时执行（通常是全带宽）点对点连接。例如，以太网交换机可能以高带宽连接<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>40</mn></mrow><annotation encoding="application/x-tex">40</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">4</span><span class="mord">0</span></span></span></span> 台服务器。请注意，交换机并不是传统计算机网络所独有的。甚至 PCIe 通道也可以是<span class="exturl" data-url="aHR0cHM6Ly93d3cuYnJvYWRjb20uY29tL3Byb2R1Y3RzL3BjaWUtc3dpdGNoZXMtYnJpZGdlcy9wY2llLXN3aXRjaGVz">可交换的</span>，例如：<span class="exturl" data-url="aHR0cHM6Ly9hd3MuYW1hem9uLmNvbS9lYzIvaW5zdGFuY2UtdHlwZXMvcDIv">P2 实例</span>就是将大量 GPU 连接到主机处理器。</li><li><strong>NVLink</strong>，是 PCIe 的替代品，适用于非常高带宽的互连。它为每条链路提供高达 300Gbit/s 的数据传输速率。服务器 GPU（Volta V100）有六个链路。而消费级 GPU（RTX 2080Ti）只有一个链路，运行速度也降低到 100Gbit/s。建议使用<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL05WSURJQS9uY2Ns"> NCCL</span> 来实现 GPU 之间的高速数据传输。</li></ul><h2 id="更多延迟"><a class="anchor" href="#更多延迟">#</a> 更多延迟</h2><p>:numref: <code>table_latency_numbers</code> 和 :numref: <code>table_latency_numbers_tesla</code> 中的小结来自<span class="exturl" data-url="aHR0cHM6Ly9naXN0LmdpdGh1Yi5jb20vZXNoZWxtYW4="> Eliot Eshelman</span>，他们将数字的更新版本保存到<span class="exturl" data-url="aHR0cHM6Ly9naXN0LmdpdGh1Yi5jb20vZXNoZWxtYW4vMzQzYTFjNDZjYjNmYmExNDJjMWFmZGNkZWVjMTc2NDY="> GitHub gist</span>。</p><p>: 常见延迟。</p><table><thead><tr><th style="text-align:left">Action</th><th style="text-align:right">Time</th><th style="text-align:left">Notes</th></tr></thead><tbody><tr><td style="text-align:left">L1 cache reference/hit</td><td style="text-align:right">1.5 ns</td><td style="text-align:left">4 cycles</td></tr><tr><td style="text-align:left">Floating-point add/mult/FMA</td><td style="text-align:right">1.5 ns</td><td style="text-align:left">4 cycles</td></tr><tr><td style="text-align:left">L2 cache reference/hit</td><td style="text-align:right">5 ns</td><td style="text-align:left">12 ~ 17 cycles</td></tr><tr><td style="text-align:left">Branch mispredict</td><td style="text-align:right">6 ns</td><td style="text-align:left">15 ~ 20 cycles</td></tr><tr><td style="text-align:left">L3 cache hit (unshared cache)</td><td style="text-align:right">16 ns</td><td style="text-align:left">42 cycles</td></tr><tr><td style="text-align:left">L3 cache hit (shared in another core)</td><td style="text-align:right">25 ns</td><td style="text-align:left">65 cycles</td></tr><tr><td style="text-align:left">Mutex lock/unlock</td><td style="text-align:right">25 ns</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">L3 cache hit (modified in another core)</td><td style="text-align:right">29 ns</td><td style="text-align:left">75 cycles</td></tr><tr><td style="text-align:left">L3 cache hit (on a remote CPU socket)</td><td style="text-align:right">40 ns</td><td style="text-align:left">100 ~ 300 cycles (40 ~ 116 ns)</td></tr><tr><td style="text-align:left">QPI hop to a another CPU (per hop)</td><td style="text-align:right">40 ns</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">64MB memory ref. (local CPU)</td><td style="text-align:right">46 ns</td><td style="text-align:left">TinyMemBench on Broadwell E5-2690v4</td></tr><tr><td style="text-align:left">64MB memory ref. (remote CPU)</td><td style="text-align:right">70 ns</td><td style="text-align:left">TinyMemBench on Broadwell E5-2690v4</td></tr><tr><td style="text-align:left">256MB memory ref. (local CPU)</td><td style="text-align:right">75 ns</td><td style="text-align:left">TinyMemBench on Broadwell E5-2690v4</td></tr><tr><td style="text-align:left">Intel Optane random write</td><td style="text-align:right">94 ns</td><td style="text-align:left">UCSD Non-Volatile Systems Lab</td></tr><tr><td style="text-align:left">256MB memory ref. (remote CPU)</td><td style="text-align:right">120 ns</td><td style="text-align:left">TinyMemBench on Broadwell E5-2690v4</td></tr><tr><td style="text-align:left">Intel Optane random read</td><td style="text-align:right">305 ns</td><td style="text-align:left">UCSD Non-Volatile Systems Lab</td></tr><tr><td style="text-align:left">Send 4KB over 100 Gbps HPC fabric</td><td style="text-align:right">1 μs</td><td style="text-align:left">MVAPICH2 over Intel Omni-Path</td></tr><tr><td style="text-align:left">Compress 1KB with Google Snappy</td><td style="text-align:right">3 μs</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">Send 4KB over 10 Gbps ethernet</td><td style="text-align:right">10 μs</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">Write 4KB randomly to NVMe SSD</td><td style="text-align:right">30 μs</td><td style="text-align:left">DC P3608 NVMe SSD (QOS 99% is 500μs)</td></tr><tr><td style="text-align:left">Transfer 1MB to/from NVLink GPU</td><td style="text-align:right">30 μs</td><td style="text-align:left">~33GB/s on NVIDIA 40GB NVLink</td></tr><tr><td style="text-align:left">Transfer 1MB to/from PCI-E GPU</td><td style="text-align:right">80 μs</td><td style="text-align:left">~12GB/s on PCIe 3.0 x16 link</td></tr><tr><td style="text-align:left">Read 4KB randomly from NVMe SSD</td><td style="text-align:right">120 μs</td><td style="text-align:left">DC P3608 NVMe SSD (QOS 99%)</td></tr><tr><td style="text-align:left">Read 1MB sequentially from NVMe SSD</td><td style="text-align:right">208 μs</td><td style="text-align:left">~4.8GB/s DC P3608 NVMe SSD</td></tr><tr><td style="text-align:left">Write 4KB randomly to SATA SSD</td><td style="text-align:right">500 μs</td><td style="text-align:left">DC S3510 SATA SSD (QOS 99.9%)</td></tr><tr><td style="text-align:left">Read 4KB randomly from SATA SSD</td><td style="text-align:right">500 μs</td><td style="text-align:left">DC S3510 SATA SSD (QOS 99.9%)</td></tr><tr><td style="text-align:left">Round trip within same datacenter</td><td style="text-align:right">500 μs</td><td style="text-align:left">One-way ping is ~250μs</td></tr><tr><td style="text-align:left">Read 1MB sequentially from SATA SSD</td><td style="text-align:right">2 ms</td><td style="text-align:left">~550MB/s DC S3510 SATA SSD</td></tr><tr><td style="text-align:left">Read 1MB sequentially from disk</td><td style="text-align:right">5 ms</td><td style="text-align:left">~200MB/s server HDD</td></tr><tr><td style="text-align:left">Random Disk Access (seek+rotation)</td><td style="text-align:right">10 ms</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">Send packet CA-&gt;Netherlands-&gt;CA</td><td style="text-align:right">150 ms</td><td style="text-align:left"></td></tr></tbody></table><p>🏷 <code>table_latency_numbers</code></p><p>:NVIDIA Tesla GPU 的延迟.</p><table><thead><tr><th style="text-align:left">Action</th><th style="text-align:right">Time</th><th style="text-align:left">Notes</th></tr></thead><tbody><tr><td style="text-align:left">GPU Shared Memory access</td><td style="text-align:right">30 ns</td><td style="text-align:left">30~90 cycles (bank conflicts add latency)</td></tr><tr><td style="text-align:left">GPU Global Memory access</td><td style="text-align:right">200 ns</td><td style="text-align:left">200~800 cycles</td></tr><tr><td style="text-align:left">Launch CUDA kernel on GPU</td><td style="text-align:right">10 μs</td><td style="text-align:left">Host CPU instructs GPU to start kernel</td></tr><tr><td style="text-align:left">Transfer 1MB to/from NVLink GPU</td><td style="text-align:right">30 μs</td><td style="text-align:left">~33GB/s on NVIDIA 40GB NVLink</td></tr><tr><td style="text-align:left">Transfer 1MB to/from PCI-E GPU</td><td style="text-align:right">80 μs</td><td style="text-align:left">~12GB/s on PCI-Express x16 link</td></tr></tbody></table><p>🏷 <code>table_latency_numbers_tesla</code></p><h2 id="小结"><a class="anchor" href="#小结">#</a> 小结</h2><ul><li>设备有运行开销。因此，数据传输要争取量大次少而不是量少次多。这适用于 RAM、固态驱动器、网络和 GPU。</li><li>矢量化是性能的关键。确保充分了解加速器的特定功能。例如，一些 Intel Xeon CPU 特别适用于 INT8 操作，NVIDIA Volta GPU 擅长 FP16 矩阵操作，NVIDIA Turing 擅长 FP16、INT8 和 INT4 操作。</li><li>在训练过程中数据类型过小导致的数值溢出可能是个问题（在推断过程中则影响不大）。</li><li>数据混叠现象会导致严重的性能退化。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>64</mn></mrow><annotation encoding="application/x-tex">64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">6</span><span class="mord">4</span></span></span></span> 位 CPU 应该按照<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>64</mn></mrow><annotation encoding="application/x-tex">64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">6</span><span class="mord">4</span></span></span></span> 位边界进行内存对齐。在 GPU 上建议保持卷积大小对齐，例如：与张量核对齐。</li><li>将算法与硬件相匹配（例如，内存占用和带宽）。将命中参数装入缓存后，可以实现很大数量级的加速比。</li><li>在验证实验结果之前，建议先在纸上勾勒出新算法的性能。关注的原因是数量级及以上的差异。</li><li>使用调试器跟踪调试寻找性能的瓶颈。</li><li>训练硬件和推断硬件在性能和价格方面有不同的优点。</li></ul><h2 id="练习"><a class="anchor" href="#练习">#</a> 练习</h2><ol><li>编写 C 语言来测试访问对齐的内存和未对齐的内存之间的速度是否有任何差异。（提示：小心缓存影响。）</li><li>测试按顺序访问或按给定步幅访问内存时的速度差异。</li><li>如何测量 CPU 上的缓存大小？</li><li>如何在多个内存通道中分配数据以获得最大带宽？如果有许多小的线程，会怎么布置？</li><li>一个企业级硬盘正在以 10000 转 / 分的速度旋转。在最坏的情况下，硬盘读取数据所需的最短时间是多少（假设磁头几乎是瞬间移动的）？为什么 2.5 英寸硬盘在商用服务器上越来越流行（相对于 3.5 英寸硬盘和 5.25 英寸硬盘）？</li><li>假设 HDD 制造商将存储密度从每平方英寸 1 Tbit 增加到每平方英寸 5 Tbit。在一个 2.5 英寸的硬盘上，多少信息能够存储一个环中？内轨和外轨有区别吗？</li><li>从<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn></mrow><annotation encoding="application/x-tex">8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">8</span></span></span></span> 位数据类型到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn></mrow><annotation encoding="application/x-tex">16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">6</span></span></span></span> 位数据类型，硅片的数量大约增加了四倍，为什么？为什么 NVIDIA 会在其图灵 GPU 中添加 INT4 运算？</li><li>在内存中向前读比向后读快多少？该数字在不同的计算机和 CPU 供应商之间是否有所不同？为什么？编写 C 代码进行实验。</li><li>磁盘的缓存大小能否测量？典型的硬盘是多少？固态驱动器需要缓存吗？</li><li>测量通过以太网发送消息时的数据包开销。查找 UDP 和 TCP/IP 连接之间的差异。</li><li>直接内存访问允许 CPU 以外的设备直接向内存写入（和读取）。为什么要这样？</li><li>看看 Turing T4GPU 的性能数字。为什么从 FP16 到 INT8 和 INT4 的性能只翻倍？</li><li>一个网络包从旧金山到阿姆斯特丹的往返旅行需要多长时间？提示：可以假设距离为 10000 公里。</li></ol><p><span class="exturl" data-url="aHR0cHM6Ly9kaXNjdXNzLmQybC5haS90LzU3MTc=">Discussions</span></p><div class="tags"><a href="/tags/chapter-computational-performance/" rel="tag"><i class="ic i-tag"></i> chapter_computational-performance</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2023-03-04 13:38:25" itemprop="dateModified" datetime="2023-03-04T13:38:25+08:00">2023-03-04</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="yuan 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="yuan 支付宝"><p>支付宝</p></div><div><img data-src="/images/paypal.png" alt="yuan 贝宝"><p>贝宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>yuan <i class="ic i-at"><em>@</em></i>yuan</li><li class="link"><strong>本文链接：</strong> <a href="https://jyuanhust.github.io/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/hardware/" title="hardware">https://jyuanhust.github.io/2023/02/15/ai/pytorch深度学习/chapter_computational-performance/hardware/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/auto-parallelism/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;gitee.com&#x2F;zkz0&#x2F;image&#x2F;raw&#x2F;master&#x2F;img&#x2F;img(73).webp" title="auto-parallelism"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> chapter_computational-performance</span><h3>auto-parallelism</h3></a></div><div class="item right"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_attention-mechanisms/transformer/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;gitee.com&#x2F;zkz0&#x2F;image&#x2F;raw&#x2F;master&#x2F;img&#x2F;img(84).webp" title="transformer"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> chapter_attention-mechanisms</span><h3>transformer</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A1%AC%E4%BB%B6"><span class="toc-number">1.</span> <span class="toc-text">硬件</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA"><span class="toc-number">1.1.</span> <span class="toc-text">计算机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%85%E5%AD%98"><span class="toc-number">1.2.</span> <span class="toc-text">内存</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%98%E5%82%A8%E5%99%A8"><span class="toc-number">1.3.</span> <span class="toc-text">存储器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A1%AC%E7%9B%98%E9%A9%B1%E5%8A%A8%E5%99%A8"><span class="toc-number">1.3.1.</span> <span class="toc-text">硬盘驱动器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BA%E6%80%81%E9%A9%B1%E5%8A%A8%E5%99%A8"><span class="toc-number">1.3.2.</span> <span class="toc-text">固态驱动器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%91%E5%AD%98%E5%82%A8"><span class="toc-number">1.3.3.</span> <span class="toc-text">云存储</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cpu"><span class="toc-number">1.4.</span> <span class="toc-text">CPU</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AE%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84"><span class="toc-number">1.4.1.</span> <span class="toc-text">微体系结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A2%E9%87%8F%E5%8C%96"><span class="toc-number">1.4.2.</span> <span class="toc-text">矢量化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%93%E5%AD%98"><span class="toc-number">1.4.3.</span> <span class="toc-text">缓存</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gpu%E5%92%8C%E5%85%B6%E4%BB%96%E5%8A%A0%E9%80%9F%E5%8D%A1"><span class="toc-number">1.5.</span> <span class="toc-text">GPU 和其他加速卡</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E5%92%8C%E6%80%BB%E7%BA%BF"><span class="toc-number">1.6.</span> <span class="toc-text">网络和总线</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9B%B4%E5%A4%9A%E5%BB%B6%E8%BF%9F"><span class="toc-number">1.7.</span> <span class="toc-text">更多延迟</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.8.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0"><span class="toc-number">1.9.</span> <span class="toc-text">练习</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/async-computation/" rel="bookmark" title="async-computation">async-computation</a></li><li class="active"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/hardware/" rel="bookmark" title="hardware">hardware</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/auto-parallelism/" rel="bookmark" title="auto-parallelism">auto-parallelism</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/multiple-gpus-concise/" rel="bookmark" title="multiple-gpus-concise">multiple-gpus-concise</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/index/" rel="bookmark" title="index">index</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/hybridize/" rel="bookmark" title="hybridize">hybridize</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/parameterserver/" rel="bookmark" title="parameterserver">parameterserver</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/multiple-gpus/" rel="bookmark" title="multiple-gpus">multiple-gpus</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="yuan" data-src="/images/avatar.jpg"><p class="name" itemprop="name">yuan</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">429</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">72</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">61</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item email" data-url="bWFpbHRvOjIwODM2MzU1MjVAcXEuY29t" title="mailto:2083635525@qq.com"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li><li class="item"><a href="/links/" rel="section"><i class="ic i-magic"></i>链接</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/auto-parallelism/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_attention-mechanisms/transformer/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"></div><span><a href="/2023/06/25/computer-science/%E6%AF%94%E8%B5%9B/%E8%93%9D%E6%A1%A5%E6%9D%AF/%E7%9F%A5%E8%AF%86%E7%82%B9%E6%B1%87%E6%80%BB/JavaScript/%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%E7%82%B9/" title="未命名">未命名</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/" title="分类于 nlp">nlp</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/huggingface/" title="分类于 huggingface">huggingface</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/huggingface/Tokenizer%E5%BA%93/" title="分类于 Tokenizer库">Tokenizer库</a></div><span><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/%E6%A0%B9%E6%8D%AE%E5%B7%B2%E6%9C%89%E7%9A%84tokenizer%E8%AE%AD%E7%BB%83%E6%96%B0%E7%9A%84tokenizer/" title="根据已有的tokenizer训练新的tokenizer">根据已有的tokenizer训练新的tokenizer</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-convolutional-modern/" title="分类于 chapter_convolutional-modern">chapter_convolutional-modern</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-modern/batch-norm/" title="batch-norm">batch-norm</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/tools/" title="分类于 tools">tools</a> <i class="ic i-angle-right"></i> <a href="/categories/tools/%E7%88%AC%E8%99%AB/" title="分类于 爬虫">爬虫</a></div><span><a href="/2022/08/26/tools/%E7%88%AC%E8%99%AB/scrapy-Item-%E5%8A%A0%E8%BD%BD%E5%99%A8/" title="scrapy-Item-加载器">scrapy-Item-加载器</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2023/08/17/computer-science/algorithm/%E5%9B%BE%E8%A7%A3%E7%AE%97%E6%B3%95%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%90%9C%E7%B4%A2%E4%B8%8E%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/" title="未命名">未命名</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-natural-language-processing-pretraining/" title="分类于 chapter_natural-language-processing-pretraining">chapter_natural-language-processing-pretraining</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_natural-language-processing-pretraining/subword-embedding/" title="subword-embedding">subword-embedding</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/" title="分类于 nlp">nlp</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/huggingface/" title="分类于 huggingface">huggingface</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81nlp%E4%BB%BB%E5%8A%A1/" title="分类于 主要nlp任务">主要nlp任务</a></div><span><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E9%97%AE%E7%AD%94/" title="问答 question answer">问答 question answer</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-preliminaries/" title="分类于 chapter_preliminaries">chapter_preliminaries</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_preliminaries/linear-algebra/" title="linear-algebra">linear-algebra</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-recurrent-modern/" title="分类于 chapter_recurrent-modern">chapter_recurrent-modern</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_recurrent-modern/seq2seq/" title="seq2seq">seq2seq</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-optimization/" title="分类于 chapter_optimization">chapter_optimization</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_optimization/gd/" title="gd">gd</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">yuan @ Mi Manchi</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">2.9m 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">44:38</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2023/02/15/ai/pytorch深度学习/chapter_computational-performance/hardware/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html>