<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="yuan" href="https://jyuanhust.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="yuan" href="https://jyuanhust.github.io/atom.xml"><link rel="alternate" type="application/json" title="yuan" href="https://jyuanhust.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="chapter_multilayer-perceptrons"><link rel="canonical" href="https://jyuanhust.github.io/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_multilayer-perceptrons/underfit-overfit/"><title>underfit-overfit - chapter_multilayer-perceptrons - pytorch深度学习 - ai | Mi Manchi = yuan = Whatever is worth doing at all is worth doing well</title><meta name="generator" content="Hexo 6.2.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">underfit-overfit</h1><div class="meta"><span class="item" title="创建时间：2023-02-15 00:00:00"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2023-02-15T00:00:00+08:00">2023-02-15</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>9.7k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>9 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Mi Manchi</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(62).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(66).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(28).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(49).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(74).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(1).webp"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/" itemprop="item" rel="index" title="分类于 ai"><span itemprop="name">ai</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="item" rel="index" title="分类于 pytorch深度学习"><span itemprop="name">pytorch深度学习</span></a><meta itemprop="position" content="2"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-multilayer-perceptrons/" itemprop="item" rel="index" title="分类于 chapter_multilayer-perceptrons"><span itemprop="name">chapter_multilayer-perceptrons</span></a><meta itemprop="position" content="3"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://jyuanhust.github.io/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_multilayer-perceptrons/underfit-overfit/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="yuan"><meta itemprop="description" content="Whatever is worth doing at all is worth doing well, "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="yuan"></span><div class="body md" itemprop="articleBody"><h1 id="模型选择-欠拟合和过拟合"><a class="anchor" href="#模型选择-欠拟合和过拟合">#</a> 模型选择、欠拟合和过拟合</h1><p>🏷 <code>sec_model_selection</code></p><p>作为机器学习科学家，我们的目标是发现<em>模式</em>（pattern）。<br>但是，我们如何才能确定模型是真正发现了一种泛化的模式，<br>而不是简单地记住了数据呢？<br>例如，我们想要在患者的基因数据与痴呆状态之间寻找模式，<br>其中标签是从集合<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mtext>痴呆</mtext><mo separator="true">,</mo><mtext>轻度认知障碍</mtext><mo separator="true">,</mo><mtext>健康</mtext><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\text{痴呆}, \text{轻度认知障碍}, \text{健康}\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">{</span><span class="mord text"><span class="mord cjk_fallback">痴呆</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord text"><span class="mord cjk_fallback">轻度认知障碍</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord text"><span class="mord cjk_fallback">健康</span></span><span class="mclose">}</span></span></span></span> 中提取的。<br>因为基因可以唯一确定每个个体（不考虑双胞胎），<br>所以在这个任务中是有可能记住整个数据集的。</p><p>我们不想让模型只会做这样的事情：“那是鲍勃！我记得他！他有痴呆症！”。<br>原因很简单：当我们将来部署该模型时，模型需要判断从未见过的患者。<br>只有当模型真正发现了一种泛化模式时，才会作出有效的预测。</p><p>更正式地说，我们的目标是发现某些模式，<br>这些模式捕捉到了我们训练集潜在总体的规律。<br>如果成功做到了这点，即使是对以前从未遇到过的个体，<br>模型也可以成功地评估风险。<br>如何发现可以泛化的模式是机器学习的根本问题。</p><p>困难在于，当我们训练模型时，我们只能访问数据中的小部分样本。<br>最大的公开图像数据集包含大约一百万张图像。<br>而在大部分时候，我们只能从数千或数万个数据样本中学习。<br>在大型医院系统中，我们可能会访问数十万份医疗记录。<br>当我们使用有限的样本时，可能会遇到这样的问题：<br>当收集到更多的数据时，会发现之前找到的明显关系并不成立。</p><p>将模型在训练数据上拟合的比在潜在分布中更接近的现象称为<em>过拟合</em>（overfitting），<br>用于对抗过拟合的技术称为<em>正则化</em>（regularization）。<br>在前面的章节中，有些读者可能在用 Fashion-MNIST 数据集做实验时已经观察到了这种过拟合现象。<br>在实验中调整模型架构或超参数时会发现：<br>如果有足够多的神经元、层数和训练迭代周期，<br>模型最终可以在训练集上达到完美的精度，此时测试集的准确性却下降了。</p><h2 id="训练误差和泛化误差"><a class="anchor" href="#训练误差和泛化误差">#</a> 训练误差和泛化误差</h2><p>为了进一步讨论这一现象，我们需要了解训练误差和泛化误差。<br><em>训练误差</em>（training error）是指，<br>模型在训练数据集上计算得到的误差。<br><em>泛化误差</em>（generalization error）是指，<br>模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。</p><p>问题是，我们永远不能准确地计算出泛化误差。<br>这是因为无限多的数据样本是一个虚构的对象。<br>在实际中，我们只能通过将模型应用于一个独立的测试集来估计泛化误差，<br>该测试集由随机选取的、未曾在训练集中出现的数据样本构成。</p><p>下面的三个思维实验将有助于更好地说明这种情况。<br>假设一个大学生正在努力准备期末考试。<br>一个勤奋的学生会努力做好练习，并利用往年的考试题目来测试自己的能力。<br>尽管如此，在过去的考试题目上取得好成绩并不能保证他会在真正考试时发挥出色。<br>例如，学生可能试图通过死记硬背考题的答案来做准备。<br>他甚至可以完全记住过去考试的答案。<br>另一名学生可能会通过试图理解给出某些答案的原因来做准备。<br>在大多数情况下，后者会考得更好。</p><p>类似地，考虑一个简单地使用查表法来回答问题的模型。<br>如果允许的输入集合是离散的并且相当小，<br>那么也许在查看许多训练样本后，该方法将执行得很好。<br>但当这个模型面对从未见过的例子时，它表现的可能比随机猜测好不到哪去。<br>这是因为输入空间太大了，远远不可能记住每一个可能的输入所对应的答案。<br>例如，考虑<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">28\times28</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">2</span><span class="mord">8</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span><span class="mord">8</span></span></span></span> 的灰度图像。<br>如果每个像素可以取<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>256</mn></mrow><annotation encoding="application/x-tex">256</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span></span></span></span> 个灰度值中的一个，<br>则有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>25</mn><msup><mn>6</mn><mn>784</mn></msup></mrow><annotation encoding="application/x-tex">256^{784}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141079999999999em;vertical-align:0"></span><span class="mord">2</span><span class="mord">5</span><span class="mord"><span class="mord">6</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">7</span><span class="mord mtight">8</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> 个可能的图像。<br>这意味着指甲大小的低分辨率灰度图像的数量比宇宙中的原子要多得多。<br>即使我们可能遇到这样的数据，我们也不可能存储整个查找表。</p><p>最后，考虑对掷硬币的结果（类别 0：正面，类别 1：反面）进行分类的问题。<br>假设硬币是公平的，无论我们想出什么算法，泛化误差始终是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>。<br>然而，对于大多数算法，我们应该期望训练误差会更低（取决于运气）。<br>考虑数据集 {0，1，1，1，0，1}。<br>我们的算法不需要额外的特征，将倾向于总是预测<em>多数类</em>，<br>从我们有限的样本来看，它似乎是 1 占主流。<br>在这种情况下，总是预测类 1 的模型将产生<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 的误差，<br>这比我们的泛化误差要好得多。<br>当我们逐渐增加数据量，正面比例明显偏离<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 的可能性将会降低，<br>我们的训练误差将与泛化误差相匹配。</p><h3 id="统计学习理论"><a class="anchor" href="#统计学习理论">#</a> 统计学习理论</h3><p>由于泛化是机器学习中的基本问题，<br>许多数学家和理论家毕生致力于研究描述这一现象的形式理论。<br>在<span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvR2xpdmVua28lRTIlODAlOTNDYW50ZWxsaV90aGVvcmVt">同名定理（eponymous theorem）</span>中，<br>格里文科和坎特利推导出了训练误差收敛到泛化误差的速率。<br>在一系列开创性的论文中，<br><span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvVmFwbmlrJUUyJTgwJTkzQ2hlcnZvbmVua2lzX3RoZW9yeQ==">Vapnik 和 Chervonenkis</span><br>将这一理论扩展到更一般种类的函数。<br>这项工作为统计学习理论奠定了基础。</p><p>在我们目前已探讨、并将在之后继续探讨的监督学习情景中，<br>我们假设训练数据和测试数据都是从相同的分布中独立提取的。<br>这通常被称为<em>独立同分布假设</em>（i.i.d. assumption），<br>这意味着对数据进行采样的过程没有进行 “记忆”。<br>换句话说，抽取的第 2 个样本和第 3 个样本的相关性，<br>并不比抽取的第 2 个样本和第 200 万个样本的相关性更强。</p><p>要成为一名优秀的机器学习科学家需要具备批判性思考能力。<br>假设是存在漏洞的，即很容易找出假设失效的情况。<br>如果我们根据从加州大学旧金山分校医学中心的患者数据训练死亡风险预测模型，<br>并将其应用于马萨诸塞州综合医院的患者数据，结果会怎么样？<br>这两个数据的分布可能不完全一样。<br>此外，抽样过程可能与时间有关。<br>比如当我们对微博的主题进行分类时，<br>新闻周期会使得正在讨论的话题产生时间依赖性，从而违反独立性假设。</p><p>有时候我们即使轻微违背独立同分布假设，模型仍将继续运行得非常好。<br>比如，我们有许多有用的工具已经应用于现实，如人脸识别、语音识别和语言翻译。<br>毕竟，几乎所有现实的应用都至少涉及到一些违背独立同分布假设的情况。</p><p>有些违背独立同分布假设的行为肯定会带来麻烦。<br>比如，我们试图只用来自大学生的人脸数据来训练一个人脸识别系统，<br>然后想要用它来监测疗养院中的老人。<br>这不太可能有效，因为大学生看起来往往与老年人有很大的不同。</p><p>在接下来的章节中，我们将讨论因违背独立同分布假设而引起的问题。<br>目前，即使认为独立同分布假设是理所当然的，理解泛化性也是一个困难的问题。<br>此外，能够解释深层神经网络泛化性能的理论基础，<br>也仍在继续困扰着学习理论领域最伟大的学者们。</p><p>当我们训练模型时，我们试图找到一个能够尽可能拟合训练数据的函数。<br>但是如果它执行地 “太好了”，而不能对看不见的数据做到很好泛化，就会导致过拟合。<br>这种情况正是我们想要避免或控制的。<br>深度学习中有许多启发式的技术旨在防止过拟合。</p><h3 id="模型复杂性"><a class="anchor" href="#模型复杂性">#</a> 模型复杂性</h3><p>当我们有简单的模型和大量的数据时，我们期望泛化误差与训练误差相近。<br>当我们有更复杂的模型和更少的样本时，我们预计训练误差会下降，但泛化误差会增大。<br>模型复杂性由什么构成是一个复杂的问题。<br>一个模型是否能很好地泛化取决于很多因素。<br>例如，具有更多参数的模型可能被认为更复杂，<br>参数有更大取值范围的模型可能更为复杂。<br>通常对于神经网络，我们认为需要更多训练迭代的模型比较复杂，<br>而需要<em>早停</em>（early stopping）的模型（即较少训练迭代周期）就不那么复杂。</p><p>我们很难比较本质上不同大类的模型之间（例如，决策树与神经网络）的复杂性。<br>就目前而言，一条简单的经验法则相当有用：<br>统计学家认为，能够轻松解释任意事实的模型是复杂的，<br>而表达能力有限但仍能很好地解释数据的模型可能更有现实用途。<br>在哲学上，这与波普尔的科学理论的可证伪性标准密切相关：<br>如果一个理论能拟合数据，且有具体的测试可以用来证明它是错误的，那么它就是好的。<br>这一点很重要，因为所有的统计估计都是<em>事后归纳</em>。<br>也就是说，我们在观察事实之后进行估计，因此容易受到相关谬误的影响。<br>目前，我们将把哲学放在一边，坚持更切实的问题。</p><p>本节为了给出一些直观的印象，我们将重点介绍几个倾向于影响模型泛化的因素。</p><ol><li>可调整参数的数量。当可调整参数的数量（有时称为<em>自由度</em>）很大时，模型往往更容易过拟合。</li><li>参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。</li><li>训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。</li></ol><h2 id="模型选择"><a class="anchor" href="#模型选择">#</a> 模型选择</h2><p>在机器学习中，我们通常在评估几个候选模型后选择最终的模型。<br>这个过程叫做<em>模型选择</em>。<br>有时，需要进行比较的模型在本质上是完全不同的（比如，决策树与线性模型）。<br>又有时，我们需要比较不同的超参数设置下的同一类模型。</p><p>例如，训练多层感知机模型时，我们可能希望比较具有<br>不同数量的隐藏层、不同数量的隐藏单元以及不同的激活函数组合的模型。<br>为了确定候选模型中的最佳模型，我们通常会使用验证集。</p><h3 id="验证集"><a class="anchor" href="#验证集">#</a> 验证集</h3><p>原则上，在我们确定所有的超参数之前，我们不希望用到测试集。<br>如果我们在模型选择过程中使用测试数据，可能会有过拟合测试数据的风险，那就麻烦大了。<br>如果我们过拟合了训练数据，还可以在测试数据上的评估来判断过拟合。<br>但是如果我们过拟合了测试数据，我们又该怎么知道呢？</p><p>因此，我们决不能依靠测试数据进行模型选择。<br>然而，我们也不能仅仅依靠训练数据来选择模型，因为我们无法估计训练数据的泛化误差。</p><p>在实际应用中，情况变得更加复杂。<br>虽然理想情况下我们只会使用测试数据一次，<br>以评估最好的模型或比较一些模型效果，但现实是测试数据很少在使用一次后被丢弃。<br>我们很少能有充足的数据来对每一轮实验采用全新测试集。</p><p>解决此问题的常见做法是将我们的数据分成三份，<br>除了训练和测试数据集之外，还增加一个<em>验证数据集</em>（validation dataset），<br>也叫<em>验证集</em>（validation set）。<br>但现实是验证数据和测试数据之间的边界模糊得令人担忧。<br>除非另有明确说明，否则在这本书的实验中，<br>我们实际上是在使用应该被正确地称为训练数据和验证数据的数据集，<br>并没有真正的测试数据集。<br>因此，书中每次实验报告的准确度都是验证集准确度，而不是测试集准确度。</p><h3 id="k折交叉验证"><a class="anchor" href="#k折交叉验证">#</a> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span></span></span></span> 折交叉验证</h3><p>当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集。<br>这个问题的一个流行的解决方案是采用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span></span></span></span><em> 折交叉验证</em>。<br>这里，原始训练数据被分成<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span></span></span></span> 个不重叠的子集。<br>然后执行<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span></span></span></span> 次模型训练和验证，每次在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">K-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.76666em;vertical-align:-.08333em"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span> 个子集上进行训练，<br>并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。<br>最后，通过对<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span></span></span></span> 次实验的结果取平均来估计训练和验证误差。</p><h2 id="欠拟合还是过拟合"><a class="anchor" href="#欠拟合还是过拟合">#</a> 欠拟合还是过拟合？</h2><p>当我们比较训练和验证误差时，我们要注意两种常见的情况。<br>首先，我们要注意这样的情况：训练误差和验证误差都很严重，<br>但它们之间仅有一点差距。<br>如果模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足），<br>无法捕获试图学习的模式。<br>此外，由于我们的训练和验证误差之间的<em>泛化误差</em>很小，<br>我们有理由相信可以用一个更复杂的模型降低训练误差。<br>这种现象被称为<em>欠拟合</em>（underfitting）。</p><p>另一方面，当我们的训练误差明显低于验证误差时要小心，<br>这表明严重的<em>过拟合</em>（overfitting）。<br>注意，<em>过拟合</em>并不总是一件坏事。<br>特别是在深度学习领域，众所周知，<br>最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。<br>最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。</p><p>是否过拟合或欠拟合可能取决于模型复杂性和可用训练数据集的大小，<br>这两个点将在下面进行讨论。</p><h3 id="模型复杂性-2"><a class="anchor" href="#模型复杂性-2">#</a> 模型复杂性</h3><p>为了说明一些关于过拟合和模型复杂性的经典直觉，<br>我们给出一个多项式的例子。<br>给定由单个特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> 和对应实数标签<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span></span></span></span> 组成的训练数据，<br>我们试图找到下面的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">d</span></span></span></span> 阶多项式来估计标签<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span></span></span></span>。</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>d</mi></munderover><msup><mi>x</mi><mi>i</mi></msup><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\hat{y}= \sum_{i=0}^d x^i w_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.69444em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.19444em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.19444em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:3.1137820000000005em;vertical-align:-1.277669em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8361130000000003em"><span style="top:-1.872331em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.050005em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8746639999999999em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.02691em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></span></p><p>这只是一个线性回归问题，我们的特征是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> 的幂给出的，<br>模型的权重是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">w_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.02691em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 给出的，偏置是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">w_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.02691em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 给出的<br>（因为对于所有的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> 都有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mn>0</mn></msup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">x^0 = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141079999999999em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span>）。<br>由于这只是一个线性回归问题，我们可以使用平方误差作为我们的损失函数。</p><p>高阶多项式函数比低阶多项式函数复杂得多。<br>高阶多项式的参数较多，模型函数的选择范围较广。<br>因此在固定训练数据集的情况下，<br>高阶多项式函数相对于低阶多项式的训练误差应该始终更低（最坏也是相等）。<br>事实上，当数据样本包含了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> 的不同值时，<br>函数阶数等于数据样本数量的多项式函数可以完美拟合训练集。<br>在 :numref: <code>fig_capacity_vs_error</code> 中，<br>我们直观地描述了多项式的阶数和欠拟合与过拟合之间的关系。</p><p><img data-src="/./images/capacity-vs-error.svg" alt="模型复杂度对欠拟合和过拟合的影响"><br>🏷 <code>fig_capacity_vs_error</code></p><h3 id="数据集大小"><a class="anchor" href="#数据集大小">#</a> 数据集大小</h3><p>另一个重要因素是数据集的大小。<br>训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。<br>随着训练数据量的增加，泛化误差通常会减小。<br>此外，一般来说，更多的数据不会有什么坏处。<br>对于固定的任务和数据分布，模型复杂性和数据集大小之间通常存在关系。<br>给出更多的数据，我们可能会尝试拟合一个更复杂的模型。<br>能够拟合更复杂的模型可能是有益的。<br>如果没有足够的数据，简单的模型可能更有用。<br>对于许多任务，深度学习只有在有数千个训练样本时才优于线性模型。<br>从一定程度上来说，深度学习目前的生机要归功于<br>廉价存储、互联设备以及数字化经济带来的海量数据集。</p><h2 id="多项式回归"><a class="anchor" href="#多项式回归">#</a> 多项式回归</h2><p>我们现在可以 (<strong>通过多项式拟合来探索这些概念</strong>)。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> math</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">import</span> torch</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">from</span> torch <span class="token keyword">import</span> nn</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l</pre></td></tr></table></figure><h3 id="生成数据集"><a class="anchor" href="#生成数据集">#</a> 生成数据集</h3><p>给定<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span>，我们将 [<strong>使用以下三阶多项式来生成训练和测试数据的标签：</strong>]</p><p>(<strong>$$y = 5 + 1.2x - 3.4\frac<ruby>x<rp>(</rp><rt>2</rt><rp>)</rp></ruby><ruby 3!>2!} + 5.6 \frac{x<rp>(</rp><rt>3</rt><rp>)</rp></ruby>+ \epsilon \text{ where }<br>\epsilon \sim \mathcal{N}(0, 0.1^2).$$</strong>)</p><p>噪声项<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">ϵ</span></span></span></span> 服从均值为 0 且标准差为 0.1 的正态分布。<br>在优化的过程中，我们通常希望避免非常大的梯度值或损失值。<br>这就是我们将特征从<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">x^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.824664em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.824664em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span> 调整为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><msup><mi>x</mi><mi>i</mi></msup><mrow><mi>i</mi><mo stretchy="false">!</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{x^i}{i!}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3704599999999998em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0254599999999998em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mclose mtight">!</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.9020857142857143em"><span style="top:-2.931em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 的原因，<br>这样可以避免很大的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.65952em;vertical-align:0"></span><span class="mord mathnormal">i</span></span></span></span> 带来的特别大的指数值。<br>我们将为训练集和测试集各生成 100 个样本。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>max_degree <span class="token operator">=</span> <span class="token number">20</span>  <span class="token comment"># 多项式的最大阶数</span></pre></td></tr><tr><td data-num="2"></td><td><pre>n_train<span class="token punctuation">,</span> n_test <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">100</span>  <span class="token comment"># 训练和测试数据集大小</span></pre></td></tr><tr><td data-num="3"></td><td><pre>true_w <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_degree<span class="token punctuation">)</span>  <span class="token comment"># 分配大量的空间</span></pre></td></tr><tr><td data-num="4"></td><td><pre>true_w<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1.2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3.4</span><span class="token punctuation">,</span> <span class="token number">5.6</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre>features <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span>n_train <span class="token operator">+</span> n_test<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>features<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>poly_features <span class="token operator">=</span> np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>features<span class="token punctuation">,</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>max_degree<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_degree<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    poly_features<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">/=</span> math<span class="token punctuation">.</span>gamma<span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># gamma(n)=(n-1)!</span></pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token comment"># labels 的维度：(n_train+n_test,)</span></pre></td></tr><tr><td data-num="12"></td><td><pre>labels <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>poly_features<span class="token punctuation">,</span> true_w<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="13"></td><td><pre>labels <span class="token operator">+=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span>scale<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> size<span class="token operator">=</span>labels<span class="token punctuation">.</span>shape<span class="token punctuation">)</span></pre></td></tr></table></figure><p>同样，存储在 <code>poly_features</code> 中的单项式由 gamma 函数重新缩放，<br>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Γ</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mi>n</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">!</mo></mrow><annotation encoding="application/x-tex">\Gamma(n)=(n-1)!</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">Γ</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">1</span><span class="mclose">)</span><span class="mclose">!</span></span></span></span>。<br>从生成的数据集中 [<strong>查看一下前 2 个样本</strong>]，<br>第一个值是与偏置相对应的常量特征。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># NumPy ndarray 转换为 tensor</span></pre></td></tr><tr><td data-num="2"></td><td><pre>true_w<span class="token punctuation">,</span> features<span class="token punctuation">,</span> poly_features<span class="token punctuation">,</span> labels <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dtype<span class="token operator">=</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token punctuation">[</span>true_w<span class="token punctuation">,</span> features<span class="token punctuation">,</span> poly_features<span class="token punctuation">,</span> labels<span class="token punctuation">]</span><span class="token punctuation">]</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>features<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> poly_features<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> labels<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>(tensor([[-0.7408],
         [ 0.9021]]),
 tensor([[ 1.0000e+00, -7.4078e-01,  2.7438e-01, -6.7751e-02,  1.2547e-02,
          -1.8589e-03,  2.2951e-04, -2.4288e-05,  2.2490e-06, -1.8511e-07,
           1.3713e-08, -9.2346e-10,  5.7007e-11, -3.2484e-12,  1.7188e-13,
          -8.4884e-15,  3.9300e-16, -1.7125e-17,  7.0478e-19, -2.7478e-20],
         [ 1.0000e+00,  9.0208e-01,  4.0687e-01,  1.2234e-01,  2.7591e-02,
           4.9777e-03,  7.4838e-04,  9.6443e-05,  1.0875e-05,  1.0900e-06,
           9.8325e-08,  8.0633e-09,  6.0614e-10,  4.2061e-11,  2.7101e-12,
           1.6298e-13,  9.1889e-15,  4.8759e-16,  2.4436e-17,  1.1602e-18]]),
 tensor([2.9074, 5.2200]))
</code></pre><h3 id="对模型进行训练和测试"><a class="anchor" href="#对模型进行训练和测试">#</a> 对模型进行训练和测试</h3><p>首先让我们 [<strong>实现一个函数来评估模型在给定数据集上的损失</strong>]。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">def</span> <span class="token function">evaluate_loss</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> data_iter<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    <span class="token triple-quoted-string string">"""评估给定数据集上模型的损失"""</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    metric <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Accumulator<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 损失的总和，样本数量</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="5"></td><td><pre>        out <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>        y <span class="token operator">=</span> y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>out<span class="token punctuation">.</span>shape<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>        l <span class="token operator">=</span> loss<span class="token punctuation">(</span>out<span class="token punctuation">,</span> y<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>        metric<span class="token punctuation">.</span>add<span class="token punctuation">(</span>l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> l<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    <span class="token keyword">return</span> metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span></pre></td></tr></table></figure><p>现在 [<strong>定义训练函数</strong>]。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>train_features<span class="token punctuation">,</span> test_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span> test_labels<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="2"></td><td><pre>          num_epochs<span class="token operator">=</span><span class="token number">400</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    input_shape <span class="token operator">=</span> train_features<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    <span class="token comment"># 不设置偏置，因为我们已经在多项式中实现了它</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_shape<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    batch_size <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> train_labels<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    train_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_array<span class="token punctuation">(</span><span class="token punctuation">(</span>train_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="9"></td><td><pre>                                batch_size<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_array<span class="token punctuation">(</span><span class="token punctuation">(</span>test_features<span class="token punctuation">,</span> test_labels<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="11"></td><td><pre>                               batch_size<span class="token punctuation">,</span> is_train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre>    trainer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="13"></td><td><pre>    animator <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Animator<span class="token punctuation">(</span>xlabel<span class="token operator">=</span><span class="token string">'epoch'</span><span class="token punctuation">,</span> ylabel<span class="token operator">=</span><span class="token string">'loss'</span><span class="token punctuation">,</span> yscale<span class="token operator">=</span><span class="token string">'log'</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="14"></td><td><pre>                            xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">,</span> ylim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1e-3</span><span class="token punctuation">,</span> <span class="token number">1e2</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="15"></td><td><pre>                            legend<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="16"></td><td><pre>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="17"></td><td><pre>        d2l<span class="token punctuation">.</span>train_epoch_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> trainer<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="18"></td><td><pre>        <span class="token keyword">if</span> epoch <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">20</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="19"></td><td><pre>            animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>evaluate_loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="20"></td><td><pre>                                     evaluate_loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="21"></td><td><pre>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'weight:'</span><span class="token punctuation">,</span> net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></table></figure><h3 id="三阶多项式函数拟合正常"><a class="anchor" href="#三阶多项式函数拟合正常">#</a> [<strong>三阶多项式函数拟合 (正常)</strong>]</h3><p>我们将首先使用三阶多项式函数，它与数据生成函数的阶数相同。<br>结果表明，该模型能有效降低训练损失和测试损失。<br>学习到的模型参数也接近真实值<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>=</mo><mo stretchy="false">[</mo><mn>5</mn><mo separator="true">,</mo><mn>1.2</mn><mo separator="true">,</mo><mo>−</mo><mn>3.4</mn><mo separator="true">,</mo><mn>5.6</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">w = [5, 1.2, -3.4, 5.6]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">[</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">1</span><span class="mord">.</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">−</span><span class="mord">3</span><span class="mord">.</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">5</span><span class="mord">.</span><span class="mord">6</span><span class="mclose">]</span></span></span></span>。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># 从多项式特征中选择前 4 个维度，即 1,x,x^2/2!,x^3/3!</span></pre></td></tr><tr><td data-num="2"></td><td><pre>train<span class="token punctuation">(</span>poly_features<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> poly_features<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="3"></td><td><pre>      labels<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">]</span><span class="token punctuation">,</span> labels<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>weight: [[ 5.000446   1.1925726 -3.3994446  5.613429 ]]
</code></pre><p><img data-src="/underfit-overfit_files/underfit-overfit_12_1.svg" alt="svg"></p><h3 id="线性函数拟合欠拟合"><a class="anchor" href="#线性函数拟合欠拟合">#</a> [<strong>线性函数拟合 (欠拟合)</strong>]</h3><p>让我们再看看线性函数拟合，减少该模型的训练损失相对困难。<br>在最后一个迭代周期完成后，训练损失仍然很高。<br>当用来拟合非线性模式（如这里的三阶多项式函数）时，线性模型容易欠拟合。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># 从多项式特征中选择前 2 个维度，即 1 和 x</span></pre></td></tr><tr><td data-num="2"></td><td><pre>train<span class="token punctuation">(</span>poly_features<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> poly_features<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="3"></td><td><pre>      labels<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">]</span><span class="token punctuation">,</span> labels<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>weight: [[3.3861537 3.623168 ]]
</code></pre><p><img data-src="/underfit-overfit_files/underfit-overfit_14_1.svg" alt="svg"></p><h3 id="高阶多项式函数拟合过拟合"><a class="anchor" href="#高阶多项式函数拟合过拟合">#</a> [<strong>高阶多项式函数拟合 (过拟合)</strong>]</h3><p>现在，让我们尝试使用一个阶数过高的多项式来训练模型。<br>在这种情况下，没有足够的数据用于学到高阶系数应该具有接近于零的值。<br>因此，这个过于复杂的模型会轻易受到训练数据中噪声的影响。<br>虽然训练损失可以有效地降低，但测试损失仍然很高。<br>结果表明，复杂模型对数据造成了过拟合。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># 从多项式特征中选取所有维度</span></pre></td></tr><tr><td data-num="2"></td><td><pre>train<span class="token punctuation">(</span>poly_features<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> poly_features<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="3"></td><td><pre>      labels<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">]</span><span class="token punctuation">,</span> labels<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> num_epochs<span class="token operator">=</span><span class="token number">1500</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>weight: [[ 4.976002    1.2835668  -3.2593832   5.150967   -0.41944516  1.348935
  -0.17952554  0.15169154 -0.15447472  0.15434313  0.11540707 -0.20328848
   0.20229222 -0.19785015 -0.13074256 -0.16738276  0.03405237 -0.03323355
  -0.08506632  0.14969553]]
</code></pre><p><img data-src="/underfit-overfit_files/underfit-overfit_16_1.svg" alt="svg"></p><p>在接下来的章节中，我们将继续讨论过拟合问题和处理这些问题的方法，例如权重衰减和 dropout。</p><h2 id="小结"><a class="anchor" href="#小结">#</a> 小结</h2><ul><li>欠拟合是指模型无法继续减少训练误差。过拟合是指训练误差远小于验证误差。</li><li>由于不能基于训练误差来估计泛化误差，因此简单地最小化训练误差并不一定意味着泛化误差的减小。机器学习模型需要注意防止过拟合，即防止泛化误差过大。</li><li>验证集可以用于模型选择，但不能过于随意地使用它。</li><li>我们应该选择一个复杂度适当的模型，避免使用数量不足的训练样本。</li></ul><h2 id="练习"><a class="anchor" href="#练习">#</a> 练习</h2><ol><li>这个多项式回归问题可以准确地解出吗？提示：使用线性代数。</li><li>考虑多项式的模型选择。<ol><li>绘制训练损失与模型复杂度（多项式的阶数）的关系图。观察到了什么？需要多少阶的多项式才能将训练损失减少到 0?</li><li>在这种情况下绘制测试的损失图。</li><li>生成同样的图，作为数据量的函数。</li></ol></li><li>如果不对多项式特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">x^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.824664em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.824664em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span> 进行标准化 (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi>i</mi><mo stretchy="false">!</mo></mrow><annotation encoding="application/x-tex">1/i!</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">1</span><span class="mord">/</span><span class="mord mathnormal">i</span><span class="mclose">!</span></span></span></span>)，会发生什么事情？能用其他方法解决这个问题吗？</li><li>泛化误差可能为零吗？</li></ol><p><span class="exturl" data-url="aHR0cHM6Ly9kaXNjdXNzLmQybC5haS90LzE4MDY=">Discussions</span></p><div class="tags"><a href="/tags/chapter-multilayer-perceptrons/" rel="tag"><i class="ic i-tag"></i> chapter_multilayer-perceptrons</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2023-03-04 13:38:25" itemprop="dateModified" datetime="2023-03-04T13:38:25+08:00">2023-03-04</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="yuan 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="yuan 支付宝"><p>支付宝</p></div><div><img data-src="/images/paypal.png" alt="yuan 贝宝"><p>贝宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>yuan <i class="ic i-at"><em>@</em></i>yuan</li><li class="link"><strong>本文链接：</strong> <a href="https://jyuanhust.github.io/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_multilayer-perceptrons/underfit-overfit/" title="underfit-overfit">https://jyuanhust.github.io/2023/02/15/ai/pytorch深度学习/chapter_multilayer-perceptrons/underfit-overfit/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_natural-language-processing-applications/natural-language-inference-attention/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;gitee.com&#x2F;zkz0&#x2F;image&#x2F;raw&#x2F;master&#x2F;img&#x2F;img(49).webp" title="natural-language-inference-attention"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> chapter_natural-language-processing-applications</span><h3>natural-language-inference-attention</h3></a></div><div class="item right"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_natural-language-processing-applications/natural-language-inference-and-dataset/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;gitee.com&#x2F;zkz0&#x2F;image&#x2F;raw&#x2F;master&#x2F;img&#x2F;img(1).webp" title="natural-language-inference-and-dataset"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> chapter_natural-language-processing-applications</span><h3>natural-language-inference-and-dataset</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9-%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">1.</span> <span class="toc-text">模型选择、欠拟合和过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E5%92%8C%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE"><span class="toc-number">1.1.</span> <span class="toc-text">训练误差和泛化误差</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA"><span class="toc-number">1.1.1.</span> <span class="toc-text">统计学习理论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82%E6%80%A7"><span class="toc-number">1.1.2.</span> <span class="toc-text">模型复杂性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-number">1.2.</span> <span class="toc-text">模型选择</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="toc-number">1.2.1.</span> <span class="toc-text">验证集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#k%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">1.2.2.</span> <span class="toc-text">KKK 折交叉验证</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88%E8%BF%98%E6%98%AF%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">1.3.</span> <span class="toc-text">欠拟合还是过拟合？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82%E6%80%A7-2"><span class="toc-number">1.3.1.</span> <span class="toc-text">模型复杂性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A4%A7%E5%B0%8F"><span class="toc-number">1.3.2.</span> <span class="toc-text">数据集大小</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="toc-number">1.4.</span> <span class="toc-text">多项式回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.4.1.</span> <span class="toc-text">生成数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95"><span class="toc-number">1.4.2.</span> <span class="toc-text">对模型进行训练和测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E9%98%B6%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%87%BD%E6%95%B0%E6%8B%9F%E5%90%88%E6%AD%A3%E5%B8%B8"><span class="toc-number">1.4.3.</span> <span class="toc-text">[三阶多项式函数拟合 (正常)]</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-number">1.4.4.</span> <span class="toc-text">[线性函数拟合 (欠拟合)]</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E9%98%B6%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%87%BD%E6%95%B0%E6%8B%9F%E5%90%88%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">1.4.5.</span> <span class="toc-text">[高阶多项式函数拟合 (过拟合)]</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.5.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0"><span class="toc-number">1.6.</span> <span class="toc-text">练习</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_multilayer-perceptrons/backprop/" rel="bookmark" title="backprop">backprop</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_multilayer-perceptrons/dropout/" rel="bookmark" title="dropout">dropout</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_multilayer-perceptrons/index/" rel="bookmark" title="index">index</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_multilayer-perceptrons/environment/" rel="bookmark" title="environment">environment</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_multilayer-perceptrons/mlp-concise/" rel="bookmark" title="mlp-concise">mlp-concise</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_multilayer-perceptrons/kaggle-house-price/" rel="bookmark" title="kaggle-house-price">kaggle-house-price</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_multilayer-perceptrons/mlp-scratch/" rel="bookmark" title="mlp-scratch">mlp-scratch</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_multilayer-perceptrons/numerical-stability-and-init/" rel="bookmark" title="numerical-stability-and-init">numerical-stability-and-init</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_multilayer-perceptrons/mlp/" rel="bookmark" title="mlp">mlp</a></li><li><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_multilayer-perceptrons/weight-decay/" rel="bookmark" title="weight-decay">weight-decay</a></li><li class="active"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_multilayer-perceptrons/underfit-overfit/" rel="bookmark" title="underfit-overfit">underfit-overfit</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="yuan" data-src="/images/avatar.jpg"><p class="name" itemprop="name">yuan</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">429</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">72</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">61</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item email" data-url="bWFpbHRvOjIwODM2MzU1MjVAcXEuY29t" title="mailto:2083635525@qq.com"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li><li class="item"><a href="/links/" rel="section"><i class="ic i-magic"></i>链接</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_natural-language-processing-applications/natural-language-inference-attention/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_natural-language-processing-applications/natural-language-inference-and-dataset/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/cv/" title="分类于 cv">cv</a></div><span><a href="/2022/07/21/ai/cv/OpenCV%E8%BD%AE%E5%BB%93%E6%A3%80%E6%B5%8B/" title="OpenCV轮廓检测">OpenCV轮廓检测</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/cv/" title="分类于 cv">cv</a></div><span><a href="/2022/09/07/ai/cv/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/" title="图像分类">图像分类</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch/" title="分类于 pytorch">pytorch</a></div><span><a href="/2022/08/24/ai/pytorch/Pytorch%E7%9A%84data-norm%EF%BC%88%E5%87%A0%E7%A7%8D%E8%8C%83%E6%95%B0-norm-%E7%9A%84%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%EF%BC%89/" title="Pytorch的data.norm（几种范数(norm)的详细介绍）">Pytorch的data.norm（几种范数(norm)的详细介绍）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/nlp/" title="分类于 nlp">nlp</a> <i class="ic i-angle-right"></i> <a href="/categories/nlp/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/nlp/ai/base/" title="分类于 base">base</a></div><span><a href="/2022/08/24/ai/nlp/base/GPT%E8%AE%BA%E6%96%87/" title="gpt">gpt</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Python/" title="分类于 Python">Python</a></div><span><a href="/2022/08/24/language/python/Python%E6%A8%A1%E5%9D%97os-system/" title="Python模块os.system()">Python模块os.system()</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2023/06/25/computer-science/%E6%AF%94%E8%B5%9B/%E8%93%9D%E6%A1%A5%E6%9D%AF/%E7%9F%A5%E8%AF%86%E7%82%B9%E6%B1%87%E6%80%BB/Vue/%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%E7%82%B92/" title="未命名">未命名</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/csp/" title="分类于 csp">csp</a></div><span><a href="/2022/08/24/computer-science/algorithm/csp/2%E9%A2%98/202109-2-%E9%9D%9E%E9%9B%B6%E6%AE%B5%E5%88%92%E5%88%86/" title="202109-2-非零段划分">202109-2-非零段划分</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-deep-learning-computation/" title="分类于 chapter_deep-learning-computation">chapter_deep-learning-computation</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_deep-learning-computation/custom-layer/" title="custom-layer">custom-layer</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 computer-science">computer-science</a></div><span><a href="/2022/03/05/computer-science/base/MAML/MAML%E5%92%8CHierarchical-MAML/" title="MAML 和 Hierarchical-MAML">MAML 和 Hierarchical-MAML</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2023/09/07/computer-science/algorithm/%E5%8D%8E%E7%A7%91%E6%9C%BA%E8%AF%95/note/" title="未命名">未命名</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">yuan @ Mi Manchi</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">2.9m 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">44:38</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2023/02/15/ai/pytorch深度学习/chapter_multilayer-perceptrons/underfit-overfit/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html>