<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="yuan" href="https://jyuanhust.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="yuan" href="https://jyuanhust.github.io/atom.xml"><link rel="alternate" type="application/json" title="yuan" href="https://jyuanhust.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><link rel="canonical" href="https://jyuanhust.github.io/page/29/"><title>Mi Manchi = yuan = Whatever is worth doing at all is worth doing well</title><meta name="generator" content="Hexo 6.2.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><a href="/" class="logo" rel="start"><p class="artboard">Mi Manchi</p><h1 itemprop="name headline" class="title">yuan</h1></a><p class="meta" itemprop="description">= Whatever is worth doing at all is worth doing well =</p></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Mi Manchi</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(17).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(68).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(48).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(34).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(96).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(15).webp"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="index wrap"><div class="segments posts"><article class="item"><div class="cover"><a href="/2022/11/12/backend/SpringMVC/SpringMVC_day02/SpringMVC_day02/" itemprop="url" title="SpringMVC_day02"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(18).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 22:42:40"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T22:42:40+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>42k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>38 分钟</span></span></div><h3><a href="/2022/11/12/backend/SpringMVC/SpringMVC_day02/SpringMVC_day02/" itemprop="url" title="SpringMVC_day02">SpringMVC_day02</a></h3><div class="excerpt"># SpringMVC_day02 今日内容 完成 SSM 的整合开发 能够理解并实现统一结果封装与统一异常处理 能够完成前后台功能整合开发 掌握拦截器的编写 # 1，SSM 整合 前面我们已经把 Mybatis 、 Spring 和 SpringMVC 三个框架进行了学习，今天主要的内容就是把这三个框架整合在一起完成我们的业务功能开发，具体如何来整合，我们一步步来学习。 # 1.1 流程分析 (1) 创建工程 创建一个 Maven 的 web 工程 pom.xml 添加 SSM 需要的依赖 jar 包 编写 Web 项目的入口配置类，实现...</div><div class="meta footer"><span><a href="/categories/backend/spring/SpringMVC/" itemprop="url" title="SpringMVC"><i class="ic i-flag"></i>SpringMVC</a></span></div><a href="/2022/11/12/backend/SpringMVC/SpringMVC_day02/SpringMVC_day02/" itemprop="url" title="SpringMVC_day02" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/backend/SpringMVC/SpringMVC_day01/SpringMVC_day01/" itemprop="url" title="SpringMVC_day01"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(50).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 22:42:40"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T22:42:40+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>55k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>50 分钟</span></span></div><h3><a href="/2022/11/12/backend/SpringMVC/SpringMVC_day01/SpringMVC_day01/" itemprop="url" title="SpringMVC_day01">SpringMVC_day01</a></h3><div class="excerpt"># SpringMVC_day01 今日内容 理解 SpringMVC 相关概念 完成 SpringMVC 的入门案例 学会使用 PostMan 工具发送请求和数据 掌握 SpringMVC 如何接收请求、数据和响应结果 掌握 RESTful 风格及其使用 完成基于 RESTful 的案例编写 SpringMVC 是隶属于 Spring 框架的一部分，主要是用来进行 Web 开发，是对 Servlet 进行了封装。 对于 SpringMVC 我们主要学习如下内容: SpringMVC 简介 请求与响应 REST 风格 SSM 整合 (注解版) 拦截器 SpringMVC 是处于 Web...</div><div class="meta footer"><span><a href="/categories/backend/spring/SpringMVC/" itemprop="url" title="SpringMVC"><i class="ic i-flag"></i>SpringMVC</a></span></div><a href="/2022/11/12/backend/SpringMVC/SpringMVC_day01/SpringMVC_day01/" itemprop="url" title="SpringMVC_day01" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/note/" itemprop="url" title="NLP和transformer大类概述"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(57).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>192</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>1 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/note/" itemprop="url" title="NLP和transformer大类概述">NLP和transformer大类概述</a></h3><div class="excerpt">对于 head 的理解，预训练的模型可能自己本身是带有 head 的，如果使用 autoModel 的话，那么就会自动加上这个 Head。但如果想要利用这个预训练模型，然后调整下游任务，那么就要换上特定的 token，这是就得使用 AutoModelFor...，这样模型就会自动替换原先的 head，然后就可以从头开始训练了。 但是因为还是使用原来的模型，所以 AutoTokenizer 还是可以继续用的。</div><div class="meta footer"><span><a href="/categories/ai/huggingface/" itemprop="url" title="huggingface"><i class="ic i-flag"></i>huggingface</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/note/" itemprop="url" title="NLP和transformer大类概述" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE/" itemprop="url" title="微调一个预训练模型-处理数据"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(69).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>11k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>10 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE/" itemprop="url" title="微调一个预训练模型-处理数据">微调一个预训练模型-处理数据</a></h3><div class="excerpt"># 处理数据 下面是我们用模型中心的数据在 PyTorch 上训练句子分类器的一个例子： import torchfrom transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification# Same as beforecheckpoint = &quot;bert-base-uncased&quot;tokenizer = AutoTokenizer.from_pretrained(checkpoint)model =...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" itemprop="url" title="微调一个预训练模型"><i class="ic i-flag"></i>微调一个预训练模型</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE/" itemprop="url" title="微调一个预训练模型-处理数据" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/NormalizationAndPre-tokenization/" itemprop="url" title="Normalization and pre-tokenization"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(11).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>371</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>1 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/NormalizationAndPre-tokenization/" itemprop="url" title="Normalization and pre-tokenization">Normalization and pre-tokenization</a></h3><div class="excerpt"># Normalization and pre-tokenization 标准化和预标记化 在我们更深入地研究与 Transformer 模型（字节对编码 Byte-Pair Encoding [BPE]、WordPiece 和 Unigram）一起使用的三种最常见的子词标记化算法之前，我们将首先看一下每个标记器 tokenizer 应用于文本的预处理。以下是 tokenization pipeline 标记化管道 中步骤的高级概述： 在将文本拆分为子标记之前（根据其模型），分词器执行两个步骤： normalization 和 pre-tokenization. #...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/Tokenizer%E5%BA%93/" itemprop="url" title="Tokenizer库"><i class="ic i-flag"></i>Tokenizer库</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/NormalizationAndPre-tokenization/" itemprop="url" title="Normalization and pre-tokenization" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E6%8E%A9%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" itemprop="url" title="微调一个掩码语言模型"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(43).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>18k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>16 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E6%8E%A9%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" itemprop="url" title="微调一个掩码语言模型">微调一个掩码语言模型</a></h3><div class="excerpt"># 微调掩码语言模型 对于许多涉及 Transformer 模型的 NLP 程序，你可以简单地从 Hugging Face Hub 中获取一个预训练的模型，然后直接在你的数据上对其进行微调，以完成手头的任务。只要用于预训练的语料库与用于微调的语料库没有太大区别，迁移学习通常会产生很好的结果。 但是，在某些情况下，你需要先微调数据上的语言模型，然后再训练特定于任务的 head。例如，如果您的数据集包含法律合同或科学文章，像 BERT 这样的普通 Transformer...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81nlp%E4%BB%BB%E5%8A%A1/" itemprop="url" title="主要nlp任务"><i class="ic i-flag"></i>主要nlp任务</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E6%8E%A9%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" itemprop="url" title="微调一个掩码语言模型" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81Summarization/" itemprop="url" title="文本摘要 summarize"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(95).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>6.1k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>6 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81Summarization/" itemprop="url" title="文本摘要 summarize">文本摘要 summarize</a></h3><div class="excerpt"># 文本摘要 在本节中，我们将看看如何使用 Transformer 模型将长文档压缩为摘要，这项任务称为文本摘要。这是最具挑战性的 NLP 任务之一，因为它需要一系列能力，例如理解长篇文章和生成能够捕捉文档中主要主题的连贯文本。但是，如果做得好，文本摘要是一种强大的工具，可以减轻领域专家详细阅读长文档的负担，从而加快各种业务流程。 尽管在 Hugging Face Hub 上已经存在各种微调模型用于文本摘要，几乎所有这些都只适用于英文文档。因此，为了在本节中添加一些变化，我们将为英语和西班牙语训练一个双语模型。在本节结束时，您将有一个可以总结客户评论的模型。 #...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81nlp%E4%BB%BB%E5%8A%A1/" itemprop="url" title="主要nlp任务"><i class="ic i-flag"></i>主要nlp任务</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81Summarization/" itemprop="url" title="文本摘要 summarize" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/Unigram%E6%A0%87%E8%AE%B0%E5%8C%96/" itemprop="url" title="Unigram"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(12).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>104</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>1 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/Unigram%E6%A0%87%E8%AE%B0%E5%8C%96/" itemprop="url" title="Unigram">Unigram</a></h3><div class="excerpt"># Unigram 标记化 Unigram tokenization 在 SentencePiece 中经常使用 Unigram 算法，该算法是 AlBERT、T5、mBART、Big Bird 和 XLNet 等模型使用的标记化算法。</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/Tokenizer%E5%BA%93/" itemprop="url" title="Tokenizer库"><i class="ic i-flag"></i>Tokenizer库</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/Unigram%E6%A0%87%E8%AE%B0%E5%8C%96/" itemprop="url" title="Unigram" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E9%97%AE%E7%AD%94/" itemprop="url" title="问答 question answer"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(93).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>18k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>16 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E9%97%AE%E7%AD%94/" itemprop="url" title="问答 question answer">问答 question answer</a></h3><div class="excerpt"># 问答 Question answering 是时候看问答了！这项任务有多种形式，但我们将在本节中关注的一项称为提取的问答 extractive question answering。问题的答案就在 给定的文档 之中。 我们将使用 SQuAD 数据集 微调一个 BERT 模型，其中包括群众工作者对一组维基百科文章提出的问题。 像 BERT 这样的纯编码器模型往往很擅长提取诸如 “谁发明了 Transformer 架构？” 之类的事实性问题的答案。但在给出诸如 “为什么天空是蓝色的？” 之类的开放式问题时表现不佳。在这些更具挑战性的情况下，T5 和 BART 等编码器 -...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81nlp%E4%BB%BB%E5%8A%A1/" itemprop="url" title="主要nlp任务"><i class="ic i-flag"></i>主要nlp任务</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E9%97%AE%E7%AD%94/" itemprop="url" title="问答 question answer" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/BPE/" itemprop="url" title="Byte-Pair Encoding tokenization"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(45).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>7.3k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>7 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/BPE/" itemprop="url" title="Byte-Pair Encoding tokenization">Byte-Pair Encoding tokenization</a></h3><div class="excerpt"># Byte-Pair Encoding tokenization 字节对编码 (BPE) 最初被开发为一种压缩文本的算法，然后在预训练 GPT 模型时被 OpenAI 用于标记化。许多 Transformer 模型都使用它，包括 GPT、GPT-2、RoBERTa、BART 和 DeBERTa。 # 训练算法 BPE 训练首先计算语料库中使用的唯一单词集 (在完成标准化和预标记化步骤之后), 然后通过获取用于编写这些单词的所有符号来构建词汇表。一个非常简单的例子，假设我们的语料库使用了这五个词: &quot;hug&quot;, &quot;pug&quot;,...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/Tokenizer%E5%BA%93/" itemprop="url" title="Tokenizer库"><i class="ic i-flag"></i>Tokenizer库</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/BPE/" itemprop="url" title="Byte-Pair Encoding tokenization" class="btn">more...</a></div></article></div></div><nav class="pagination"><div class="inner"><a class="extend prev" rel="prev" href="/page/28/"><i class="ic i-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/28/">28</a><span class="page-number current">29</span><a class="page-number" href="/page/30/">30</a><span class="space">&hellip;</span><a class="page-number" href="/page/43/">43</a><a class="extend next" rel="next" href="/page/30/"><i class="ic i-angle-right" aria-label="下一页"></i></a></div></nav></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"></div><div class="related panel pjax" data-title="系列文章"></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="yuan" data-src="/images/avatar.jpg"><p class="name" itemprop="name">yuan</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">429</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">72</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">61</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item email" data-url="bWFpbHRvOjIwODM2MzU1MjVAcXEuY29t" title="mailto:2083635525@qq.com"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li><li class="item"><a href="/links/" rel="section"><i class="ic i-magic"></i>链接</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/page/28/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/page/30/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/cv/" title="分类于 cv">cv</a></div><span><a href="/2022/09/07/ai/cv/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/" title="图像分类">图像分类</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-convolutional-modern/" title="分类于 chapter_convolutional-modern">chapter_convolutional-modern</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-modern/densenet/" title="densenet">densenet</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-appendix-tools-for-deep-learning/" title="分类于 chapter_appendix-tools-for-deep-learning">chapter_appendix-tools-for-deep-learning</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_appendix-tools-for-deep-learning/jupyter/" title="jupyter">jupyter</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 computer-science">computer-science</a></div><span><a href="/2023/02/27/computer-science/%E6%AF%94%E8%B5%9B/%E8%93%9D%E6%A1%A5%E6%9D%AF/note/" title="蓝桥杯省赛题目理解">蓝桥杯省赛题目理解</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/" title="分类于 nlp">nlp</a></div><span><a href="/2022/08/05/ai/nlp/base/Summarization-huggingface/" title="Summarization - huggingface">Summarization - huggingface</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/" title="分类于 nlp">nlp</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/base/" title="分类于 base">base</a></div><span><a href="/2022/08/24/ai/nlp/base/pytorch%E4%B8%ADLSTM%E7%9A%84output%E5%92%8Chidden%E5%85%B3%E7%B3%BB/" title="pytorch中LSTM的output和hidden关系">pytorch中LSTM的output和hidden关系</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-convolutional-neural-networks/" title="分类于 chapter_convolutional-neural-networks">chapter_convolutional-neural-networks</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-neural-networks/padding-and-strides/" title="padding-and-strides">padding-and-strides</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-computational-performance/" title="分类于 chapter_computational-performance">chapter_computational-performance</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/hybridize/" title="hybridize">hybridize</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/" title="分类于 nlp">nlp</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/huggingface/" title="分类于 huggingface">huggingface</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/huggingface/Tokenizer%E5%BA%93/" title="分类于 Tokenizer库">Tokenizer库</a></div><span><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/WordPiece/" title="WordPiece">WordPiece</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/cv/" title="分类于 cv">cv</a></div><span><a href="/2022/07/21/ai/cv/OpenCV%E8%BD%AE%E5%BB%93%E6%A3%80%E6%B5%8B/" title="OpenCV轮廓检测">OpenCV轮廓检测</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">yuan @ Mi Manchi</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">2.9m 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">44:38</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"page/29/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html>