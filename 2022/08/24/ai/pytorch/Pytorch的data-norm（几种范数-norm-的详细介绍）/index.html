<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="yuan" href="https://jyuanhust.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="yuan" href="https://jyuanhust.github.io/atom.xml"><link rel="alternate" type="application/json" title="yuan" href="https://jyuanhust.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><link rel="canonical" href="https://jyuanhust.github.io/2022/08/24/ai/pytorch/Pytorch%E7%9A%84data-norm%EF%BC%88%E5%87%A0%E7%A7%8D%E8%8C%83%E6%95%B0-norm-%E7%9A%84%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%EF%BC%89/"><title>Pytorch的data.norm（几种范数(norm)的详细介绍） - pytorch - ai | Mi Manchi = yuan = Whatever is worth doing at all is worth doing well</title><meta name="generator" content="Hexo 6.2.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">Pytorch的data.norm（几种范数(norm)的详细介绍）</h1><div class="meta"><span class="item" title="创建时间：2022-08-24 22:05:48"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2022-08-24T22:05:48+08:00">2022-08-24</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>2.8k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>3 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Mi Manchi</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(29).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(91).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(61).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(65).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(23).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(78).webp"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/" itemprop="item" rel="index" title="分类于 ai"><span itemprop="name">ai</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/pytorch/" itemprop="item" rel="index" title="分类于 pytorch"><span itemprop="name">pytorch</span></a><meta itemprop="position" content="2"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://jyuanhust.github.io/2022/08/24/ai/pytorch/Pytorch%E7%9A%84data-norm%EF%BC%88%E5%87%A0%E7%A7%8D%E8%8C%83%E6%95%B0-norm-%E7%9A%84%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%EF%BC%89/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="yuan"><meta itemprop="description" content="Whatever is worth doing at all is worth doing well, "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="yuan"></span><div class="body md" itemprop="articleBody"><h3 id="范数norm-几种范数的简单介绍datanorm使用"><a class="anchor" href="#范数norm-几种范数的简单介绍datanorm使用">#</a> 范数（norm） 几种范数的简单介绍 &amp; data.norm（）使用</h3><ul><li><p><a href="/#1_norm_1">1. 范数 (norm) 的简单介绍</a></p></li><li></li><li><p><a href="/#11_LP_13">1.1 L-P 范数</a></p></li><li><p><a href="/#12_L0_23">1.2 L0 范数</a></p></li><li><p><a href="/#13_L1_26">1.3 L1 范数</a></p></li><li><p><a href="/#14_L2_34">1.4 L2 范数</a></p></li><li><p><a href="/#15___47">1.5 ∞- 范数</a></p></li><li><p><a href="/#2__51">2. 矩阵范数</a></p></li><li></li><li><p><a href="/#21_1_52">2.1 1 - 范数</a></p></li><li><p><a href="/#22__2_56">2.2 2 - 范数</a></p></li><li><p><a href="/#23__62">2.3 ∞- 范数</a></p></li><li><p><a href="/#24_F_66">2.4 F - 范数</a></p></li><li><p><a href="/#26__70">2.6 核范数</a></p></li><li><p><a href="/#3_pytorchxnormp2dim1keepdimTrue_74">3. pytorch 中 x.norm (p=2,dim=1,keepdim=True) 的理解</a></p></li><li></li><li><p><a href="/#31__76">3.1 方法介绍</a></p></li><li><p><a href="/#32__83">3.2 函数参数</a></p></li><li><p><a href="/#33__90">3.3 实例演示</a></p></li><li></li><li><p><a href="/#331_dim_104">3.3.1 dim 参数</a></p></li><li><p><a href="/#332_keepdim_135">3.3.2 keepdim 参数</a></p></li></ul><h1 id="1-范数norm的简单介绍"><a class="anchor" href="#1-范数norm的简单介绍">#</a> 1. 范数 (norm) 的简单介绍</h1><p><strong>什么是范数？</strong></p><p>我们知道距离的定义是一个宽泛的概念，只要满足非负、自反、三角不等式就可以称之为距离。范数是一种强化了的距离概念，它在定义上比距离多了一条数乘的运算法则。有时候为了便于理解，我们可以把范数当作距离来理解。</p><p>在数学上，<strong>范数包括向量范数和矩阵范数，向量范数表征向量空间中向量的大小，矩阵范数表征矩阵引起变化的大小</strong>。</p><p>一种非严密的解释就是，对应向量范数，向量空间中的向量都是有大小的，这个大小如何度量，就是用范数来度量的，不同的范数都可以来度量这个大小，就好比米和尺都可以来度量远近一样；对于矩阵范数，学过线性代数，我们知道，通过运算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>X</mi><mo>=</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">AX=B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.05017em">B</span></span></span></span>，可以将向量 X 变化为 B，<strong>矩阵范数</strong>就是来度量这个变化大小的。</p><p>这里简单地介绍以下几种向量范数的定义和含义:</p><h2 id="11-l-p范数"><a class="anchor" href="#11-l-p范数">#</a> 1.1 L-P 范数</h2><p>与闵可夫斯基距离的定义一样，L-P 范数不是一个范数，而是一组范数，其定义如下：</p><p><img data-src="/../images/1ce6f79f63ea453586811b70302dac6e.png" alt="图 1"><br>根据 P 的变化，范数也有着不同的变化，一个经典的有关 P 范数的变化图如下：</p><p><img data-src="/../images/5a9032fa86c44e63bbb42e00f491d477.png" alt="图 2"><br>上图表示了 p 从无穷到 0 变化时，三维空间中到原点的距离（范数）为 1 的点构成的图形的变化情况。以常见的 L-2 范数（p=2）为例，此时的范数也即欧氏距离，空间中到原点的欧氏距离为 1 的点构成了一个球面。</p><h2 id="12-l0范数"><a class="anchor" href="#12-l0范数">#</a> 1.2 L0 范数</h2><p>当 P=0 时，也就是 L0 范数，由上面可知，L0 范数并不是一个真正的范数，它主要被用来度量向量中非零元素的个数。</p><h2 id="13-l1范数"><a class="anchor" href="#13-l1范数">#</a> 1.3 L1 范数</h2><p>L1 范数是我们经常见到的一种范数，它的定义如下：<br><img data-src="/../images/515b51199250497ab71869765d40a38d.png" alt="图 3"><br>表示向量 x 中非零元素的绝对值之和。</p><p>L1 范数有很多的名字，例如我们熟悉的曼哈顿距离、最小绝对误差等。使用 L1 范数可以度量两个向量间的差异，如<strong>绝对误差和</strong>（Sum of Absolute Difference）：</p><h2 id="14-l2范数"><a class="anchor" href="#14-l2范数">#</a> 1.4 L2 范数</h2><p>L2 范数是我们最常见最常用的范数了，我们用的最多的度量距离欧氏距离就是一种 L2 范数，它的定义如下：</p><p><img data-src="/../images/2d20790d968c43eb89897a6a5ca84687.png" alt="图 4"></p><p>Euclid 范数（欧几里得范数，常用计算向量长度），即向量元素绝对值的平方和再开方，pytorch 调用函数 norm (x, 2)。</p><p>像 L1 范数一样，L2 也可以度量两个向量间的差异，如平方差和（Sum of Squared Difference）: SSD</p><p><strong>L2 范数</strong>通常会被用来做优化目标函数的正则化项，防止模型为了迎合训练集而过于复杂造成过拟合的情况，从而提高模型的泛化能力。</p><h2 id="15-范数"><a class="anchor" href="#15-范数">#</a> 1.5 ∞- 范数</h2><p><img data-src="/../images/9a11add93ee74af094924b5947a8db7f.png" alt="图 5"><br>即所有向量元素绝对值中的最小值，matlab 调用函数 norm (x, -inf)。</p><h1 id="2-矩阵范数"><a class="anchor" href="#2-矩阵范数">#</a> 2. 矩阵范数</h1><h2 id="21-1-范数"><a class="anchor" href="#21-1-范数">#</a> 2.1 1 - 范数</h2><p><img data-src="/../images/8bb9cd98d9c64a1db0b1027e9bdc854a.png" alt="图 6"><br>列和范数，即所有矩阵列向量绝对值之和的最大值，matlab 调用函数 norm (A, 1)。</p><h2 id="22-2-范数"><a class="anchor" href="#22-2-范数">#</a> 2.2 2 - 范数</h2><p>对于实矩阵 A，它的谱范数定义为：</p><p><img data-src="/../images/f39be5ee6e894aea9194f27d1fac9d39.png" alt="图 7"><br>其中，eig (X) 为计算方阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span></span></span></span> 特征值，它返回特征值向量：谱范数，即 A’A 矩阵的最大特征值的开平方。matlab 调用函数 norm (x, 2)。</p><h2 id="23-范数"><a class="anchor" href="#23-范数">#</a> 2.3 ∞- 范数</h2><p><img data-src="/../images/cb9ad661da5f44718c04c66761448f31.png" alt="图 8"><br>行和范数，即所有矩阵行向量绝对值之和的最大值，matlab 调用函数 norm (A, inf)。</p><h2 id="24-f-范数"><a class="anchor" href="#24-f-范数">#</a> 2.4 F - 范数</h2><p><img data-src="/../images/48a986e1e0d24a648b33eb77b2973c04.png" alt="图 9"><br>Frobenius 范数，即矩阵元素绝对值的平方和再开平方，matlab 调用函数 norm (A, ’fro‘)。</p><h2 id="26-核范数"><a class="anchor" href="#26-核范数">#</a> 2.6 核范数</h2><p><img data-src="/../images/5d1463cae7cb4e58b09944967c9d7dde.png" alt="图 10"><br>是 A 的奇异值，核范数即奇异值之和。</p><h1 id="3-pytorch中xnormp2dim1keepdimtrue的理解"><a class="anchor" href="#3-pytorch中xnormp2dim1keepdimtrue的理解">#</a> 3. pytorch 中 x.norm (p=2,dim=1,keepdim=True) 的理解</h1><h2 id="31-方法介绍"><a class="anchor" href="#31-方法介绍">#</a> 3.1 方法介绍</h2><ul><li><p><strong>代码</strong>：x.norm (p=2,dim=1,keepdim=True)</p></li><li><p><strong>功能</strong>：求指定维度上的范数</p></li><li><p>函数原型：返回输入张量给定维 dim 上每行的 p 范数</p></li><li><p>torch.norm(input, p, dim, out=None,keepdim=False) → Tensor</p></li><li><p>注：范数求法：对 N 个数据求 p 范数（上面已介绍）<br><img data-src="/../images/4b5a18323825497d97d9c345eb56d15d.png" alt="图 11"></p></li></ul><h2 id="32-函数参数"><a class="anchor" href="#32-函数参数">#</a> 3.2 函数参数</h2><ul><li>input (Tensor) – 输入张量</li><li>p (float) – 范数计算中的幂指数值</li><li>dim (int) – 缩减的维度，dim=0 是对 0 维度上的一个向量求范数，返回结果数量等于其列的个数，也就是说有多少个 0 维度的向量，将得到多少个范数。dim=1 同理。</li><li>out (Tensor, optional) – 结果张量</li><li>keepdim（bool）– 保持输出的维度 。当 keepdim=False 时，输出比输入少一个维度（就是指定的 dim 求范数的维度）。而 keepdim=True 时，输出与输入维度相同，仅仅是输出在求范数的维度上元素个数变为 1。这也是为什么有时我们把参数中的 dim 称为缩减的维度，因为 norm 运算之后，此维度或者消失或者元素个数变为 1。</li></ul><h2 id="33-实例演示"><a class="anchor" href="#33-实例演示">#</a> 3.3 实例演示</h2><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> torch</pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>data <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span></pre></td></tr><tr><td data-num="2"></td><td><pre>            <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">3.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="3"></td><td><pre>            <span class="token punctuation">[</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">8.</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="4"></td><td><pre>            <span class="token punctuation">[</span> <span class="token number">3.</span><span class="token punctuation">,</span> <span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">9.</span><span class="token punctuation">,</span> <span class="token number">12.</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="5"></td><td><pre>        <span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><h3 id="331-dim参数"><a class="anchor" href="#331-dim参数">#</a> 3.3.1 dim 参数</h3><p>分别对其行和列分别求 2 范数:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># 按行</span></pre></td></tr><tr><td data-num="2"></td><td><pre>torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>data<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">5.4772</span><span class="token punctuation">]</span>,</pre></td></tr><tr><td data-num="2"></td><td><pre>        <span class="token punctuation">[</span><span class="token number">10.9545</span><span class="token punctuation">]</span>,</pre></td></tr><tr><td data-num="3"></td><td><pre>        <span class="token punctuation">[</span><span class="token number">16.4317</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># 按列</span></pre></td></tr><tr><td data-num="2"></td><td><pre>torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>data<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">3.7417</span>,  <span class="token number">7.4833</span>, <span class="token number">11.2250</span>, <span class="token number">14.9666</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><h3 id="332-keepdim参数"><a class="anchor" href="#332-keepdim参数">#</a> 3.3.2 keepdim 参数</h3><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>data<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">5.4772</span>, <span class="token number">10.9545</span>, <span class="token number">16.4317</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>可以看到输出少了一维，其实就是 dim=1（求范数）那一维 (列) 少了，因为从 4 列变成 1 列，就是 3 行中求每一行的 2 范数，就剩 1 列了，不保持这一维不会对数据产生影响。或者也可以这么理解，就是数据每个数据有没有用 [] 扩起来。</p><p><strong>不写 keepdim，则默认不保留 dim 的那个维度</strong></p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>data<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">5.4772</span>, <span class="token number">10.9545</span>, <span class="token number">16.4317</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p><strong>不写 dim，则计算 Tensor 中所有元素的 2 范数</strong></p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>data<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>tensor<span class="token punctuation">(</span><span class="token number">20.4939</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p><img data-src="/../images/c2c20fd7f24b43649a278ff8b5268af8.png" alt="图 12"><br>与上面一样的效果，<strong>默认</strong>是 L2 范数</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>data<span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>tensor<span class="token punctuation">(</span><span class="token number">20.4939</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>首先，它对张量 y 每个元素进行平方，然后对它们求和，最后取平方根。 这些操作计算就是所谓的 L2 或欧几里德范数。</p><p>参考：<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2E0OTM4MjM4ODIvYXJ0aWNsZS9kZXRhaWxzLzgwNTY5ODg4P29wc19yZXF1ZXN0X21pc2M9JmFtcDtyZXF1ZXN0X2lkPSZhbXA7Yml6X2lkPTEwMiZhbXA7dXRtX3Rlcm09eC5ub3JtJTI4JTI5JmFtcDt1dG1fbWVkaXVtPWRpc3RyaWJ1dGUucGNfc2VhcmNoX3Jlc3VsdC5ub25lLXRhc2stYmxvZy0yfmFsbH5zb2JhaWR1d2VifmRlZmF1bHQtMi04MDU2OTg4OC5maXJzdF9yYW5rX3YyX3BjX3JhbmtfdjI5JmFtcDtzcG09MTAxOC4yMjI2LjMwMDEuNDE4Nw==">Link</span> <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTIwL2FydGljbGUvZGV0YWlscy83ODIyODgzOT9vcHNfcmVxdWVzdF9taXNjPSZhbXA7cmVxdWVzdF9pZD0mYW1wO2Jpel9pZD0xMDImYW1wO3V0bV90ZXJtPXgubm9ybSUyOCUyOSZhbXA7dXRtX21lZGl1bT1kaXN0cmlidXRlLnBjX3NlYXJjaF9yZXN1bHQubm9uZS10YXNrLWJsb2ctMn5hbGx+c29iYWlkdXdlYn5kZWZhdWx0LTUtNzgyMjg4MzkuZmlyc3RfcmFua192Ml9wY19yYW5rX3YyOSZhbXA7c3BtPTEwMTguMjIyNi4zMDAxLjQxODc=">Link</span> <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM5MjUzNzgvYXJ0aWNsZS9kZXRhaWxzLzEwMzM2MzQ3MT9vcHNfcmVxdWVzdF9taXNjPSUyNTdCJTI1MjJyZXF1ZXN0JTI1NUZpZCUyNTIyJTI1M0ElMjUyMjE2Mzc2NTk2MTQxNjc4MDI3NDE0MjExOCUyNTIyJTI1MkMlMjUyMnNjbSUyNTIyJTI1M0ElMjUyMjIwMTQwNzEzLjEzMDEwMjMzNC5wYyUyNTVGYWxsLiUyNTIyJTI1N0QmYW1wO3JlcXVlc3RfaWQ9MTYzNzY1OTYxNDE2NzgwMjc0MTQyMTE4JmFtcDtiaXpfaWQ9MCZhbXA7dXRtX21lZGl1bT1kaXN0cmlidXRlLnBjX3NlYXJjaF9yZXN1bHQubm9uZS10YXNrLWJsb2ctMn5hbGx+Zmlyc3RfcmFua19lY3BtX3YxfnJhbmtfdjMxX2VjcG0tMi0xMDMzNjM0NzEuZmlyc3RfcmFua192Ml9wY19yYW5rX3YyOSZhbXA7dXRtX3Rlcm09cHl0b3JjaCUyMHgubm9ybSUyOCUyOSZhbXA7c3BtPTEwMTguMjIyNi4zMDAxLjQxODc=">Link</span> <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2puYmZrbmFzZjExMy9hcnRpY2xlL2RldGFpbHMvMTEwMTQxNTM3P29wc19yZXF1ZXN0X21pc2M9JTI1N0IlMjUyMnJlcXVlc3QlMjU1RmlkJTI1MjIlMjUzQSUyNTIyMTYzNzY1OTYxNDE2NzgwMjc0MTQyMTE4JTI1MjIlMjUyQyUyNTIyc2NtJTI1MjIlMjUzQSUyNTIyMjAxNDA3MTMuMTMwMTAyMzM0LnBjJTI1NUZhbGwuJTI1MjIlMjU3RCZhbXA7cmVxdWVzdF9pZD0xNjM3NjU5NjE0MTY3ODAyNzQxNDIxMTgmYW1wO2Jpel9pZD0wJmFtcDt1dG1fbWVkaXVtPWRpc3RyaWJ1dGUucGNfc2VhcmNoX3Jlc3VsdC5ub25lLXRhc2stYmxvZy0yfmFsbH5maXJzdF9yYW5rX2VjcG1fdjF+cmFua192MzFfZWNwbS0xLTExMDE0MTUzNy5maXJzdF9yYW5rX3YyX3BjX3JhbmtfdjI5JmFtcDt1dG1fdGVybT1weXRvcmNoJTIweC5ub3JtJTI4JTI5JmFtcDtzcG09MTAxOC4yMjI2LjMwMDEuNDE4Nw==">Link</span></p><p><mark><strong>加油！</strong></mark></p><p><mark><strong>感谢！</strong></mark></p><p><mark><strong>努力！</strong></mark></p></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2022-08-30 08:36:50" itemprop="dateModified" datetime="2022-08-30T08:36:50+08:00">2022-08-30</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="yuan 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="yuan 支付宝"><p>支付宝</p></div><div><img data-src="/images/paypal.png" alt="yuan 贝宝"><p>贝宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>yuan <i class="ic i-at"><em>@</em></i>yuan</li><li class="link"><strong>本文链接：</strong> <a href="https://jyuanhust.github.io/2022/08/24/ai/pytorch/Pytorch%E7%9A%84data-norm%EF%BC%88%E5%87%A0%E7%A7%8D%E8%8C%83%E6%95%B0-norm-%E7%9A%84%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%EF%BC%89/" title="Pytorch的data.norm（几种范数(norm)的详细介绍）">https://jyuanhust.github.io/2022/08/24/ai/pytorch/Pytorch的data-norm（几种范数-norm-的详细介绍）/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2022/08/24/ai/pytorch/pytorch%E5%9B%BA%E5%AE%9A%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90-%E8%AE%AD%E7%BB%83%E7%A8%B3%E5%AE%9A%E5%A4%8D%E7%8E%B0/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;gitee.com&#x2F;zkz0&#x2F;image&#x2F;raw&#x2F;master&#x2F;img&#x2F;img(8).webp" title="pytorch固定随机种子-训练稳定复现"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> pytorch</span><h3>pytorch固定随机种子-训练稳定复现</h3></a></div><div class="item right"><a href="/2022/08/24/ai/pytorch/Pytorch%E9%80%9A%E8%BF%87requires-grad%E5%9B%BA%E5%AE%9A%E9%83%A8%E5%88%86%E5%8F%82%E6%95%B0%E8%BF%9B%E8%A1%8C%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;gitee.com&#x2F;zkz0&#x2F;image&#x2F;raw&#x2F;master&#x2F;img&#x2F;img(68).webp" title="Pytorch通过requires_grad固定部分参数进行网络训练"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> pytorch</span><h3>Pytorch通过requires_grad固定部分参数进行网络训练</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8C%83%E6%95%B0norm-%E5%87%A0%E7%A7%8D%E8%8C%83%E6%95%B0%E7%9A%84%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8Ddatanorm%E4%BD%BF%E7%94%A8"><span class="toc-number">1.</span> <span class="toc-text">范数（norm） 几种范数的简单介绍 &amp; data.norm（）使用</span></a></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E8%8C%83%E6%95%B0norm%E7%9A%84%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D"><span class="toc-number"></span> <span class="toc-text">1. 范数 (norm) 的简单介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#11-l-p%E8%8C%83%E6%95%B0"><span class="toc-number"></span> <span class="toc-text">1.1 L-P 范数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-l0%E8%8C%83%E6%95%B0"><span class="toc-number"></span> <span class="toc-text">1.2 L0 范数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-l1%E8%8C%83%E6%95%B0"><span class="toc-number"></span> <span class="toc-text">1.3 L1 范数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-l2%E8%8C%83%E6%95%B0"><span class="toc-number"></span> <span class="toc-text">1.4 L2 范数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-%E8%8C%83%E6%95%B0"><span class="toc-number"></span> <span class="toc-text">1.5 ∞- 范数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E7%9F%A9%E9%98%B5%E8%8C%83%E6%95%B0"><span class="toc-number"></span> <span class="toc-text">2. 矩阵范数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#21-1-%E8%8C%83%E6%95%B0"><span class="toc-number"></span> <span class="toc-text">2.1 1 - 范数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-2-%E8%8C%83%E6%95%B0"><span class="toc-number"></span> <span class="toc-text">2.2 2 - 范数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#23-%E8%8C%83%E6%95%B0"><span class="toc-number"></span> <span class="toc-text">2.3 ∞- 范数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-f-%E8%8C%83%E6%95%B0"><span class="toc-number"></span> <span class="toc-text">2.4 F - 范数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-%E6%A0%B8%E8%8C%83%E6%95%B0"><span class="toc-number"></span> <span class="toc-text">2.6 核范数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-pytorch%E4%B8%ADxnormp2dim1keepdimtrue%E7%9A%84%E7%90%86%E8%A7%A3"><span class="toc-number"></span> <span class="toc-text">3. pytorch 中 x.norm (p&#x3D;2,dim&#x3D;1,keepdim&#x3D;True) 的理解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#31-%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D"><span class="toc-number"></span> <span class="toc-text">3.1 方法介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-%E5%87%BD%E6%95%B0%E5%8F%82%E6%95%B0"><span class="toc-number"></span> <span class="toc-text">3.2 函数参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#33-%E5%AE%9E%E4%BE%8B%E6%BC%94%E7%A4%BA"><span class="toc-number"></span> <span class="toc-text">3.3 实例演示</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#331-dim%E5%8F%82%E6%95%B0"><span class="toc-number">1.</span> <span class="toc-text">3.3.1 dim 参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#332-keepdim%E5%8F%82%E6%95%B0"><span class="toc-number">2.</span> <span class="toc-text">3.3.2 keepdim 参数</span></a></li></ol></li></ol></li></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/2022/07/22/ai/pytorch/pytorch%E5%85%A5%E9%97%A8/" rel="bookmark" title="pytorch入门">pytorch入门</a></li><li><a href="/2022/07/25/ai/pytorch/argmax-torch/" rel="bookmark" title="argmax-torch">argmax-torch</a></li><li><a href="/2022/07/25/ai/pytorch/%E4%BD%BF%E7%94%A8Pytorch%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/" rel="bookmark" title="使用Pytorch实现手写数字识别">使用Pytorch实现手写数字识别</a></li><li><a href="/2022/08/24/ai/pytorch/Pytorch%E4%B8%ADtransforms-RandomResizedCrop-%E7%AD%89%E5%9B%BE%E5%83%8F%E6%93%8D%E4%BD%9C/" rel="bookmark" title="Pytorch中transforms.RandomResizedCrop()等图像操作">Pytorch中transforms.RandomResizedCrop()等图像操作</a></li><li><a href="/2022/08/24/ai/pytorch/pytorch%E4%B8%AD%E7%9A%84transpose%E6%96%B9%E6%B3%95%EF%BC%88%E5%87%BD%E6%95%B0%EF%BC%89/" rel="bookmark" title="pytorch中的transpose方法（函数）">pytorch中的transpose方法（函数）</a></li><li><a href="/2022/08/24/ai/pytorch/PyTorch%E5%85%B3%E4%BA%8E%E4%BB%A5%E4%B8%8B%E6%96%B9%E6%B3%95%E4%BD%BF%E7%94%A8%EF%BC%9Adetach-cpu-numpy-%E4%BB%A5%E5%8F%8Aitem/" rel="bookmark" title="PyTorch关于以下方法使用：detach()_cpu()_numpy()_以及item()">PyTorch关于以下方法使用：detach()_cpu()_numpy()_以及item()</a></li><li><a href="/2022/08/24/ai/pytorch/Pytorch%E5%87%BD%E6%95%B0expand-%E8%AF%A6%E8%A7%A3/" rel="bookmark" title="Pytorch函数expand()详解">Pytorch函数expand()详解</a></li><li><a href="/2022/08/24/ai/pytorch/pytorch%E5%9B%BA%E5%AE%9A%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90-%E8%AE%AD%E7%BB%83%E7%A8%B3%E5%AE%9A%E5%A4%8D%E7%8E%B0/" rel="bookmark" title="pytorch固定随机种子-训练稳定复现">pytorch固定随机种子-训练稳定复现</a></li><li class="active"><a href="/2022/08/24/ai/pytorch/Pytorch%E7%9A%84data-norm%EF%BC%88%E5%87%A0%E7%A7%8D%E8%8C%83%E6%95%B0-norm-%E7%9A%84%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%EF%BC%89/" rel="bookmark" title="Pytorch的data.norm（几种范数(norm)的详细介绍）">Pytorch的data.norm（几种范数(norm)的详细介绍）</a></li><li><a href="/2022/08/24/ai/pytorch/Pytorch%E9%80%9A%E8%BF%87requires-grad%E5%9B%BA%E5%AE%9A%E9%83%A8%E5%88%86%E5%8F%82%E6%95%B0%E8%BF%9B%E8%A1%8C%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83/" rel="bookmark" title="Pytorch通过requires_grad固定部分参数进行网络训练">Pytorch通过requires_grad固定部分参数进行网络训练</a></li><li><a href="/2022/08/24/ai/pytorch/torch-matmul-%E7%94%A8%E6%B3%95%E4%BB%8B%E7%BB%8D/" rel="bookmark" title="torch.matmul()用法介绍">torch.matmul()用法介绍</a></li><li><a href="/2022/08/24/ai/pytorch/torchvision-datasets-ImageFolder/" rel="bookmark" title="torchvision.datasets.ImageFolder">torchvision.datasets.ImageFolder</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="yuan" data-src="/images/avatar.jpg"><p class="name" itemprop="name">yuan</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">429</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">72</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">61</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item email" data-url="bWFpbHRvOjIwODM2MzU1MjVAcXEuY29t" title="mailto:2083635525@qq.com"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li><li class="item"><a href="/links/" rel="section"><i class="ic i-magic"></i>链接</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/2022/08/24/ai/pytorch/pytorch%E5%9B%BA%E5%AE%9A%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90-%E8%AE%AD%E7%BB%83%E7%A8%B3%E5%AE%9A%E5%A4%8D%E7%8E%B0/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2022/08/24/ai/pytorch/Pytorch%E9%80%9A%E8%BF%87requires-grad%E5%9B%BA%E5%AE%9A%E9%83%A8%E5%88%86%E5%8F%82%E6%95%B0%E8%BF%9B%E8%A1%8C%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 computer-science">computer-science</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/algorithm/" title="分类于 algorithm">algorithm</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/algorithm/leetCode/" title="分类于 leetCode">leetCode</a></div><span><a href="/2022/12/26/computer-science/algorithm/leetCode/7-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" title="7-深入浅出动态规划">7-深入浅出动态规划</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/frontend/" title="分类于 前端">前端</a></div><span><a href="/2022/07/25/frontend/javascript/JavaScript/" title="JavaScript">JavaScript</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/backend/" title="分类于 后端">后端</a> <i class="ic i-angle-right"></i> <a href="/categories/backend/django/" title="分类于 django">django</a></div><span><a href="/2022/08/25/backend/django/%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0-django/" title="文件上传-django">文件上传-django</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2022/08/24/language/python/python%E4%BD%BF%E7%94%A8shutil-copyfile-%E5%A4%8D%E5%88%B6%E6%96%87%E4%BB%B6/" title="python使用shutil_copyfile_复制文件">python使用shutil_copyfile_复制文件</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-attention-mechanisms/" title="分类于 chapter_attention-mechanisms">chapter_attention-mechanisms</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_attention-mechanisms/attention-scoring-functions/" title="attention-scoring-functions">attention-scoring-functions</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/frontend/" title="分类于 前端">前端</a></div><span><a href="/2022/08/09/frontend/base/TypeScript/" title="TypeScript">TypeScript</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/tools/" title="分类于 tools">tools</a> <i class="ic i-angle-right"></i> <a href="/categories/tools/%E7%88%AC%E8%99%AB/" title="分类于 爬虫">爬虫</a></div><span><a href="/2022/09/09/tools/%E7%88%AC%E8%99%AB/selenium/" title="selenium">selenium</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2023/06/24/language/C++/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/" title="未命名">未命名</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-recurrent-neural-networks/" title="分类于 chapter_recurrent-neural-networks">chapter_recurrent-neural-networks</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_recurrent-neural-networks/bptt/" title="bptt">bptt</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-computational-performance/" title="分类于 chapter_computational-performance">chapter_computational-performance</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_computational-performance/multiple-gpus/" title="multiple-gpus">multiple-gpus</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">yuan @ Mi Manchi</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">2.9m 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">44:38</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2022/08/24/ai/pytorch/Pytorch的data-norm（几种范数-norm-的详细介绍）/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(t){return t.includes("#")},function(t){return new RegExp(LOCAL.path+"$").test(t)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html>