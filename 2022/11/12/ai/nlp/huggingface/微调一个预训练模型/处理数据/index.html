<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="yuan" href="https://huang-junyuan.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="yuan" href="https://huang-junyuan.github.io/atom.xml"><link rel="alternate" type="application/json" title="yuan" href="https://huang-junyuan.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="自然语言处理"><link rel="canonical" href="https://huang-junyuan.github.io/2022/11/12/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE/"><title>微调一个预训练模型-处理数据 - 微调一个预训练模型 - huggingface - nlp - ai | Mi Manchi = yuan = Whatever is worth doing at all is worth doing well</title><meta name="generator" content="Hexo 6.2.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">微调一个预训练模型-处理数据</h1><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>11k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>10 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Mi Manchi</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giclfw2t96j20zk0m8x6p.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giciszlczyj20zk0m816d.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1gipexe4oykj20zk0m87ji.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giclffsa1cj20zk0m811l.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giclfb3vzhj20zk0m8wny.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giciub8ja1j20zk0m81ky.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/" itemprop="item" rel="index" title="分类于 ai"><span itemprop="name">ai</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/nlp/" itemprop="item" rel="index" title="分类于 nlp"><span itemprop="name">nlp</span></a><meta itemprop="position" content="2"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/nlp/huggingface/" itemprop="item" rel="index" title="分类于 huggingface"><span itemprop="name">huggingface</span></a><meta itemprop="position" content="3"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" itemprop="item" rel="index" title="分类于 微调一个预训练模型"><span itemprop="name">微调一个预训练模型</span></a><meta itemprop="position" content="4"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://huang-junyuan.github.io/2022/11/12/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="yuan"><meta itemprop="description" content="Whatever is worth doing at all is worth doing well, "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="yuan"></span><div class="body md" itemprop="articleBody"><h1 id="处理数据"><a class="anchor" href="#处理数据">#</a> 处理数据</h1><p>下面是我们用模型中心的数据在 PyTorch 上训练句子分类器的一个例子：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> torch</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AdamW<span class="token punctuation">,</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForSequenceClassification</pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token comment"># Same as before</span></pre></td></tr><tr><td data-num="5"></td><td><pre>checkpoint <span class="token operator">=</span> <span class="token string">"bert-base-uncased"</span></pre></td></tr><tr><td data-num="6"></td><td><pre>tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>sequences <span class="token operator">=</span> <span class="token punctuation">[</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    <span class="token string">"I've been waiting for a HuggingFace course my whole life."</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    <span class="token string">"This course is amazing!"</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="12"></td><td><pre>batch <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>sequences<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="13"></td><td><pre></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token comment"># This is new</span></pre></td></tr><tr><td data-num="15"></td><td><pre>batch<span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="16"></td><td><pre></pre></td></tr><tr><td data-num="17"></td><td><pre>optimizer <span class="token operator">=</span> AdamW<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="18"></td><td><pre>loss <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span><span class="token punctuation">.</span>loss</pre></td></tr><tr><td data-num="19"></td><td><pre>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="20"></td><td><pre>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="21"></td><td><pre></pre></td></tr><tr><td data-num="22"></td><td><pre>``</pre></td></tr><tr><td data-num="23"></td><td><pre></pre></td></tr><tr><td data-num="24"></td><td><pre>当然，仅仅用两句话训练模型不会产生很好的效果。为了获得更好的结果，您需要准备一个更大的数据集。</pre></td></tr><tr><td data-num="25"></td><td><pre></pre></td></tr><tr><td data-num="26"></td><td><pre>在本节中，我们将使用MRPC（微软研究释义语料库）数据集作为示例，该数据集由威廉·多兰和克里斯·布罗克特在这篇文章发布。该数据集由<span class="token number">5801</span>对句子组成，每个句子对带有一个标签，指示它们是否为同义（即，如果两个句子的意思相同）。我们在本章中选择了它，因为它是一个小数据集，所以很容易对它进行训练。</pre></td></tr><tr><td data-num="27"></td><td><pre></pre></td></tr><tr><td data-num="28"></td><td><pre></pre></td></tr><tr><td data-num="29"></td><td><pre></pre></td></tr><tr><td data-num="30"></td><td><pre><span class="token comment">## 从模型中心（Hub）加载数据集</span></pre></td></tr><tr><td data-num="31"></td><td><pre></pre></td></tr><tr><td data-num="32"></td><td><pre>模型中心（hub）不只是包含模型；它也有许多不同语言的多个数据集。点击数据集的链接即可进行浏览。我们建议您在阅读本节后阅读一下加载和处理新的数据集这篇文章，这会让您对huggingface的darasets更加清晰。但现在，让我们使用MRPC数据集中的<span class="token punctuation">[</span>GLUE 基准测试数据集<span class="token punctuation">]</span><span class="token punctuation">(</span>https<span class="token punctuation">:</span><span class="token operator">//</span>gluebenchmark<span class="token punctuation">.</span>com<span class="token operator">/</span><span class="token punctuation">)</span>，它是构成MRPC数据集的<span class="token number">10</span>个数据集之一，这是一个学术基准，用于衡量机器学习模型在<span class="token number">10</span>个不同文本分类任务中的性能。</pre></td></tr><tr><td data-num="33"></td><td><pre></pre></td></tr><tr><td data-num="34"></td><td><pre></pre></td></tr><tr><td data-num="35"></td><td><pre>Datasets库提供了一个非常便捷的命令，可以在模型中心（hub）上下载和缓存数据集。我们可以通过以下的代码下载MRPC数据集</pre></td></tr><tr><td data-num="36"></td><td><pre></pre></td></tr><tr><td data-num="37"></td><td><pre>```python</pre></td></tr><tr><td data-num="38"></td><td><pre></pre></td></tr><tr><td data-num="39"></td><td><pre><span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset</pre></td></tr><tr><td data-num="40"></td><td><pre></pre></td></tr><tr><td data-num="41"></td><td><pre>raw_datasets <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"glue"</span><span class="token punctuation">,</span> <span class="token string">"mrpc"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="42"></td><td><pre>raw_datasets</pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>DatasetDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    train<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="3"></td><td><pre>        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'sentence1'</span><span class="token punctuation">,</span> <span class="token string">'sentence2'</span><span class="token punctuation">,</span> <span class="token string">'label'</span><span class="token punctuation">,</span> <span class="token string">'idx'</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="4"></td><td><pre>        num_rows<span class="token punctuation">:</span> <span class="token number">3668</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    validation<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="7"></td><td><pre>        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'sentence1'</span><span class="token punctuation">,</span> <span class="token string">'sentence2'</span><span class="token punctuation">,</span> <span class="token string">'label'</span><span class="token punctuation">,</span> <span class="token string">'idx'</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="8"></td><td><pre>        num_rows<span class="token punctuation">:</span> <span class="token number">408</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    test<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="11"></td><td><pre>        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'sentence1'</span><span class="token punctuation">,</span> <span class="token string">'sentence2'</span><span class="token punctuation">,</span> <span class="token string">'label'</span><span class="token punctuation">,</span> <span class="token string">'idx'</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="12"></td><td><pre>        num_rows<span class="token punctuation">:</span> <span class="token number">1725</span></pre></td></tr><tr><td data-num="13"></td><td><pre>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>正如你所看到的，我们获得了一个 DatasetDict 对象，其中包含训练集、验证集和测试集。每一个集合都包含几个列 (sentence1, sentence2, label, and idx) 以及一个代表行数的变量，即每个集合中的行的个数（因此，训练集中有 3668 对句子，验证集中有 408 对，测试集中有 1725 对）。</p><p>默认情况下，此命令在下载数据集并缓存到～/.cache/huggingface/dataset. 回想一下第 2 章，您可以通过设置 HF_HOME 环境变量来自定义缓存的文件夹。</p><p>我们可以访问我们数据集中的每一个 raw_train_dataset 对象，如使用字典：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>raw_train_dataset <span class="token operator">=</span> raw_datasets<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="2"></td><td><pre>raw_train_dataset<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">&#123;</span><span class="token string">'idx'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="2"></td><td><pre> <span class="token string">'label'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="3"></td><td><pre> <span class="token string">'sentence1'</span><span class="token punctuation">:</span> <span class="token string">'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .'</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="4"></td><td><pre> <span class="token string">'sentence2'</span><span class="token punctuation">:</span> <span class="token string">'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'</span><span class="token punctuation">&#125;</span></pre></td></tr></table></figure><p>我们可以看到标签已经是整数了，所以我们不需要对标签做任何预处理。要知道哪个数字对应于哪个标签，我们可以查看 raw_train_dataset 的 features. 这将告诉我们每列的类型：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>raw_train_dataset<span class="token punctuation">.</span>features</pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">&#123;</span><span class="token string">'sentence1'</span><span class="token punctuation">:</span> Value<span class="token punctuation">(</span>dtype<span class="token operator">=</span><span class="token string">'string'</span><span class="token punctuation">,</span> <span class="token builtin">id</span><span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="2"></td><td><pre> <span class="token string">'sentence2'</span><span class="token punctuation">:</span> Value<span class="token punctuation">(</span>dtype<span class="token operator">=</span><span class="token string">'string'</span><span class="token punctuation">,</span> <span class="token builtin">id</span><span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="3"></td><td><pre> <span class="token string">'label'</span><span class="token punctuation">:</span> ClassLabel<span class="token punctuation">(</span>num_classes<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'not_equivalent'</span><span class="token punctuation">,</span> <span class="token string">'equivalent'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> names_file<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token builtin">id</span><span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="4"></td><td><pre> <span class="token string">'idx'</span><span class="token punctuation">:</span> Value<span class="token punctuation">(</span>dtype<span class="token operator">=</span><span class="token string">'int32'</span><span class="token punctuation">,</span> <span class="token builtin">id</span><span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></pre></td></tr></table></figure><p>在上面的例子之中，Label（标签） 是一种 ClassLabel（分类标签），使用整数建立起到类别标签的映射关系。0 对应于 not_equivalent，1 对应于 equivalent。</p><h2 id="预处理数据集"><a class="anchor" href="#预处理数据集">#</a> 预处理数据集</h2><p>为了预处理数据集，我们需要将文本转换为模型能够理解的数字。正如你在第二章上看到的那样</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre>checkpoint <span class="token operator">=</span> <span class="token string">"bert-base-uncased"</span></pre></td></tr><tr><td data-num="4"></td><td><pre>tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>tokenized_sentences_1 <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>raw_datasets<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"sentence1"</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>tokenized_sentences_2 <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>raw_datasets<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"sentence2"</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>然而，在两句话传递给模型，预测这两句话是否是同义之前。我们需要这两句话依次进行适当的预处理。幸运的是，标记器不仅仅可以输入单个句子还可以输入一组句子，并按照我们的 BERT 模型所期望的输入进行处理：</p><pre><code>inputs = tokenizer(&quot;This is the first sentence.&quot;, &quot;This is the second one.&quot;)
inputs
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">&#123;</span> </pre></td></tr><tr><td data-num="2"></td><td><pre>  <span class="token string">'input_ids'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">2003</span><span class="token punctuation">,</span> <span class="token number">1996</span><span class="token punctuation">,</span> <span class="token number">2034</span><span class="token punctuation">,</span> <span class="token number">6251</span><span class="token punctuation">,</span> <span class="token number">1012</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">,</span> <span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">2003</span><span class="token punctuation">,</span> <span class="token number">1996</span><span class="token punctuation">,</span> <span class="token number">2117</span><span class="token punctuation">,</span> <span class="token number">2028</span><span class="token punctuation">,</span> <span class="token number">1012</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="3"></td><td><pre>  <span class="token string">'token_type_ids'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="4"></td><td><pre>  <span class="token string">'attention_mask'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token punctuation">&#125;</span></pre></td></tr></table></figure><p>我们在第二章 讨论了输入词 id (input_ids) 和 注意力遮罩 (attention_mask) ，但我们在那个时候没有讨论类型标记 ID (token_type_ids)。在这个例子中，类型标记 ID (token_type_ids) 的作用就是告诉模型输入的哪一部分是第一句，哪一部分是第二句。</p><p>如果我们将 input_ids 中的 id 转换回文字:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>tokenizer<span class="token punctuation">.</span>convert_ids_to_tokens<span class="token punctuation">(</span>inputs<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">[</span><span class="token string">'[CLS]'</span><span class="token punctuation">,</span> <span class="token string">'this'</span><span class="token punctuation">,</span> <span class="token string">'is'</span><span class="token punctuation">,</span> <span class="token string">'the'</span><span class="token punctuation">,</span> <span class="token string">'first'</span><span class="token punctuation">,</span> <span class="token string">'sentence'</span><span class="token punctuation">,</span> <span class="token string">'.'</span><span class="token punctuation">,</span> <span class="token string">'[SEP]'</span><span class="token punctuation">,</span> <span class="token string">'this'</span><span class="token punctuation">,</span> <span class="token string">'is'</span><span class="token punctuation">,</span> <span class="token string">'the'</span><span class="token punctuation">,</span> <span class="token string">'second'</span><span class="token punctuation">,</span> <span class="token string">'one'</span><span class="token punctuation">,</span> <span class="token string">'.'</span><span class="token punctuation">,</span> <span class="token string">'[SEP]'</span><span class="token punctuation">]</span></pre></td></tr></table></figure><p>所以我们看到模型需要输入的形式是 <code>[CLS] sentence1 [SEP] sentence2 [SEP]</code> 。因此，当有两句话的时候。类型标记 <code>ID(token_type_ids)</code> 的值是：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">[</span><span class="token string">'[CLS]'</span><span class="token punctuation">,</span> <span class="token string">'this'</span><span class="token punctuation">,</span> <span class="token string">'is'</span><span class="token punctuation">,</span> <span class="token string">'the'</span><span class="token punctuation">,</span> <span class="token string">'first'</span><span class="token punctuation">,</span> <span class="token string">'sentence'</span><span class="token punctuation">,</span> <span class="token string">'.'</span><span class="token punctuation">,</span> <span class="token string">'[SEP]'</span><span class="token punctuation">,</span> <span class="token string">'this'</span><span class="token punctuation">,</span> <span class="token string">'is'</span><span class="token punctuation">,</span> <span class="token string">'the'</span><span class="token punctuation">,</span> <span class="token string">'second'</span><span class="token punctuation">,</span> <span class="token string">'one'</span><span class="token punctuation">,</span> <span class="token string">'.'</span><span class="token punctuation">,</span> <span class="token string">'[SEP]'</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token punctuation">[</span>      <span class="token number">0</span><span class="token punctuation">,</span>      <span class="token number">0</span><span class="token punctuation">,</span>    <span class="token number">0</span><span class="token punctuation">,</span>     <span class="token number">0</span><span class="token punctuation">,</span>       <span class="token number">0</span><span class="token punctuation">,</span>          <span class="token number">0</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">,</span>       <span class="token number">0</span><span class="token punctuation">,</span>      <span class="token number">1</span><span class="token punctuation">,</span>    <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>        <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>   <span class="token number">1</span><span class="token punctuation">,</span>       <span class="token number">1</span><span class="token punctuation">]</span></pre></td></tr></table></figure><p>如您所见，输入中 <code>[CLS] sentence1 [SEP]</code> 它们的类型标记 ID 均为 0，而其他部分，对应于 <code>sentence2 [SEP]</code> ，所有的类型标记 ID 均为 1.</p><p>请注意，如果选择其他的 checkpoint，则不一定具有类型标记 ID (token_type_ids)（例如，如果使用 DistilBERT 模型，就不会返回它们）。只有当它在预训练期间使用过这一层，模型在构建时依赖它们，才会返回它们。</p><p>用类型标记 ID 对 BERT 进行预训练，并且使用第一章的遮罩语言模型，还有一个额外的应用类型，叫做下一句预测。这项任务的目标是建立成对句子之间关系的模型。</p><p>在下一个句子预测任务中，会给模型输入成对的句子（带有随机遮罩的标记），并被要求预测第二个句子是否紧跟第一个句子。为了提高模型的泛化能力，数据集中一半的两个句子在原始文档中挨在一起，另一半的两个句子来自两个不同的文档。</p><p>一般来说，你不需要担心是否有类型标记 ID (token_type_ids)。在您的输入中：只要您对标记器和模型使用相同的检查点，一切都会很好，因为标记器知道向其模型提供什么。</p><h3 id="一份完整的代码-mrpc"><a class="anchor" href="#一份完整的代码-mrpc">#</a> 一份完整的代码 MRPC</h3><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForSequenceClassification</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">from</span> transformers <span class="token keyword">import</span> Trainer<span class="token punctuation">,</span> TrainingArguments</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_metric</pre></td></tr><tr><td data-num="4"></td><td><pre></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> DataCollatorWithPadding</pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token keyword">import</span> datasets</pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre>checkpoint <span class="token operator">=</span> <span class="token string">'bert-base-cased'</span></pre></td></tr><tr><td data-num="10"></td><td><pre>tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="11"></td><td><pre>raw_datasets <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_dataset<span class="token punctuation">(</span><span class="token string">'glue'</span><span class="token punctuation">,</span> <span class="token string">'mrpc'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre></pre></td></tr><tr><td data-num="13"></td><td><pre><span class="token keyword">def</span> <span class="token function">tokenize_function</span><span class="token punctuation">(</span>sample<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="14"></td><td><pre>    <span class="token keyword">return</span> tokenizer<span class="token punctuation">(</span>sample<span class="token punctuation">[</span><span class="token string">'sentence1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sample<span class="token punctuation">[</span><span class="token string">'sentence2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="15"></td><td><pre>tokenized_datasets <span class="token operator">=</span> raw_datasets<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tokenize_function<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="16"></td><td><pre></pre></td></tr><tr><td data-num="17"></td><td><pre>data_collator <span class="token operator">=</span> DataCollatorWithPadding<span class="token punctuation">(</span>tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="18"></td><td><pre></pre></td></tr><tr><td data-num="19"></td><td><pre><span class="token keyword">def</span> <span class="token function">compute_metrics</span><span class="token punctuation">(</span>eval_preds<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="20"></td><td><pre>    metric <span class="token operator">=</span> load_metric<span class="token punctuation">(</span><span class="token string">"glue"</span><span class="token punctuation">,</span> <span class="token string">"mrpc"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="21"></td><td><pre>    logits<span class="token punctuation">,</span> labels <span class="token operator">=</span> eval_preds<span class="token punctuation">.</span>predictions<span class="token punctuation">,</span> eval_preds<span class="token punctuation">.</span>label_ids</pre></td></tr><tr><td data-num="22"></td><td><pre>    <span class="token comment"># 上一行可以直接简写成：</span></pre></td></tr><tr><td data-num="23"></td><td><pre>    <span class="token comment"># logits, labels = eval_preds  因为它相当于一个 tuple</span></pre></td></tr><tr><td data-num="24"></td><td><pre>    predictions <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="25"></td><td><pre>    <span class="token keyword">return</span> metric<span class="token punctuation">.</span>compute<span class="token punctuation">(</span>predictions<span class="token operator">=</span>predictions<span class="token punctuation">,</span> references<span class="token operator">=</span>labels<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="26"></td><td><pre></pre></td></tr><tr><td data-num="27"></td><td><pre>training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>output_dir<span class="token operator">=</span><span class="token string">'test_trainer'</span><span class="token punctuation">,</span> evaluation_strategy<span class="token operator">=</span><span class="token string">'epoch'</span><span class="token punctuation">)</span>  <span class="token comment"># 指定输出文件夹，没有会自动创建</span></pre></td></tr><tr><td data-num="28"></td><td><pre></pre></td></tr><tr><td data-num="29"></td><td><pre>model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># new model</span></pre></td></tr><tr><td data-num="30"></td><td><pre></pre></td></tr><tr><td data-num="31"></td><td><pre>trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span></pre></td></tr><tr><td data-num="32"></td><td><pre>    model<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="33"></td><td><pre>    training_args<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="34"></td><td><pre>    train_dataset<span class="token operator">=</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="35"></td><td><pre>    eval_dataset<span class="token operator">=</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">"validation"</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="36"></td><td><pre>    data_collator<span class="token operator">=</span>data_collator<span class="token punctuation">,</span>  <span class="token comment"># 在定义了 tokenizer 之后，其实这里的 data_collator 就不用再写了，会自动根据 tokenizer 创建</span></pre></td></tr><tr><td data-num="37"></td><td><pre>    tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="38"></td><td><pre>    compute_metrics<span class="token operator">=</span>compute_metrics</pre></td></tr><tr><td data-num="39"></td><td><pre><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="40"></td><td><pre></pre></td></tr><tr><td data-num="41"></td><td><pre>trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>针对上面的问题，输入进入 model 的应该是什么呢？</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>model<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>tokenizer<span class="token punctuation">(</span>raw_datasets<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'sentence1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> raw_datasets<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'sentence2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>上面这种写法中，是没有传入 attention_masked 的，对比下面两个样本测试</p><p>之所以取两条数据，是因为这样得出来的就是二维的，如果只是取出一条的话，还得自己加一个维度。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>model<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>tokenizer<span class="token punctuation">(</span>raw_datasets<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'sentence1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> raw_datasets<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'sentence2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>结果为</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>SequenceClassifierOutput<span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> logits<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.5339</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.2458</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="2"></td><td><pre>        <span class="token punctuation">[</span> <span class="token number">0.5840</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.2698</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>AddmmBackward0<span class="token operator">></span><span class="token punctuation">)</span><span class="token punctuation">,</span> hidden_states<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> attentions<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>这两条数据为</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>raw_datasets<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token punctuation">&#123;</span><span class="token string">'sentence1'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .'</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="4"></td><td><pre>  <span class="token string">"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion ."</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="5"></td><td><pre> <span class="token string">'sentence2'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="6"></td><td><pre>  <span class="token string">"Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 ."</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="7"></td><td><pre> <span class="token string">'label'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="8"></td><td><pre> <span class="token string">'idx'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span></pre></td></tr></table></figure><p>在 model 中传入 attention_mask 时</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>temp <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>raw_datasets<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'sentence1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> raw_datasets<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'sentence2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>temp<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])</span></pre></td></tr><tr><td data-num="4"></td><td><pre></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>temp<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>temp<span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>temp<span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># &lt;class 'list'></span></pre></td></tr><tr><td data-num="10"></td><td><pre></pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>temp<span class="token punctuation">[</span><span class="token string">'token_type_ids'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># &lt;class 'list'></span></pre></td></tr><tr><td data-num="12"></td><td><pre></pre></td></tr><tr><td data-num="13"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>temp<span class="token punctuation">[</span><span class="token string">'attention_mask'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># &lt;class 'list'></span></pre></td></tr><tr><td data-num="14"></td><td><pre></pre></td></tr><tr><td data-num="15"></td><td><pre><span class="token comment"># 转为 tensor  &lt;class 'torch.Tensor'></span></pre></td></tr><tr><td data-num="16"></td><td><pre></pre></td></tr><tr><td data-num="17"></td><td><pre>temp<span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>temp<span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="18"></td><td><pre></pre></td></tr><tr><td data-num="19"></td><td><pre>temp<span class="token punctuation">[</span><span class="token string">'token_type_ids'</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>temp<span class="token punctuation">[</span><span class="token string">'token_type_ids'</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="20"></td><td><pre></pre></td></tr><tr><td data-num="21"></td><td><pre>temp<span class="token punctuation">[</span><span class="token string">'attention_mask'</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>temp<span class="token punctuation">[</span><span class="token string">'attention_mask'</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="22"></td><td><pre></pre></td></tr><tr><td data-num="23"></td><td><pre>model<span class="token punctuation">(</span><span class="token operator">**</span>temp<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="24"></td><td><pre></pre></td></tr><tr><td data-num="25"></td><td><pre>SequenceClassifierOutput<span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> logits<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.1645</span><span class="token punctuation">,</span> <span class="token number">0.6985</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="26"></td><td><pre>        <span class="token punctuation">[</span><span class="token number">0.1670</span><span class="token punctuation">,</span> <span class="token number">0.6946</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>AddmmBackward0<span class="token operator">></span><span class="token punctuation">)</span><span class="token punctuation">,</span> hidden_states<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> attentions<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>如果是在有 GPU 的环境下进行训练，则 model 会被转移到 GPU 上面，则要将样本数据也要转移到 GPU 上面去</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre>temp<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token triple-quoted-string string">'''</span></pre></td></tr><tr><td data-num="6"></td><td><pre>也可用下面的来代替上面的 temp.to(device)</pre></td></tr><tr><td data-num="7"></td><td><pre>temp['input_ids'] = temp['input_ids'].to(device)</pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre>temp['attention_mask'] = temp['attention_mask'].to(device)</pre></td></tr><tr><td data-num="10"></td><td><pre></pre></td></tr><tr><td data-num="11"></td><td><pre>temp['token_type_ids'] = temp['token_type_ids'].to(device)</pre></td></tr><tr><td data-num="12"></td><td><pre></pre></td></tr><tr><td data-num="13"></td><td><pre>'''</pre></td></tr><tr><td data-num="14"></td><td><pre></pre></td></tr><tr><td data-num="15"></td><td><pre></pre></td></tr><tr><td data-num="16"></td><td><pre>model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="17"></td><td><pre></pre></td></tr><tr><td data-num="18"></td><td><pre>model<span class="token punctuation">(</span><span class="token operator">**</span>temp<span class="token punctuation">)</span></pre></td></tr></table></figure><h3 id="如何写-compute_metric-的代码"><a class="anchor" href="#如何写-compute_metric-的代码">#</a> 如何写 compute_metric 的代码</h3><p>如果是</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>predictions <span class="token operator">=</span> trainer<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">'validation'</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>predictions<span class="token punctuation">.</span>predictions<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment"># logits</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token comment"># array([[-2.7887206,  3.1986978],</span></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token comment">#       [ 2.5258656, -1.832253 ], ...], dtype=float32)</span></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>predictions<span class="token punctuation">.</span>label_ids<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># array([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, ...], dtype=int64)</span></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>predictions<span class="token punctuation">.</span>metrics<span class="token punctuation">)</span></pre></td></tr></table></figure><p>第一行改成 <code>predictions = trainer.predict(tokenized_datasets['validation'][0:3])</code> ，就会出现报错：原因是 <code>tokenized_datasets['validation'][0:3]</code> 的类型是 <code>&lt;class 'dict'&gt;</code> ，而 <code>tokenized_datasets['validation']</code> 的类型是 <code>&lt;class 'datasets.arrow_dataset.Dataset'&gt;</code></p><p>更改代码为如下就可以了：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> datasets <span class="token keyword">import</span> Dataset</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre>x <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">[</span><span class="token string">'validation'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="4"></td><td><pre></pre></td></tr><tr><td data-num="5"></td><td><pre>y <span class="token operator">=</span> Dataset<span class="token punctuation">.</span>from_dict<span class="token punctuation">(</span>x<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre>predictions <span class="token operator">=</span> trainer<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>y<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre></pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>predictions<span class="token punctuation">.</span>predictions<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment"># logits</span></pre></td></tr><tr><td data-num="12"></td><td><pre><span class="token comment"># array([[-2.7887206,  3.1986978],</span></pre></td></tr><tr><td data-num="13"></td><td><pre><span class="token comment">#       [ 2.5258656, -1.832253 ], ...], dtype=float32)</span></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>predictions<span class="token punctuation">.</span>label_ids<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># array([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, ...], dtype=int64)</span></pre></td></tr><tr><td data-num="15"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>predictions<span class="token punctuation">.</span>metrics<span class="token punctuation">)</span></pre></td></tr></table></figure><p>值得注意的是，上面得到的结果 <code>predictions.predictions</code> 和 <code>model(**temp)</code> 得到的结果是相同的。</p><h3 id="下面探索label"><a class="anchor" href="#下面探索label">#</a> 下面探索 label</h3><p>将数据集中的 label 列变成 predict 中的 label_ids，可能数据类型不同，但是数据是相同的</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> datasets <span class="token keyword">import</span> Dataset</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre>x <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">[</span><span class="token string">'validation'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="4"></td><td><pre></pre></td></tr><tr><td data-num="5"></td><td><pre>y <span class="token operator">=</span> Dataset<span class="token punctuation">.</span>from_dict<span class="token punctuation">(</span>x<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre>predictions <span class="token operator">=</span> trainer<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>y<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre></pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>predictions<span class="token punctuation">.</span>predictions<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment"># logits</span></pre></td></tr><tr><td data-num="12"></td><td><pre><span class="token comment"># array([[-2.7887206,  3.1986978],</span></pre></td></tr><tr><td data-num="13"></td><td><pre><span class="token comment">#       [ 2.5258656, -1.832253 ], ...], dtype=float32)</span></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>predictions<span class="token punctuation">.</span>label_ids<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># array([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, ...], dtype=int64)</span></pre></td></tr><tr><td data-num="15"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>predictions<span class="token punctuation">.</span>metrics<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="16"></td><td><pre></pre></td></tr><tr><td data-num="17"></td><td><pre><span class="token comment"># 输出 label</span></pre></td></tr><tr><td data-num="18"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>predictions<span class="token punctuation">.</span>label_ids<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="19"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token string">'label'</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><h3 id="总结"><a class="anchor" href="#总结">#</a> 总结</h3><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> datasets <span class="token keyword">import</span> Dataset</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre>x <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">[</span><span class="token string">'validation'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="4"></td><td><pre></pre></td></tr><tr><td data-num="5"></td><td><pre>y <span class="token operator">=</span> Dataset<span class="token punctuation">.</span>from_dict<span class="token punctuation">(</span>x<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre>predictions <span class="token operator">=</span> trainer<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>y<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre></pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>predictions<span class="token punctuation">.</span>predictions<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment"># logits</span></pre></td></tr><tr><td data-num="12"></td><td><pre><span class="token comment"># array([[-2.7887206,  3.1986978],</span></pre></td></tr><tr><td data-num="13"></td><td><pre><span class="token comment">#       [ 2.5258656, -1.832253 ], ...], dtype=float32)</span></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>predictions<span class="token punctuation">.</span>label_ids<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># array([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, ...], dtype=int64)</span></pre></td></tr><tr><td data-num="15"></td><td><pre></pre></td></tr><tr><td data-num="16"></td><td><pre><span class="token comment"># 这里就计算出来指标了，可以直接查看</span></pre></td></tr><tr><td data-num="17"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>predictions<span class="token punctuation">.</span>metrics<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="18"></td><td><pre></pre></td></tr><tr><td data-num="19"></td><td><pre>metric <span class="token operator">=</span> load_metric<span class="token punctuation">(</span><span class="token string">"glue"</span><span class="token punctuation">,</span> <span class="token string">"mrpc"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="20"></td><td><pre></pre></td></tr><tr><td data-num="21"></td><td><pre>metric<span class="token punctuation">.</span>compute<span class="token punctuation">(</span>predictions<span class="token operator">=</span>np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>predictions<span class="token punctuation">.</span>predictions<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> references<span class="token operator">=</span>predictions<span class="token punctuation">.</span>label_ids<span class="token punctuation">)</span></pre></td></tr></table></figure><p>如果是对 temp 的话</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>res <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>temp<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre>metric <span class="token operator">=</span> load_metric<span class="token punctuation">(</span><span class="token string">"glue"</span><span class="token punctuation">,</span> <span class="token string">"mrpc"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre></pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token comment"># 要将 res.logits 转移到 cpu 上，</span></pre></td></tr><tr><td data-num="7"></td><td><pre>metric<span class="token punctuation">.</span>compute<span class="token punctuation">(</span>predictions<span class="token operator">=</span>np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>res<span class="token punctuation">.</span>logits<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> references<span class="token operator">=</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">'validation'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'label'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>detach () 函数的用法：<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hvZG9ycy9hcnRpY2xlL2RldGFpbHMvMTE5MjQ4ODM4">https://blog.csdn.net/Hodors/article/details/119248838</span></p><p>现在的问题是，在换 head 之后，模型的输出结果和 labels 指定的形式不一定是一样的，这会对 compute_metric 造成影响，解决方案是：</p><p>在训练模型前（此时 head 中的权重是随机初始化的），先</p><h3 id="cola"><a class="anchor" href="#cola">#</a> cola</h3><p>在上面的代码中，使用的都是 <code>AutoModelFor...</code> ，但是查看官网发现，是存在着 <code>BertFor...</code> 这样的类存在的，目前尚不清楚两者的区别。</p><p>对于 <code>AutoModelForSequenceClassification</code> ，模型的输出并不是非 0 即 1，对于输入的每个句子都会得到一个浮点数的结果，如果目标是二分类的话，最后使用 <code>argmax(..., axis=-1)</code> ，得到两个句子中的 0 或 1。</p><p>如果目标是类似 stsb 的 1-5 呢？该怎么处理输出的结果呢？</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForSequenceClassification</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">from</span> transformers <span class="token keyword">import</span> Trainer<span class="token punctuation">,</span> TrainingArguments</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_metric</pre></td></tr><tr><td data-num="4"></td><td><pre></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> DataCollatorWithPadding</pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token keyword">import</span> datasets</pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre>checkpoint <span class="token operator">=</span> <span class="token string">'bert-base-cased'</span></pre></td></tr><tr><td data-num="10"></td><td><pre>tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="11"></td><td><pre></pre></td></tr><tr><td data-num="12"></td><td><pre>raw_datasets <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_dataset<span class="token punctuation">(</span><span class="token string">'glue'</span><span class="token punctuation">,</span> <span class="token string">'cola'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="13"></td><td><pre></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token comment"># 查看数据集的基本信息</span></pre></td></tr><tr><td data-num="15"></td><td><pre>raw_datasets<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>features</pre></td></tr><tr><td data-num="16"></td><td><pre></pre></td></tr><tr><td data-num="17"></td><td><pre><span class="token keyword">def</span> <span class="token function">tokenize_function</span><span class="token punctuation">(</span>sample<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="18"></td><td><pre>    <span class="token keyword">return</span> tokenizer<span class="token punctuation">(</span>sample<span class="token punctuation">[</span><span class="token string">'sentence'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="19"></td><td><pre>tokenized_datasets <span class="token operator">=</span> raw_datasets<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tokenize_function<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="20"></td><td><pre></pre></td></tr><tr><td data-num="21"></td><td><pre>model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># new model</span></pre></td></tr><tr><td data-num="22"></td><td><pre></pre></td></tr><tr><td data-num="23"></td><td><pre><span class="token keyword">def</span> <span class="token function">compute_metrics</span><span class="token punctuation">(</span>eval_preds<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="24"></td><td><pre>    metric <span class="token operator">=</span> load_metric<span class="token punctuation">(</span><span class="token string">"glue"</span><span class="token punctuation">,</span> <span class="token string">"cola"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="25"></td><td><pre>    logits<span class="token punctuation">,</span> labels <span class="token operator">=</span> eval_preds<span class="token punctuation">.</span>predictions<span class="token punctuation">,</span> eval_preds<span class="token punctuation">.</span>label_ids</pre></td></tr><tr><td data-num="26"></td><td><pre>    <span class="token comment"># 上一行可以直接简写成：</span></pre></td></tr><tr><td data-num="27"></td><td><pre>    <span class="token comment"># logits, labels = eval_preds  因为它相当于一个 tuple</span></pre></td></tr><tr><td data-num="28"></td><td><pre>    predictions <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="29"></td><td><pre>    <span class="token keyword">return</span> metric<span class="token punctuation">.</span>compute<span class="token punctuation">(</span>predictions<span class="token operator">=</span>predictions<span class="token punctuation">,</span> references<span class="token operator">=</span>labels<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="30"></td><td><pre></pre></td></tr><tr><td data-num="31"></td><td><pre>training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>output_dir<span class="token operator">=</span><span class="token string">'test_trainer'</span><span class="token punctuation">,</span> evaluation_strategy<span class="token operator">=</span><span class="token string">'epoch'</span><span class="token punctuation">)</span>  <span class="token comment"># 指定输出文件夹，没有会自动创建</span></pre></td></tr><tr><td data-num="32"></td><td><pre></pre></td></tr><tr><td data-num="33"></td><td><pre>trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span></pre></td></tr><tr><td data-num="34"></td><td><pre>    model<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="35"></td><td><pre>    training_args<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="36"></td><td><pre>    train_dataset<span class="token operator">=</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="37"></td><td><pre>    eval_dataset<span class="token operator">=</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">"validation"</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="38"></td><td><pre>    <span class="token comment"># data_collator=data_collator,  # 在定义了 tokenizer 之后，其实这里的 data_collator 就不用再写了，会自动根据 tokenizer 创建</span></pre></td></tr><tr><td data-num="39"></td><td><pre>    tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="40"></td><td><pre>    compute_metrics<span class="token operator">=</span>compute_metrics</pre></td></tr><tr><td data-num="41"></td><td><pre><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="42"></td><td><pre></pre></td></tr><tr><td data-num="43"></td><td><pre>trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></table></figure><h1 id="补充"><a class="anchor" href="#补充">#</a> 补充</h1><h2 id="collate"><a class="anchor" href="#collate">#</a> collate</h2><div class="tags"><a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" rel="tag"><i class="ic i-tag"></i> 自然语言处理</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2022-11-12 18:43:48" itemprop="dateModified" datetime="2022-11-12T18:43:48+08:00">2022-11-12</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="yuan 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="yuan 支付宝"><p>支付宝</p></div><div><img data-src="/images/paypal.png" alt="yuan 贝宝"><p>贝宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>yuan <i class="ic i-at"><em>@</em></i>yuan</li><li class="link"><strong>本文链接：</strong> <a href="https://huang-junyuan.github.io/2022/11/12/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE/" title="微调一个预训练模型-处理数据">https://huang-junyuan.github.io/2022/11/12/ai/nlp/huggingface/微调一个预训练模型/处理数据/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/BPE/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1gipetlbztpj20zk0m84qp.jpg" title="Byte-Pair Encoding tokenization"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> Tokenizer库</span><h3>Byte-Pair Encoding tokenization</h3></a></div><div class="item right"><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E9%97%AE%E7%AD%94/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1giph47e9vtj20zk0m8x6l.jpg" title="问答 question answer"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> 主要nlp任务</span><h3>问答 question answer</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE"><span class="toc-number">1.</span> <span class="toc-text">处理数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.1.</span> <span class="toc-text">预处理数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%BB%BD%E5%AE%8C%E6%95%B4%E7%9A%84%E4%BB%A3%E7%A0%81-mrpc"><span class="toc-number">1.1.1.</span> <span class="toc-text">一份完整的代码 MRPC</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%86%99-compute_metric-%E7%9A%84%E4%BB%A3%E7%A0%81"><span class="toc-number">1.1.2.</span> <span class="toc-text">如何写 compute_metric 的代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E9%9D%A2%E6%8E%A2%E7%B4%A2label"><span class="toc-number">1.1.3.</span> <span class="toc-text">下面探索 label</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.1.4.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cola"><span class="toc-number">1.1.5.</span> <span class="toc-text">cola</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%A1%A5%E5%85%85"><span class="toc-number">2.</span> <span class="toc-text">补充</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#collate"><span class="toc-number">2.1.</span> <span class="toc-text">collate</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li class="active"><a href="/2022/11/12/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE/" rel="bookmark" title="微调一个预训练模型-处理数据">微调一个预训练模型-处理数据</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="yuan" data-src="/images/avatar.jpg"><p class="name" itemprop="name">yuan</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">187</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">40</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">37</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item email" data-url="bWFpbHRvOjIwODM2MzU1MjVAcXEuY29t" title="mailto:2083635525@qq.com"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/BPE/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E9%97%AE%E7%AD%94/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/" title="分类于 计算机科学">计算机科学</a></div><span><a href="/2022/08/24/computer-science/algorithm/base/%E7%AE%97%E6%B3%951/" title="算法1">算法1</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/" title="分类于 数据库">数据库</a></div><span><a href="/2022/07/26/database/Python%E6%93%8D%E4%BD%9CMongoDB/" title="Python操作MongoDB">Python操作MongoDB</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/backend/" title="分类于 后端">后端</a> <i class="ic i-angle-right"></i> <a href="/categories/backend/django/" title="分类于 django">django</a></div><span><a href="/2022/09/13/backend/django/%E5%9C%A8app%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%8Btests%E6%A8%A1%E5%9D%97%E4%B8%AD%E6%B5%8B%E8%AF%95/" title="在app文件夹下tests模块中测试">在app文件夹下tests模块中测试</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/" title="分类于 nlp">nlp</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/base/" title="分类于 base">base</a></div><span><a href="/2022/08/24/ai/nlp/base/PyTorch-%E4%B8%AD-LSTM-%E7%9A%84-output%E3%80%81h-n-%E5%92%8C-c-n-%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/" title="PyTorch_中_LSTM_的_output、h_n_和_c_n_之间的关系">PyTorch_中_LSTM_的_output、h_n_和_c_n_之间的关系</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/" title="分类于 nlp">nlp</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/huggingface/" title="分类于 huggingface">huggingface</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" title="分类于 微调一个预训练模型">微调一个预训练模型</a></div><span><a href="/2022/11/12/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE/" title="微调一个预训练模型-处理数据">微调一个预训练模型-处理数据</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/frontend/" title="分类于 前端">前端</a> <i class="ic i-angle-right"></i> <a href="/categories/frontend/vue/" title="分类于 vue">vue</a></div><span><a href="/2022/09/11/frontend/vue/Less/" title="Less">Less</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/" title="分类于 nlp">nlp</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/base/" title="分类于 base">base</a></div><span><a href="/2022/08/24/ai/nlp/base/pytorch%E4%B8%ADLSTM%E7%9A%84output%E5%92%8Chidden%E5%85%B3%E7%B3%BB/" title="pytorch中LSTM的output和hidden关系">pytorch中LSTM的output和hidden关系</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/" title="分类于 nlp">nlp</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/base/" title="分类于 base">base</a></div><span><a href="/2022/09/22/ai/nlp/base/Subword%E7%AE%97%E6%B3%95/" title="Subword算法">Subword算法</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/" title="分类于 nlp">nlp</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/huggingface/" title="分类于 huggingface">huggingface</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81nlp%E4%BB%BB%E5%8A%A1/" title="分类于 主要nlp任务">主要nlp任务</a></div><span><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E9%97%AE%E7%AD%94/" title="问答 question answer">问答 question answer</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2022/10/17/computer-science/base/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C/%E7%AC%94%E8%AE%B0/" title="未命名">未命名</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2022</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">yuan @ Mi Manchi</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">1.1m 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">16:30</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2022/11/12/ai/nlp/huggingface/微调一个预训练模型/处理数据/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html>