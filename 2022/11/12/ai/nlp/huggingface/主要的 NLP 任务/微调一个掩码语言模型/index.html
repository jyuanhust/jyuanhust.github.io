<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="yuan" href="https://jyuanhust.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="yuan" href="https://jyuanhust.github.io/atom.xml"><link rel="alternate" type="application/json" title="yuan" href="https://jyuanhust.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="自然语言处理"><link rel="canonical" href="https://jyuanhust.github.io/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E6%8E%A9%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"><title>微调一个掩码语言模型 - 主要nlp任务 - huggingface - nlp - ai | Mi Manchi = yuan = Whatever is worth doing at all is worth doing well</title><meta name="generator" content="Hexo 6.2.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">微调一个掩码语言模型</h1><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>18k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>16 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Mi Manchi</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(18).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(19).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(24).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(8).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(69).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(79).webp"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/" itemprop="item" rel="index" title="分类于 ai"><span itemprop="name">ai</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/nlp/" itemprop="item" rel="index" title="分类于 nlp"><span itemprop="name">nlp</span></a><meta itemprop="position" content="2"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/nlp/huggingface/" itemprop="item" rel="index" title="分类于 huggingface"><span itemprop="name">huggingface</span></a><meta itemprop="position" content="3"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81nlp%E4%BB%BB%E5%8A%A1/" itemprop="item" rel="index" title="分类于 主要nlp任务"><span itemprop="name">主要nlp任务</span></a><meta itemprop="position" content="4"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://jyuanhust.github.io/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E6%8E%A9%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="yuan"><meta itemprop="description" content="Whatever is worth doing at all is worth doing well, "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="yuan"></span><div class="body md" itemprop="articleBody"><h1 id="微调掩码语言模型"><a class="anchor" href="#微调掩码语言模型">#</a> 微调掩码语言模型</h1><p>对于许多涉及 Transformer 模型的 NLP 程序，你可以简单地从 Hugging Face Hub 中获取一个预训练的模型，然后直接在你的数据上对其进行微调，以完成手头的任务。只要用于预训练的语料库与用于微调的语料库没有太大区别，迁移学习通常会产生很好的结果。</p><p>但是，在某些情况下，你需要<strong>先微调数据上的语言模型，然后再训练特定于任务的 head</strong>。例如，如果您的数据集包含法律合同或科学文章，像 BERT 这样的普通 Transformer 模型通常会将您语料库中的特定领域词视为稀有标记，结果性能可能不尽如人意。通过在域内数据上微调语言模型，你可以提高许多下游任务的性能，这意味着您通常只需执行一次此步骤！</p><p>这种在域内数据上微调预训练语言模型的过程通常称为 <strong>领域适应</strong>。 它于 2018 年由 ULMFiT 推广，这是使迁移学习真正适用于 NLP 的首批神经架构之一 (基于 LSTM)。 下图显示了使用 ULMFiT 进行域自适应的示例；在本节中，我们将做类似的事情，但使用的是 Transformer 而不是 LSTM!</p><p><img data-src="/./images/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E6%8E%A9%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/1666263675192.png" alt="1666263675192"></p><h2 id="选择用于掩码语言建模的预训练模型"><a class="anchor" href="#选择用于掩码语言建模的预训练模型">#</a> 选择用于掩码语言建模的预训练模型</h2><p>使用名为 <span class="exturl" data-url="aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9kaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZA==">DistilBERT</span> 的模型。 可以更快地训练，而下游性能几乎没有损失。这个模型使用一种称为<span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvS25vd2xlZGdlX2Rpc3RpbGxhdGlvbg==">知识蒸馏</span>的特殊技术进行训练，其中使用像 BERT 这样的大型 “教师模型” 来指导参数少得多的 “学生模型” 的训练。在本节中对知识蒸馏细节的解释会使我们离题太远，但如果你有兴趣，可以阅读所有相关内容 Natural Language Processing with Transformers (俗称 Transformers 教科书)。</p><p><span class="exturl" data-url="aHR0cHM6Ly9sZWFybmluZy5vcmVpbGx5LmNvbS9saWJyYXJ5L3ZpZXcvbmF0dXJhbC1sYW5ndWFnZS1wcm9jZXNzaW5nLzk3ODEwOTgxMDMyMzEvY2gwNS5odG1s">https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/ch05.html</span></p><p>让我们继续，使用 AutoModelForMaskedLM 类下载 DistilBERT:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForMaskedLM</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre>model_checkpoint <span class="token operator">=</span> <span class="token string">"distilbert-base-uncased"</span></pre></td></tr><tr><td data-num="4"></td><td><pre>model <span class="token operator">=</span> AutoModelForMaskedLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_checkpoint<span class="token punctuation">)</span></pre></td></tr></table></figure><p>问题：上面的 AutoModelForMaskedLM 是只能用这个么？记得还有个 AutoModel</p><p>这里面应该是头，即特定于任务的区别？</p><p>可以通过调用 num_parameters () 方法查看模型有多少参数:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>distilbert_num_parameters <span class="token operator">=</span> model<span class="token punctuation">.</span>num_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">1_000_000</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"'>>> DistilBERT number of parameters: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">round</span><span class="token punctuation">(</span>distilbert_num_parameters<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">M'"</span></span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"'>>> BERT number of parameters: 110M'"</span></span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token string">'>>> DistilBERT number of parameters: 67M'</span></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token string">'>>> BERT number of parameters: 110M'</span></pre></td></tr></table></figure><p>DistilBERT 大约有 6700 万个参数，大约比 BERT 基本模型小两倍，这大致意味着训练的速度提高了两倍 — 非常棒！现在让我们看看这个模型预测什么样的标记最有可能完成一小部分文本:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>text <span class="token operator">=</span> <span class="token string">"This is a great [MASK]."</span></pre></td></tr></table></figure><p>作为人类，我们可以想象 [MASK] 标记有很多可能性，例如 “day”、 “ride” 或者 “painting”。对于预训练模型，预测取决于模型所训练的语料库，因为它会学习获取数据中存在的统计模式。与 BERT 一样，DistilBERT 在 English Wikipedia 和 BookCorpus 数据集上进行预训练，所以我们期望对 [MASK] 的预测能够反映这些领域。为了预测掩码，我们需要 DistilBERT 的标记器来生成模型的输入，所以让我们也从 Hub 下载它:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre>tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_checkpoint<span class="token punctuation">)</span></pre></td></tr></table></figure><p>使用标记器和模型，我们现在可以将我们的文本示例传递给模型，提取 logits, 并打印出前 5 个候选:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> torch</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre>inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>text<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre></pre></td></tr><tr><td data-num="5"></td><td><pre>token_logits <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span><span class="token punctuation">.</span>logits</pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token comment"># Find the location of [MASK] and extract its logits</span></pre></td></tr><tr><td data-num="7"></td><td><pre>mask_token_index <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>inputs<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">==</span> tokenizer<span class="token punctuation">.</span>mask_token_id<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="8"></td><td><pre>mask_token_logits <span class="token operator">=</span> token_logits<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> mask_token_index<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token comment"># Pick the [MASK] candidates with the highest logits</span></pre></td></tr><tr><td data-num="10"></td><td><pre>top_5_tokens <span class="token operator">=</span> torch<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>mask_token_logits<span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>indices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="11"></td><td><pre></pre></td></tr><tr><td data-num="12"></td><td><pre><span class="token keyword">for</span> token <span class="token keyword">in</span> top_5_tokens<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="13"></td><td><pre>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"'>>> </span><span class="token interpolation"><span class="token punctuation">&#123;</span>text<span class="token punctuation">.</span>replace<span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>mask_token<span class="token punctuation">,</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">[</span>token<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">'"</span></span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>对上面的代码输出分析如下</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>inputs</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token punctuation">&#123;</span><span class="token string">'input_ids'</span><span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">2003</span><span class="token punctuation">,</span> <span class="token number">1037</span><span class="token punctuation">,</span> <span class="token number">2307</span><span class="token punctuation">,</span>  <span class="token number">103</span><span class="token punctuation">,</span> <span class="token number">1012</span><span class="token punctuation">,</span>  <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'attention_mask'</span><span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="4"></td><td><pre></pre></td></tr><tr><td data-num="5"></td><td><pre>tokenizer<span class="token punctuation">.</span>convert_ids_to_tokens<span class="token punctuation">(</span>inputs<span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token punctuation">[</span><span class="token string">'[CLS]'</span><span class="token punctuation">,</span> <span class="token string">'this'</span><span class="token punctuation">,</span> <span class="token string">'is'</span><span class="token punctuation">,</span> <span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token string">'great'</span><span class="token punctuation">,</span> <span class="token string">'[MASK]'</span><span class="token punctuation">,</span> <span class="token string">'.'</span><span class="token punctuation">,</span> <span class="token string">'[SEP]'</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre>model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre></pre></td></tr><tr><td data-num="11"></td><td><pre>MaskedLMOutput<span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> logits<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token operator">-</span><span class="token number">5.5882</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">5.5868</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">5.5958</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">4.9448</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">4.8174</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">2.9905</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="12"></td><td><pre>         <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">11.9031</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">11.8872</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">12.0623</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">10.9570</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">10.6464</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">8.6324</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="13"></td><td><pre>         <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">11.9604</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">12.1520</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">12.1279</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">10.0218</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">8.6074</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">8.0971</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="14"></td><td><pre>         <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="15"></td><td><pre>         <span class="token punctuation">[</span> <span class="token operator">-</span><span class="token number">4.8228</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">4.6268</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">5.1041</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">4.2771</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">5.0184</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">3.9428</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="16"></td><td><pre>         <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">11.2945</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">11.2388</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">11.3857</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">9.2063</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">9.3411</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">6.1505</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="17"></td><td><pre>         <span class="token punctuation">[</span> <span class="token operator">-</span><span class="token number">9.5213</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">9.4632</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">9.5022</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">8.6561</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">8.4908</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">4.6903</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="18"></td><td><pre>       grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>ViewBackward0<span class="token operator">></span><span class="token punctuation">)</span><span class="token punctuation">,</span> hidden_states<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> attentions<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="19"></td><td><pre></pre></td></tr><tr><td data-num="20"></td><td><pre></pre></td></tr><tr><td data-num="21"></td><td><pre>model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'logits'</span><span class="token punctuation">]</span>和model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span><span class="token punctuation">.</span>logits相同</pre></td></tr><tr><td data-num="22"></td><td><pre></pre></td></tr><tr><td data-num="23"></td><td><pre></pre></td></tr><tr><td data-num="24"></td><td><pre>model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'logits'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape</pre></td></tr><tr><td data-num="25"></td><td><pre></pre></td></tr><tr><td data-num="26"></td><td><pre>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">30522</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="27"></td><td><pre></pre></td></tr><tr><td data-num="28"></td><td><pre></pre></td></tr><tr><td data-num="29"></td><td><pre>torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>inputs<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">==</span> tokenizer<span class="token punctuation">.</span>mask_token_id<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="30"></td><td><pre></pre></td></tr><tr><td data-num="31"></td><td><pre><span class="token punctuation">(</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 列出横纵坐标</span></pre></td></tr><tr><td data-num="32"></td><td><pre></pre></td></tr><tr><td data-num="33"></td><td><pre>mask_token_logits<span class="token punctuation">.</span>shape</pre></td></tr><tr><td data-num="34"></td><td><pre></pre></td></tr><tr><td data-num="35"></td><td><pre>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">30522</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="36"></td><td><pre></pre></td></tr><tr><td data-num="37"></td><td><pre>torch<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>mask_token_logits<span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="38"></td><td><pre></pre></td></tr><tr><td data-num="39"></td><td><pre>torch<span class="token punctuation">.</span>return_types<span class="token punctuation">.</span>topk<span class="token punctuation">(</span></pre></td></tr><tr><td data-num="40"></td><td><pre>values<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">7.0727</span><span class="token punctuation">,</span> <span class="token number">6.6514</span><span class="token punctuation">,</span> <span class="token number">6.6425</span><span class="token punctuation">,</span> <span class="token number">6.2530</span><span class="token punctuation">,</span> <span class="token number">5.8618</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>TopkBackward0<span class="token operator">></span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="41"></td><td><pre>indices<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3066</span><span class="token punctuation">,</span> <span class="token number">3112</span><span class="token punctuation">,</span> <span class="token number">6172</span><span class="token punctuation">,</span> <span class="token number">2801</span><span class="token punctuation">,</span> <span class="token number">8658</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="42"></td><td><pre></pre></td></tr><tr><td data-num="43"></td><td><pre><span class="token comment"># 上面应该是对 mask_token_logits 的 topk 进行了优化，所以返回值中有 indices，这表示词表中 token 对应的数字</span></pre></td></tr><tr><td data-num="44"></td><td><pre></pre></td></tr><tr><td data-num="45"></td><td><pre>mask_token_logits<span class="token punctuation">.</span>topk<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token string">'>>> This is a great deal.'</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token string">'>>> This is a great success.'</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token string">'>>> This is a great adventure.'</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token string">'>>> This is a great idea.'</span></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token string">'>>> This is a great feat.'</span></pre></td></tr></table></figure><p>我们可以从输出中看到模型的预测是指日常用语，鉴于英语维基百科的基础，这也许并不奇怪。让我们看看我们如何将这个领域改变为更小众的东西 — 高度两极分化的电影评论！</p><h2 id="数据集"><a class="anchor" href="#数据集">#</a> 数据集</h2><p>为了展示域适配，我们将使用著名的<span class="exturl" data-url="aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9pbWRi">大型电影评论数据集</span> (Large Movie Review Dataset) (或者简称为 IMDb), 这是一个电影评论语料库，通常用于对情感分析模型进行基准测试。通过在这个语料库上对 DistilBERT 进行微调，我们预计语言模型将根据维基百科的事实数据调整其词汇表，这些数据已经预先训练到电影评论中更主观的元素。我们可以使用 Datasets 中的 <code>load_dataset()</code> 函数从 Hugging Face 中获取数据:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre>imdb_dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"imdb"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>imdb_dataset</pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre>DatasetDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    train<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="8"></td><td><pre>        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">,</span> <span class="token string">'label'</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="9"></td><td><pre>        num_rows<span class="token punctuation">:</span> <span class="token number">25000</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="11"></td><td><pre>    test<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="12"></td><td><pre>        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">,</span> <span class="token string">'label'</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="13"></td><td><pre>        num_rows<span class="token punctuation">:</span> <span class="token number">25000</span></pre></td></tr><tr><td data-num="14"></td><td><pre>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="15"></td><td><pre>    unsupervised<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="16"></td><td><pre>        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">,</span> <span class="token string">'label'</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="17"></td><td><pre>        num_rows<span class="token punctuation">:</span> <span class="token number">50000</span></pre></td></tr><tr><td data-num="18"></td><td><pre>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="19"></td><td><pre><span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>我们可以看到 train 和 test 每个拆分包含 25,000 条评论，而有一个未标记的拆分称为 unsupervised 包含 50,000 条评论。让我们看一些示例，以了解我们正在处理的文本类型。正如我们在本课程的前几章中所做的那样，我们将链接 <code>Dataset.shuffle()</code> 和 <code>Dataset.select()</code> 函数创建随机样本:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>sample <span class="token operator">=</span> imdb_dataset<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>seed<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span><span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">for</span> row <span class="token keyword">in</span> sample<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"\n'>>> Review: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>row<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span></span><span class="token string">'"</span></span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"'>>> Label: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>row<span class="token punctuation">[</span><span class="token string">'label'</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span></span><span class="token string">'"</span></span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre></pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token string">'>>> Review: This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, "kodokoo" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\'t get me wrong; as clichéd and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.&lt;br />&lt;br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'</span></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token string">'>>> Label: 0'</span></pre></td></tr><tr><td data-num="10"></td><td><pre></pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token string">'>>> Review: Okay, the story makes no sense, the characters lack any dimensionally, the best dialogue is ad-libs about the low quality of movie, the cinematography is dismal, and only editing saves a bit of the muddle, but Sam" Peckinpah directed the film. Somehow, his direction is not enough. For those who appreciate Peckinpah and his great work, this movie is a disappointment. Even a great cast cannot redeem the time the viewer wastes with this minimal effort.&lt;br />&lt;br />The proper response to the movie is the contempt that the director San Peckinpah, James Caan, Robert Duvall, Burt Young, Bo Hopkins, Arthur Hill, and even Gig Young bring to their work. Watch the great Peckinpah films. Skip this mess.'</span></pre></td></tr><tr><td data-num="12"></td><td><pre><span class="token string">'>>> Label: 0'</span></pre></td></tr><tr><td data-num="13"></td><td><pre></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token string">'>>> Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, and have recently come to own a VHS version. &lt;br />&lt;br />My 4 and 6 year old children love this movie and have been asking again and again to watch it. &lt;br />&lt;br />I have enjoyed watching it again too. Though I have to admit it is not as good on a little TV.&lt;br />&lt;br />I do not have older children so I do not know what they would think of it. &lt;br />&lt;br />The songs are very cute. My daughter keeps singing them over and over.&lt;br />&lt;br />Hope this helps.'</span></pre></td></tr><tr><td data-num="15"></td><td><pre><span class="token string">'>>> Label: 1'</span></pre></td></tr></table></figure><p>虽然我们不需要语言建模的标签，但我们已经可以看到 0 表示负面评论，而 1 对应正面。</p><p>现在我们已经快速浏览了数据，让我们深入研究为掩码语言建模做准备。正如我们将看到的，与我们在第三章中看到的序列分类任务相比，还需要采取一些额外的步骤。让我们继续！</p><h2 id="预处理数据"><a class="anchor" href="#预处理数据">#</a> 预处理数据</h2><p><strong>对于自回归和掩码语言建模，一个常见的预处理步骤是连接所有示例，然后将整个语料库拆分为相同大小的块。</strong> 这与我们通常的方法完全不同，我们只是简单地标记单个示例。为什么要将所有内容连接在一起？原因是单个示例如果太长可能会被截断，这将导致丢失可能对语言建模任务有用的信息！</p><p>因此，我们将像往常一样首先标记我们的语料库，但是 没有 在我们的标记器中设置 <code>truncation=True</code> 选项。 我们还将获取可用的单词 ID ((如果我们使用快速标记器，它们是可用的，如 第六章中所述), 因为我们稍后将需要它们来进行全字屏蔽。我们将把它包装在一个简单的函数中，当我们在做的时候，我们将删除 text 和 label 列，因为我们不再需要它们:</p><p>全字屏蔽 是什么意思</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">def</span> <span class="token function">tokenize_function</span><span class="token punctuation">(</span>examples<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    result <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>examples<span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    <span class="token keyword">if</span> tokenizer<span class="token punctuation">.</span>is_fast<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="4"></td><td><pre>        result<span class="token punctuation">[</span><span class="token string">"word_ids"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span>result<span class="token punctuation">.</span>word_ids<span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>result<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    <span class="token keyword">return</span> result</pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre></pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token comment"># Use batched=True to activate fast multithreading!</span></pre></td></tr><tr><td data-num="9"></td><td><pre>tokenized_datasets <span class="token operator">=</span> imdb_dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    tokenize_function<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> remove_columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">,</span> <span class="token string">"label"</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre>tokenized_datasets</pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>DatasetDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    train<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="3"></td><td><pre>        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'attention_mask'</span><span class="token punctuation">,</span> <span class="token string">'input_ids'</span><span class="token punctuation">,</span> <span class="token string">'word_ids'</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="4"></td><td><pre>        num_rows<span class="token punctuation">:</span> <span class="token number">25000</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    test<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="7"></td><td><pre>        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'attention_mask'</span><span class="token punctuation">,</span> <span class="token string">'input_ids'</span><span class="token punctuation">,</span> <span class="token string">'word_ids'</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="8"></td><td><pre>        num_rows<span class="token punctuation">:</span> <span class="token number">25000</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    unsupervised<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="11"></td><td><pre>        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'attention_mask'</span><span class="token punctuation">,</span> <span class="token string">'input_ids'</span><span class="token punctuation">,</span> <span class="token string">'word_ids'</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="12"></td><td><pre>        num_rows<span class="token punctuation">:</span> <span class="token number">50000</span></pre></td></tr><tr><td data-num="13"></td><td><pre>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>上面的代码运行时出现警告：</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre>Token indices sequence length is longer than the specified maximum sequence length <span class="token keyword">for</span> this model <span class="token punctuation">(</span><span class="token number">720</span> <span class="token operator">></span> <span class="token number">512</span><span class="token punctuation">)</span>. Running this sequence through the model will result <span class="token keyword">in</span> indexing errors</pre></td></tr></table></figure><p>是指对句子分词后，句子的 token 太多的意思么？</p><p>那个 720 是怎么来的</p><p>由于 DistilBERT 是一个类似 BERT 的模型，我们可以看到编码文本由我们在其他章节中看到的 input_ids 和 attention_mask 组成，以及我们添加的 word_ids。</p><p>现在我们已经标记了我们的电影评论，下一步是将它们组合在一起并将结果分成块。 但是这些块应该有多大？这最终将取决于你可用的 GPU 内存量，但一个好的起点是查看模型的最大上下文大小是多少。这可以通过检查标记器的 model_max_length 属性来判断:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>tokenizer<span class="token punctuation">.</span>model_max_length</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token number">512</span></pre></td></tr></table></figure><p>该值来自于与检查点相关联的 tokenizer_config.json 文件；在这种情况下，我们可以看到上下文大小是 512 个标记，就像 BERT 一样。</p><p>因此，以便在像 Google Colab 那样的 GPU 上运行我们的实验，我们将选择可以放入内存的更小一些的东西:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>chunk_size <span class="token operator">=</span> <span class="token number">128</span></pre></td></tr></table></figure><p>请注意，在实际场景中使用较小的块大小可能是有害的，因此你应该使用与将应用模型的用例相对应的大小。</p><p>有趣的来了。为了展示串联是如何工作的，让我们从我们的标记化训练集中取一些评论并打印出每个评论的标记数量:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># Slicing produces a list of lists for each feature</span></pre></td></tr><tr><td data-num="2"></td><td><pre>tokenized_samples <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">for</span> idx<span class="token punctuation">,</span> sample <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>tokenized_samples<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"'>>> Review </span><span class="token interpolation"><span class="token punctuation">&#123;</span>idx<span class="token punctuation">&#125;</span></span><span class="token string"> length: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">len</span><span class="token punctuation">(</span>sample<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">'"</span></span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token string">'>>> Review 0 length: 200'</span></pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token string">'>>> Review 1 length: 559'</span></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token string">'>>> Review 2 length: 192'</span></pre></td></tr></table></figure><p>然后我们可以用一个简单的字典理解来连接所有例子，如下所示:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>concatenated_examples <span class="token operator">=</span> <span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    k<span class="token punctuation">:</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>tokenized_samples<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> tokenized_samples<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="4"></td><td><pre>total_length <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>concatenated_examples<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"'>>> Concatenated reviews length: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>total_length<span class="token punctuation">&#125;</span></span><span class="token string">'"</span></span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token string">'>>> Concatenated reviews length: 951'</span></pre></td></tr></table></figure><p>很棒，总长度检查出来了 — 现在，让我们将连接的评论拆分为大小为 block_size 的块。为此，我们迭代了 concatenated_examples 中的特征，并使用列表理解来创建每个特征的切片。结果是每个特征的块字典:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>chunks <span class="token operator">=</span> <span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    k<span class="token punctuation">:</span> <span class="token punctuation">[</span>t<span class="token punctuation">[</span>i <span class="token punctuation">:</span> i <span class="token operator">+</span> chunk_size<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> total_length<span class="token punctuation">,</span> chunk_size<span class="token punctuation">)</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    <span class="token keyword">for</span> k<span class="token punctuation">,</span> t <span class="token keyword">in</span> concatenated_examples<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token keyword">for</span> chunk <span class="token keyword">in</span> chunks<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"'>>> Chunk length: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">len</span><span class="token punctuation">(</span>chunk<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">'"</span></span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token string">'>>> Chunk length: 128'</span></pre></td></tr><tr><td data-num="10"></td><td><pre><span class="token string">'>>> Chunk length: 128'</span></pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token string">'>>> Chunk length: 128'</span></pre></td></tr><tr><td data-num="12"></td><td><pre><span class="token string">'>>> Chunk length: 128'</span></pre></td></tr><tr><td data-num="13"></td><td><pre><span class="token string">'>>> Chunk length: 128'</span></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token string">'>>> Chunk length: 128'</span></pre></td></tr><tr><td data-num="15"></td><td><pre><span class="token string">'>>> Chunk length: 128'</span></pre></td></tr><tr><td data-num="16"></td><td><pre><span class="token string">'>>> Chunk length: 55'</span></pre></td></tr></table></figure><p>正如你在这个例子中看到的，最后一个块通常会小于最大块大小。有两种主要的策略来处理这个问题:</p><ul><li>如果最后一个块小于 chunk_size, 请删除它。</li><li>填充最后一个块，直到其长度等于 chunk_size。</li></ul><p>我们将在这里采用第一种方法，因此让我们将上述所有逻辑包装在一个函数中，我们可以将其应用于我们的标记化数据集:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">def</span> <span class="token function">group_texts</span><span class="token punctuation">(</span>examples<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    <span class="token comment"># Concatenate all texts</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    concatenated_examples <span class="token operator">=</span> <span class="token punctuation">&#123;</span>k<span class="token punctuation">:</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>examples<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> examples<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    <span class="token comment"># Compute length of concatenated texts</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    total_length <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>concatenated_examples<span class="token punctuation">[</span><span class="token builtin">list</span><span class="token punctuation">(</span>examples<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    <span class="token comment"># We drop the last chunk if it's smaller than chunk_size</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    total_length <span class="token operator">=</span> <span class="token punctuation">(</span>total_length <span class="token operator">//</span> chunk_size<span class="token punctuation">)</span> <span class="token operator">*</span> chunk_size</pre></td></tr><tr><td data-num="8"></td><td><pre>    <span class="token comment"># Split by chunks of max_len</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    result <span class="token operator">=</span> <span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="10"></td><td><pre>        k<span class="token punctuation">:</span> <span class="token punctuation">[</span>t<span class="token punctuation">[</span>i <span class="token punctuation">:</span> i <span class="token operator">+</span> chunk_size<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> total_length<span class="token punctuation">,</span> chunk_size<span class="token punctuation">)</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="11"></td><td><pre>        <span class="token keyword">for</span> k<span class="token punctuation">,</span> t <span class="token keyword">in</span> concatenated_examples<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre>    <span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="13"></td><td><pre>    <span class="token comment"># Create a new labels column</span></pre></td></tr><tr><td data-num="14"></td><td><pre>    result<span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span> <span class="token operator">=</span> result<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="15"></td><td><pre>    <span class="token keyword">return</span> result</pre></td></tr></table></figure><p>注意，在 <code>group_texts()</code> 的最后一步中，我们创建了一个新的 labels 列，它是 <code>input_ids</code> 列的副本。我们很快就会看到，这是因为在掩码语言建模中，目标是预测输入批次中随机掩码的标记，并通过创建一个 labels 列，们为我们的语言模型提供了基础事实以供学习。</p><p>现在，让我们使用我们可信赖的 <code>Dataset.map()</code> 函数将 <code>group_texts()</code> 应用到我们的标记化数据集:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>lm_datasets <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>group_texts<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>lm_datasets</pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>DatasetDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    train<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="3"></td><td><pre>        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'attention_mask'</span><span class="token punctuation">,</span> <span class="token string">'input_ids'</span><span class="token punctuation">,</span> <span class="token string">'labels'</span><span class="token punctuation">,</span> <span class="token string">'word_ids'</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="4"></td><td><pre>        num_rows<span class="token punctuation">:</span> <span class="token number">61289</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    test<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="7"></td><td><pre>        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'attention_mask'</span><span class="token punctuation">,</span> <span class="token string">'input_ids'</span><span class="token punctuation">,</span> <span class="token string">'labels'</span><span class="token punctuation">,</span> <span class="token string">'word_ids'</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="8"></td><td><pre>        num_rows<span class="token punctuation">:</span> <span class="token number">59905</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    unsupervised<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="11"></td><td><pre>        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'attention_mask'</span><span class="token punctuation">,</span> <span class="token string">'input_ids'</span><span class="token punctuation">,</span> <span class="token string">'labels'</span><span class="token punctuation">,</span> <span class="token string">'word_ids'</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="12"></td><td><pre>        num_rows<span class="token punctuation">:</span> <span class="token number">122963</span></pre></td></tr><tr><td data-num="13"></td><td><pre>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>tokenized_datasets.map (group_texts, batched=True) 设置 <code>batched=True</code> 是不是将好几条数据一起处理，这样 group_texts 中进行数据的连接才有意义？</p><p>你可以看到，对文本进行分组，然后对文本进行分块，产生的示例比我们最初的 25,000 用于 train 和 test 拆分的示例多得多。那是因为我们现在有了涉及 连续标记 的示例，这些示例跨越了原始语料库中的多个示例。你可以通过在其中一个块中查找特殊的 [SEP] 和 [CLS] 标记来明确的看到这一点:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>lm_datasets<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token string">".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"</span></pre></td></tr></table></figure><p>在此示例中，你可以看到两篇重叠的电影评论，一篇关于高中电影，另一篇关于无家可归。 让我们也看看掩码语言建模的标签是什么样的:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>lm_datasets<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token string">".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"</span></pre></td></tr></table></figure><p>正如前面的 group_texts () 函数所期望的那样，这看起来与解码后的 input_ids 相同 — 但是我们的模型怎么可能学到任何东西呢？我们错过了一个关键步骤：在输入中的随机位置插入 [MASK] 标记！让我们看看如何使用特殊的数据整理器在微调期间即时执行此操作。</p><h2 id="使用-trainer-api-微调distilbert"><a class="anchor" href="#使用-trainer-api-微调distilbert">#</a> 使用 Trainer API 微调 DistilBERT</h2><p>微调屏蔽语言模型几乎与微调序列分类模型相同，就像我们在 第三章所作的那样。 唯一的区别是我们需要一个特殊的数据整理器，它可以随机屏蔽每批文本中的一些标记。幸运的是， <code>Transformers</code> 为这项任务准备了专用的 <code>DataCollatorForLanguageModeling</code> 。我们只需要将它转递给标记器和一个 <code>mlm_probability</code> 参数，该参数指定要屏蔽的标记的分数。我们将选择 15%, 这是 BERT 使用的数量也是文献中的常见选择:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> transformers <span class="token keyword">import</span> DataCollatorForLanguageModeling</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre>data_collator <span class="token operator">=</span> DataCollatorForLanguageModeling<span class="token punctuation">(</span>tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span> mlm_probability<span class="token operator">=</span><span class="token number">0.15</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>要了解随机掩码的工作原理，让我们向数据整理器提供一些示例。由于它需要一个 dict 的列表，其中每个 dict 表示单个连续文本块，我们首先迭代数据集，然后再将批次提供给整理器。我们删除了这个数据整理器的 &quot;word_ids&quot; 键，因为它不需要它:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>samples <span class="token operator">=</span> <span class="token punctuation">[</span>lm_datasets<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">for</span> sample <span class="token keyword">in</span> samples<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    _ <span class="token operator">=</span> sample<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token string">"word_ids"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token keyword">for</span> chunk <span class="token keyword">in</span> data_collator<span class="token punctuation">(</span>samples<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"\n'>>> </span><span class="token interpolation"><span class="token punctuation">&#123;</span>tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>chunk<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">'"</span></span><span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token string">'>>> [CLS] bromwell [MASK] is a cartoon comedy. it ran at the same [MASK] as some other [MASK] about school life, [MASK] as " teachers ". [MASK] [MASK] [MASK] in the teaching [MASK] lead [MASK] to believe that bromwell high\'[MASK] satire is much closer to reality than is " teachers ". the scramble [MASK] [MASK] financially, the [MASK]ful students whogn [MASK] right through [MASK] pathetic teachers\'pomp, the pettiness of the whole situation, distinction remind me of the schools i knew and their students. when i saw [MASK] episode in [MASK] a student repeatedly tried to burn down the school, [MASK] immediately recalled. [MASK]...'</span></pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token string">'>>> .... at.. [MASK]... [MASK]... high. a classic line plucked inspector : i\'[MASK] here to [MASK] one of your [MASK]. student : welcome to bromwell [MASK]. i expect that many adults of my age think that [MASK]mwell [MASK] is [MASK] fetched. what a pity that it isn\'t! [SEP] [CLS] [MASK]ness ( or [MASK]lessness as george 宇in stated )公 been an issue for years but never [MASK] plan to help those on the street that were once considered human [MASK] did everything from going to school, [MASK], [MASK] vote for the matter. most people think [MASK] the homeless'</span></pre></td></tr></table></figure><p>我们可以看到，[MASK] 标记已随机插入我们文本中的不同位置。 这些将是我们的模型在训练期间必须预测的标记 — 数据整理器的美妙之处在于，它将随机化每个批次的 [MASK] 插入！</p><p>多次运行上面的代码片段，看看随机屏蔽发生在你眼前！还要将 tokenizer.decode () 方法替换为 tokenizer.convert_ids_to_tokens () 以查看有时会屏蔽给定单词中的单个标记，而不是其他标记。</p><p>随机掩码的一个副作用是，当使用 Trainer 时，我们的评估指标将不是确定性的，因为我们对训练集和测试集使用相同的数据整理器。稍后我们会看到，当我们使用 Accelerate 进行微调时，我们将如何利用自定义评估循环的灵活性来冻结随机性。</p><p>在为掩码语言建模训练模型时，可以使用的一种技术是将整个单词一起屏蔽，而不仅仅是单个标记。这种方法称为 全词屏蔽。 如果我们想使用全词屏蔽，我们需要自己构建一个数据整理器。数据整理器只是一个函数，它接受一个样本列表并将它们转换为一个批次，所以现在让我们这样做吧！我们将使用之前计算的单词 ID 在单词索引和相应标记之间进行映射，然后随机决定要屏蔽哪些单词并将该屏蔽应用于输入。请注意，除了与掩码对应的标签外，所有的标签均为 -100。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> collections</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">from</span> transformers <span class="token keyword">import</span> default_data_collator</pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre>wwm_probability <span class="token operator">=</span> <span class="token number">0.2</span></pre></td></tr><tr><td data-num="7"></td><td><pre></pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token keyword">def</span> <span class="token function">whole_word_masking_data_collator</span><span class="token punctuation">(</span>features<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    <span class="token keyword">for</span> feature <span class="token keyword">in</span> features<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="11"></td><td><pre>        word_ids <span class="token operator">=</span> feature<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token string">"word_ids"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre></pre></td></tr><tr><td data-num="13"></td><td><pre>        <span class="token comment"># Create a map between words and corresponding token indices</span></pre></td></tr><tr><td data-num="14"></td><td><pre>        mapping <span class="token operator">=</span> collections<span class="token punctuation">.</span>defaultdict<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="15"></td><td><pre>        current_word_index <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span></pre></td></tr><tr><td data-num="16"></td><td><pre>        current_word <span class="token operator">=</span> <span class="token boolean">None</span></pre></td></tr><tr><td data-num="17"></td><td><pre>        <span class="token keyword">for</span> idx<span class="token punctuation">,</span> word_id <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>word_ids<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="18"></td><td><pre>            <span class="token keyword">if</span> word_id <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="19"></td><td><pre>                <span class="token keyword">if</span> word_id <span class="token operator">!=</span> current_word<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="20"></td><td><pre>                    current_word <span class="token operator">=</span> word_id</pre></td></tr><tr><td data-num="21"></td><td><pre>                    current_word_index <span class="token operator">+=</span> <span class="token number">1</span></pre></td></tr><tr><td data-num="22"></td><td><pre>                mapping<span class="token punctuation">[</span>current_word_index<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>idx<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="23"></td><td><pre></pre></td></tr><tr><td data-num="24"></td><td><pre>        <span class="token comment"># Randomly mask words</span></pre></td></tr><tr><td data-num="25"></td><td><pre>        mask <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>binomial<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> wwm_probability<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>mapping<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="26"></td><td><pre>        input_ids <span class="token operator">=</span> feature<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="27"></td><td><pre>        labels <span class="token operator">=</span> feature<span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="28"></td><td><pre>        new_labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>labels<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="29"></td><td><pre>        <span class="token keyword">for</span> word_id <span class="token keyword">in</span> np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>mask<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="30"></td><td><pre>            word_id <span class="token operator">=</span> word_id<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="31"></td><td><pre>            <span class="token keyword">for</span> idx <span class="token keyword">in</span> mapping<span class="token punctuation">[</span>word_id<span class="token punctuation">]</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="32"></td><td><pre>                new_labels<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> labels<span class="token punctuation">[</span>idx<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="33"></td><td><pre>                input_ids<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>mask_token_id</pre></td></tr><tr><td data-num="34"></td><td><pre>        feature<span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span> <span class="token operator">=</span> new_labels</pre></td></tr><tr><td data-num="35"></td><td><pre></pre></td></tr><tr><td data-num="36"></td><td><pre>    <span class="token keyword">return</span> default_data_collator<span class="token punctuation">(</span>features<span class="token punctuation">)</span></pre></td></tr></table></figure><p>Next, we can try it on the same samples as before:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>samples <span class="token operator">=</span> <span class="token punctuation">[</span>lm_datasets<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="2"></td><td><pre>batch <span class="token operator">=</span> whole_word_masking_data_collator<span class="token punctuation">(</span>samples<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">for</span> chunk <span class="token keyword">in</span> batch<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"\n'>>> </span><span class="token interpolation"><span class="token punctuation">&#123;</span>tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>chunk<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">'"</span></span><span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token string">'>>> [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as some other programs about school life, such as " teachers ". my 35 years in the teaching profession lead me to believe that bromwell high\'s satire is much closer to reality than is " teachers ". the scramble to survive financially, the insightful students who can see right through their pathetic teachers\'pomp, the pettiness of the whole situation, all remind me of the schools i knew and their students. when i saw the episode in which a student repeatedly tried to burn down the school, i immediately recalled.....'</span></pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token string">'>>> .... [MASK] [MASK] [MASK] [MASK]....... high. a classic line : inspector : i\'m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn\'t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless'</span></pre></td></tr></table></figure><p>多次运行上面的代码片段，看看随机屏蔽发生在你眼前！还要将 tokenizer.decode () 方法替换为 tokenizer.convert_ids_to_tokens () 以查看来自给定单词的标记始终被屏蔽在一起。</p><p>现在我们有两个数据整理器，其余的微调步骤是标准的。如果您没有足够幸运地获得神话般的 P100 GPU , 在 Google Colab 上进行训练可能需要一段时间，因此我们将首先将训练集的大小缩减为几千个示例。别担心，我们仍然会得到一个相当不错的语言模型！在 Datasets 中快速下采样数据集的方法是通过我们在 第五章 中看到的 Dataset.train_test_split () 函数:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>train_size <span class="token operator">=</span> <span class="token number">10_000</span></pre></td></tr><tr><td data-num="2"></td><td><pre>test_size <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token number">0.1</span> <span class="token operator">*</span> train_size<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre>downsampled_dataset <span class="token operator">=</span> lm_datasets<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>train_test_split<span class="token punctuation">(</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    train_size<span class="token operator">=</span>train_size<span class="token punctuation">,</span> test_size<span class="token operator">=</span>test_size<span class="token punctuation">,</span> seed<span class="token operator">=</span><span class="token number">42</span></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>downsampled_dataset</pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>DatasetDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    train<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="3"></td><td><pre>        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'attention_mask'</span><span class="token punctuation">,</span> <span class="token string">'input_ids'</span><span class="token punctuation">,</span> <span class="token string">'labels'</span><span class="token punctuation">,</span> <span class="token string">'word_ids'</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="4"></td><td><pre>        num_rows<span class="token punctuation">:</span> <span class="token number">10000</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    test<span class="token punctuation">:</span> Dataset<span class="token punctuation">(</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="7"></td><td><pre>        features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'attention_mask'</span><span class="token punctuation">,</span> <span class="token string">'input_ids'</span><span class="token punctuation">,</span> <span class="token string">'labels'</span><span class="token punctuation">,</span> <span class="token string">'word_ids'</span><span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="8"></td><td><pre>        num_rows<span class="token punctuation">:</span> <span class="token number">1000</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre><span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>可以指定 Trainer 参数:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> transformers <span class="token keyword">import</span> TrainingArguments</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre>batch_size <span class="token operator">=</span> <span class="token number">64</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token comment"># Show the training loss with every epoch</span></pre></td></tr><tr><td data-num="5"></td><td><pre>logging_steps <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>downsampled_dataset<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">//</span> batch_size</pre></td></tr><tr><td data-num="6"></td><td><pre>model_name <span class="token operator">=</span> model_checkpoint<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"/"</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="7"></td><td><pre></pre></td></tr><tr><td data-num="8"></td><td><pre>training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    output_dir<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>model_name<span class="token punctuation">&#125;</span></span><span class="token string">-finetuned-imdb"</span></span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    overwrite_output_dir<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="11"></td><td><pre>    evaluation_strategy<span class="token operator">=</span><span class="token string">"epoch"</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="12"></td><td><pre>    learning_rate<span class="token operator">=</span><span class="token number">2e-5</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="13"></td><td><pre>    weight_decay<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="14"></td><td><pre>    per_device_train_batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="15"></td><td><pre>    per_device_eval_batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="16"></td><td><pre>    push_to_hub<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="17"></td><td><pre>    fp16<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="18"></td><td><pre>    logging_steps<span class="token operator">=</span>logging_steps<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="19"></td><td><pre><span class="token punctuation">)</span></pre></td></tr></table></figure><p>在这里，我们调整了一些默认选项，包括 logging_steps , 以确保我们跟踪每个 epoch 的训练损失。我们还使用了 fp16=True 来实现混合精度训练，这给我们带来了另一个速度提升。默认情况下，Trainer 将删除不属于模型的 forward () 方法的列。这意味着，如果你使用整个单词屏蔽排序器，你还需要设置 remove_unused_columns=False, 以确保我们不会在训练期间丢失 word_ids 列。</p><h2 id="语言模型的perplexity"><a class="anchor" href="#语言模型的perplexity">#</a> 语言模型的 perplexity</h2><p>与文本分类或问答等其他任务不同，在这些任务中，我们会得到一个带标签的语料库进行训练，而语言建模则没有任何明确的标签。那么我们如何确定什么是好的语言模型呢？就像手机中的自动更正功能一样，一个好的语言模型是为语法正确的句子分配高概率，为无意义的句子分配低概率。为了让你更好地了解这是什么样子，您可以在网上找到一整套 “autocorrect fails”, 其中一个人手机中的模型产生了一些相当有趣 (而且通常不合适) 的结果！</p><p>假设我们的测试集主要由语法正确的句子组成，那么衡量我们的语言模型质量的一种方法是计算它分配给测试集中所有句子中的下一个单词的概率。高概率表明模型对看不见的例子并不感到 “惊讶” 或 “疑惑”, 并表明它已经学习了语言中的基本语法模式。 perplexity 度有多种数学定义，但我们将使用的定义是交叉熵损失的指数。因此，我们可以通过 Trainer.evaluate () 函数计算测试集上的交叉熵损失，然后取结果的指数来计算预训练模型的 perplexity 度:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> math</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre>eval_results <span class="token operator">=</span> trainer<span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f">>> Perplexity: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>eval_results<span class="token punctuation">[</span><span class="token string">'eval_loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre>Perplexity<span class="token punctuation">:</span> <span class="token number">21.75</span></pre></td></tr></table></figure><p>较低的 perplexity 分数意味着更好的语言模型，我们可以在这里看到我们的起始模型有一个较大的值。看看我们能不能通过微调来降低它！为此，我们首先运行训练循环:</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></table></figure><div class="tags"><a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" rel="tag"><i class="ic i-tag"></i> 自然语言处理</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2022-11-12 13:39:21" itemprop="dateModified" datetime="2022-11-12T13:39:21+08:00">2022-11-12</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="yuan 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="yuan 支付宝"><p>支付宝</p></div><div><img data-src="/images/paypal.png" alt="yuan 贝宝"><p>贝宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>yuan <i class="ic i-at"><em>@</em></i>yuan</li><li class="link"><strong>本文链接：</strong> <a href="https://jyuanhust.github.io/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E6%8E%A9%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" title="微调一个掩码语言模型">https://jyuanhust.github.io/2022/11/12/ai/nlp/huggingface/主要的 NLP 任务/微调一个掩码语言模型/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81Summarization/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;gitee.com&#x2F;zkz0&#x2F;image&#x2F;raw&#x2F;master&#x2F;img&#x2F;img(43).webp" title="文本摘要 summarize"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 主要nlp任务</span><h3>文本摘要 summarize</h3></a></div><div class="item right"><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/BPE/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;gitee.com&#x2F;zkz0&#x2F;image&#x2F;raw&#x2F;master&#x2F;img&#x2F;img(68).webp" title="Byte-Pair Encoding tokenization"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> Tokenizer库</span><h3>Byte-Pair Encoding tokenization</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E6%8E%A9%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">微调掩码语言模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E7%94%A8%E4%BA%8E%E6%8E%A9%E7%A0%81%E8%AF%AD%E8%A8%80%E5%BB%BA%E6%A8%A1%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.</span> <span class="toc-text">选择用于掩码语言建模的预训练模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.2.</span> <span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE"><span class="toc-number">1.3.</span> <span class="toc-text">预处理数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-trainer-api-%E5%BE%AE%E8%B0%83distilbert"><span class="toc-number">1.4.</span> <span class="toc-text">使用 Trainer API 微调 DistilBERT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84perplexity"><span class="toc-number">1.5.</span> <span class="toc-text">语言模型的 perplexity</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li class="active"><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E6%8E%A9%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" rel="bookmark" title="微调一个掩码语言模型">微调一个掩码语言模型</a></li><li><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81Summarization/" rel="bookmark" title="文本摘要 summarize">文本摘要 summarize</a></li><li><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E9%97%AE%E7%AD%94/" rel="bookmark" title="问答 question answer">问答 question answer</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="yuan" data-src="/images/avatar.jpg"><p class="name" itemprop="name">yuan</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">429</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">72</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">61</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item email" data-url="bWFpbHRvOjIwODM2MzU1MjVAcXEuY29t" title="mailto:2083635525@qq.com"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li><li class="item"><a href="/links/" rel="section"><i class="ic i-magic"></i>链接</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81Summarization/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/BPE/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"></div><span><a href="/2023/07/30/computer-science/algorithm/%E5%9B%BE%E8%A7%A3%E7%AE%97%E6%B3%95%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%8E%92%E5%BA%8F/" title="未命名">未命名</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/frontend/" title="分类于 前端">前端</a></div><span><a href="/2022/09/08/frontend/CSS/CSS%E5%B8%83%E5%B1%80/" title="CSS布局">CSS布局</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/tools/" title="分类于 tools">tools</a></div><span><a href="/2022/07/26/tools/vscode%E6%8A%80%E5%B7%A7/" title="vscode技巧">vscode技巧</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2022/08/24/language/python/python%E4%BD%BF%E7%94%A8shutil-copyfile-%E5%A4%8D%E5%88%B6%E6%96%87%E4%BB%B6/" title="python使用shutil_copyfile_复制文件">python使用shutil_copyfile_复制文件</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-convolutional-modern/" title="分类于 chapter_convolutional-modern">chapter_convolutional-modern</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_convolutional-modern/alexnet/" title="alexnet">alexnet</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 computer-science">computer-science</a></div><span><a href="/2023/02/27/computer-science/%E6%AF%94%E8%B5%9B/%E8%93%9D%E6%A1%A5%E6%9D%AF/note/" title="蓝桥杯省赛题目理解">蓝桥杯省赛题目理解</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2023/06/25/computer-science/algorithm/leetCode/5-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/" title="未命名">未命名</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 computer-science">computer-science</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86%E5%AE%9E%E8%B7%B5MySQL/" title="分类于 数据库系统原理实践MySQL">数据库系统原理实践MySQL</a></div><span><a href="/2022/08/24/computer-science/base/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86%E5%AE%9E%E8%B7%B5MySQL/%E5%AE%9E%E8%AE%AD11-%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%E4%B8%8E%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/" title="实训11-并发控制与事务的隔离级别">实训11-并发控制与事务的隔离级别</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-linear-networks/" title="分类于 chapter_linear-networks">chapter_linear-networks</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_linear-networks/softmax-regression-scratch/" title="softmax-regression-scratch">softmax-regression-scratch</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 pytorch深度学习">pytorch深度学习</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter-attention-mechanisms/" title="分类于 chapter_attention-mechanisms">chapter_attention-mechanisms</a></div><span><a href="/2023/02/15/ai/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapter_attention-mechanisms/self-attention-and-positional-encoding/" title="self-attention-and-positional-encoding">self-attention-and-positional-encoding</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">yuan @ Mi Manchi</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">2.9m 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">44:38</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2022/11/12/ai/nlp/huggingface/主要的 NLP 任务/微调一个掩码语言模型/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html>